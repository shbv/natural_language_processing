<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>spooky_author_identification</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    color: #000 !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.2.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.2.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.2.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.2.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.2.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.2.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=1);
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2);
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=3);
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1);
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1);
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
@media (max-width: 991px) {
  #ipython_notebook {
    margin-left: 10px;
  }
}
[dir="rtl"] #ipython_notebook {
  float: right !important;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#login_widget {
  float: right;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  text-align: center;
  vertical-align: middle;
  display: inline;
  opacity: 0;
  z-index: 2;
  width: 12ex;
  margin-right: -12ex;
}
.alternate_upload .btn-upload {
  height: 22px;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
[dir="rtl"] #tabs li {
  float: right;
}
ul#tabs {
  margin-bottom: 4px;
}
[dir="rtl"] ul#tabs {
  margin-right: 0px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons {
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-right {
  padding-top: 1px;
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-left {
  float: right !important;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: baseline;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
#tree-selector {
  padding-right: 0px;
}
[dir="rtl"] #tree-selector a {
  float: right;
}
#button-select-all {
  min-width: 50px;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
[dir="rtl"] #new-menu {
  text-align: right;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
[dir="rtl"] #running .col-sm-8 {
  float: right !important;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul {
  list-style: disc;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ul ul {
  list-style: square;
  margin: 0em 2em;
}
.rendered_html ul ul ul {
  list-style: circle;
  margin: 0em 2em;
}
.rendered_html ol {
  list-style: decimal;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
  margin: 0em 2em;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  background-color: #fff;
  color: #000;
  font-size: 100%;
  padding: 0px;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: 1px solid black;
  border-collapse: collapse;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  border: 1px solid black;
  border-collapse: collapse;
  margin: 1em 2em;
}
.rendered_html td,
.rendered_html th {
  text-align: left;
  vertical-align: middle;
  padding: 4px;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget {
  float: right !important;
  float: right;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  margin-top: 6px;
}
span.save_widget span.filename {
  height: 1em;
  line-height: 1em;
  padding: 3px;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  display: none;
}
.command-shortcut:before {
  content: "(command)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Read-packages">Read packages<a class="anchor-link" href="#Read-packages">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Basic: </span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">os</span> <span class="k">import</span> <span class="n">listdir</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="k">import</span> <span class="n">display</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="k">import</span> <span class="n">tqdm</span>

<span class="c1"># Features, and other misc.:</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">StratifiedKFold</span><span class="p">,</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="k">import</span> <span class="n">CountVectorizer</span><span class="p">,</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="k">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">LabelEncoder</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="k">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="k">import</span> <span class="n">pad_sequences</span>
<span class="kn">from</span> <span class="nn">keras.utils.np_utils</span> <span class="k">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="k">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">EarlyStopping</span>

<span class="c1"># Models:</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">SGDClassifier</span><span class="p">,</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="k">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">SpatialDropout1D</span> 
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">MaxPooling1D</span><span class="p">,</span> <span class="n">GlobalAveragePooling1D</span> 
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Bidirectional</span>
<span class="kn">from</span> <span class="nn">keras.layers.recurrent</span> <span class="k">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">GRU</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Sequential</span> 
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>

<span class="c1"># Metrics:</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">log_loss</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">make_scorer</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">make_scorer</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">re</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Read-data">Read data<a class="anchor-link" href="#Read-data">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">listdir</span><span class="p">(</span><span class="s2">&quot;../input/&quot;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[27]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>[&#39;test.csv&#39;, &#39;train.csv&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_raw</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../input/train.csv&quot;</span><span class="p">)</span>
<span class="n">test_raw</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../input/test.csv&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_raw</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_raw</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">train_raw</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="n">display</span><span class="p">(</span><span class="n">test_raw</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>(19579, 3) (8392, 2)
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>author</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id26305</td>
      <td>This process, however, afforded me no means of...</td>
      <td>EAP</td>
    </tr>
    <tr>
      <th>1</th>
      <td>id17569</td>
      <td>It never once occurred to me that the fumbling...</td>
      <td>HPL</td>
    </tr>
    <tr>
      <th>2</th>
      <td>id11008</td>
      <td>In his left hand was a gold snuff box, from wh...</td>
      <td>EAP</td>
    </tr>
    <tr>
      <th>3</th>
      <td>id27763</td>
      <td>How lovely is spring As we looked from Windsor...</td>
      <td>MWS</td>
    </tr>
    <tr>
      <th>4</th>
      <td>id12958</td>
      <td>Finding nothing else, not even gold, the Super...</td>
      <td>HPL</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id02310</td>
      <td>Still, as I urged our leaving Ireland with suc...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>id24541</td>
      <td>If a fire wanted fanning, it could readily be ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>id00134</td>
      <td>And when they had broken down the frail door t...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>id27757</td>
      <td>While I was thinking how I should possibly man...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>id04081</td>
      <td>I am not sure to what limit his knowledge may ...</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Data-analysis-and-cleanup:">Data analysis and cleanup:<a class="anchor-link" href="#Data-analysis-and-cleanup:">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Is data clean and mem usage</span>
<span class="n">train_raw</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
<span class="n">test_raw</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 19579 entries, 0 to 19578
Data columns (total 3 columns):
id        19579 non-null object
text      19579 non-null object
author    19579 non-null object
dtypes: object(3)
memory usage: 459.0+ KB
&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 8392 entries, 0 to 8391
Data columns (total 2 columns):
id      8392 non-null object
text    8392 non-null object
dtypes: object(2)
memory usage: 131.2+ KB
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[30]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Is number of training examples per class balanced?</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;author&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">train_raw</span><span class="p">)</span>
<span class="c1">#sns.countplot(train.author)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Num of training examples per class (i.e author)&quot;</span><span class="p">);</span>
<span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;author&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[30]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>EAP    7900
MWS    6044
HPL    5635
Name: author, dtype: int64</pre>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[31]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Peak at the sentences:</span>
<span class="k">for</span> <span class="n">author</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;EAP&quot;</span><span class="p">,</span> <span class="s2">&quot;MWS&quot;</span><span class="p">,</span> <span class="s2">&quot;HPL&quot;</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;===&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">author</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;===&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">display</span><span class="p">(</span><span class="n">train_raw</span><span class="p">[</span><span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;author&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">author</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>===EAP===
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>&#39;This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.&#39;</pre>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>&#39;In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.&#39;</pre>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>&#39;The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall.&#39;</pre>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>===MWS===
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>&#39;How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.&#39;</pre>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>&#39;A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.&#39;</pre>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>&#39;I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me.&#39;</pre>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>===HPL===
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>&#39;It never once occurred to me that the fumbling might be a mere mistake.&#39;</pre>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>&#39;Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.&#39;</pre>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>&#39;Herbert West needed fresh bodies because his life work was the reanimation of the dead.&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Check stop words and punctuation per author:</span>
<span class="c1"># Add them as features:</span>

<span class="c1"># Punctuations:</span>
<span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;num_punctuations&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">([</span><span class="n">char</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">]))</span>
<span class="n">test_raw</span><span class="p">[</span><span class="s1">&#39;num_punctuations&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_raw</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">([</span><span class="n">char</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">]))</span>

<span class="c1"># Stopwords:</span>
<span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;num_stopwords&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)]))</span>
<span class="n">test_raw</span><span class="p">[</span><span class="s1">&#39;num_stopwords&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_raw</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)]))</span>

<span class="c1"># Num of words:</span>
<span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;num_words&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()]))</span>
<span class="n">test_raw</span><span class="p">[</span><span class="s1">&#39;num_words&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_raw</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()]))</span>

<span class="c1">#print(train_raw.loc[0:4][&#39;text&#39;])</span>
<span class="c1">#print(train_raw[&#39;num_punctuations&#39;][0:4])</span>
<span class="n">display</span><span class="p">(</span><span class="n">train_raw</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="n">display</span><span class="p">(</span><span class="n">test_raw</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">train_raw</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;num_punctuations ---&gt; &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;num_punctuations&#39;</span><span class="p">][</span><span class="n">idx</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;num_stopwords ---&gt; &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;num_stopwords&#39;</span><span class="p">][</span><span class="n">idx</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;num_words ---&gt; &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;num_words&#39;</span><span class="p">][</span><span class="n">idx</span><span class="p">]))</span>

<span class="c1">## Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Distribution of num of punctuations per author&quot;</span><span class="p">);</span>
<span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;author&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;num_punctuations&#39;</span><span class="p">,</span><span class="n">data</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Distribution of num of stopwords per author&quot;</span><span class="p">);</span>
<span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;author&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;num_stopwords&#39;</span><span class="p">,</span><span class="n">data</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Distribution of num of words per author&quot;</span><span class="p">);</span>
<span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;author&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;num_words&#39;</span><span class="p">,</span><span class="n">data</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>author</th>
      <th>num_punctuations</th>
      <th>num_stopwords</th>
      <th>num_words</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id26305</td>
      <td>This process, however, afforded me no means of...</td>
      <td>EAP</td>
      <td>7</td>
      <td>19</td>
      <td>41</td>
    </tr>
    <tr>
      <th>1</th>
      <td>id17569</td>
      <td>It never once occurred to me that the fumbling...</td>
      <td>HPL</td>
      <td>1</td>
      <td>8</td>
      <td>14</td>
    </tr>
    <tr>
      <th>2</th>
      <td>id11008</td>
      <td>In his left hand was a gold snuff box, from wh...</td>
      <td>EAP</td>
      <td>5</td>
      <td>16</td>
      <td>36</td>
    </tr>
    <tr>
      <th>3</th>
      <td>id27763</td>
      <td>How lovely is spring As we looked from Windsor...</td>
      <td>MWS</td>
      <td>4</td>
      <td>13</td>
      <td>34</td>
    </tr>
    <tr>
      <th>4</th>
      <td>id12958</td>
      <td>Finding nothing else, not even gold, the Super...</td>
      <td>HPL</td>
      <td>4</td>
      <td>11</td>
      <td>27</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>num_punctuations</th>
      <th>num_stopwords</th>
      <th>num_words</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id02310</td>
      <td>Still, as I urged our leaving Ireland with suc...</td>
      <td>3</td>
      <td>9</td>
      <td>19</td>
    </tr>
    <tr>
      <th>1</th>
      <td>id24541</td>
      <td>If a fire wanted fanning, it could readily be ...</td>
      <td>7</td>
      <td>33</td>
      <td>62</td>
    </tr>
    <tr>
      <th>2</th>
      <td>id00134</td>
      <td>And when they had broken down the frail door t...</td>
      <td>3</td>
      <td>15</td>
      <td>33</td>
    </tr>
    <tr>
      <th>3</th>
      <td>id27757</td>
      <td>While I was thinking how I should possibly man...</td>
      <td>5</td>
      <td>19</td>
      <td>41</td>
    </tr>
    <tr>
      <th>4</th>
      <td>id04081</td>
      <td>I am not sure to what limit his knowledge may ...</td>
      <td>1</td>
      <td>6</td>
      <td>11</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.
num_punctuations ---&gt; 7
num_stopwords ---&gt; 19
num_words ---&gt; 41
It never once occurred to me that the fumbling might be a mere mistake.
num_punctuations ---&gt; 1
num_stopwords ---&gt; 8
num_words ---&gt; 14
In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.
num_punctuations ---&gt; 5
num_stopwords ---&gt; 16
num_words ---&gt; 36
How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.
num_punctuations ---&gt; 4
num_stopwords ---&gt; 13
num_words ---&gt; 34
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt output_prompt">Out[32]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f649b18dc50&gt;</pre>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYkAAAEWCAYAAACT7WsrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYkAAAEWCAYAAACT7WsrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Encode-the-output-as-numerical-data-for-training">Encode the output as numerical data for training<a class="anchor-link" href="#Encode-the-output-as-numerical-data-for-training">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[33]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Encode the output labels as 3 classes (from string to int)</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;author&#39;</span><span class="p">])</span>
<span class="n">y_train_raw</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;author&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s1">&#39;EAP&#39;</span><span class="p">,</span><span class="s1">&#39;HPL&#39;</span><span class="p">,</span><span class="s1">&#39;MWS&#39;</span><span class="p">]))</span>

<span class="c1"># Store inverse transform for submissions csv</span>
<span class="nb">print</span><span class="p">(</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]))</span>
<span class="n">label_inv_transform</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;EAP&#39; &#39;HPL&#39; &#39;MWS&#39;]
[0 1 2]
[&#39;EAP&#39; &#39;HPL&#39; &#39;MWS&#39;]
[&#39;EAP&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Baseline-accuracy">Baseline accuracy<a class="anchor-link" href="#Baseline-accuracy">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">## Submission file function</span>

<span class="c1"># Given prob, print it to file</span>
<span class="k">def</span> <span class="nf">create_submission_final</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">predictions_prob</span><span class="p">):</span>

    <span class="n">subm_file</span> <span class="o">=</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">&#39;_submission.csv&#39;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Creating submission file: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">subm_file</span><span class="p">))</span>
    
    <span class="c1"># Get probabilities</span>
    <span class="n">test_id_values</span> <span class="o">=</span> <span class="n">test_raw</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="c1">#print(predictions[::100])</span>
    <span class="c1">#print(predictions_prob[::100])</span>

    <span class="c1"># Print to file</span>
    <span class="n">op_file</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">op_file</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_id_values</span>
    <span class="n">op_file</span><span class="p">[</span><span class="n">label_inv_transform</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">predictions_prob</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="p">;</span> <span class="c1"># EAP</span>
    <span class="n">op_file</span><span class="p">[</span><span class="n">label_inv_transform</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">predictions_prob</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="p">;</span> <span class="c1"># HPL</span>
    <span class="n">op_file</span><span class="p">[</span><span class="n">label_inv_transform</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span> <span class="o">=</span> <span class="n">predictions_prob</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span> <span class="p">;</span> <span class="c1"># MWS</span>
    <span class="n">op_file</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">subm_file</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[35]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Baseline accuracy:</span>
<span class="n">baseline_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="mi">8392</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">create_submission_final</span><span class="p">(</span><span class="s2">&quot;baseline_1&quot;</span><span class="p">,</span> <span class="n">baseline_prob</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Creating submission file: baseline_1_submission.csv
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="K-fold-training-function:-It-will-be-used-throughout-this-notebook">K-fold training function: It will be used throughout this notebook<a class="anchor-link" href="#K-fold-training-function:-It-will-be-used-throughout-this-notebook">&#182;</a></h3><h4 id="Also,-includes-function-to-add-predictions-as-features">Also, includes function to add predictions as features<a class="anchor-link" href="#Also,-includes-function-to-add-predictions-as-features">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[36]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#############################</span>
<span class="c1"># K-fold CV training function:</span>
<span class="c1">#############################</span>
<span class="c1"># Inputs: x_train (includes y labels), y_train (only y labels), x_test</span>
<span class="c1"># Preprocess train_raw, test_raw to give x_train, y_train, x_test</span>
<span class="k">def</span> <span class="nf">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                       <span class="n">run_model_preprocess</span><span class="p">,</span> <span class="n">model_preprocess_params</span><span class="p">,</span>
                       <span class="n">run_model</span><span class="p">,</span> <span class="n">model_params</span><span class="p">,</span>
                       <span class="n">algorithm_type</span><span class="p">,</span>
                       <span class="n">x_train_raw</span><span class="p">,</span> <span class="n">y_train_raw</span><span class="p">,</span> <span class="n">x_test_raw</span><span class="p">,</span>
                       <span class="n">kfold_idx_not_on_df</span><span class="p">):</span>

    <span class="c1"># Print some messages</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running kfold training with model&quot;</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shapes: x_train_raw.shape </span><span class="si">{}</span><span class="s2">, y_train_raw.shape </span><span class="si">{}</span><span class="s2"> , x_test_raw.shape </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">x_train_raw</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train_raw</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_test_raw</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    
    <span class="c1"># Preprocess the data to input to the model</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running preprocess:&quot;</span><span class="p">,</span> <span class="n">run_model_preprocess</span><span class="p">)</span>
    <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> 
     <span class="n">extra_params</span><span class="p">)</span> <span class="o">=</span> <span class="n">run_model_preprocess</span><span class="p">(</span><span class="n">x_train_raw</span><span class="p">,</span> <span class="n">x_test_raw</span><span class="p">,</span> <span class="n">model_preprocess_params</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train_raw</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shapes: x_train.shape </span><span class="si">{}</span><span class="s2">, y_train.shape </span><span class="si">{}</span><span class="s2"> , x_test.shape </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    
    
    <span class="c1"># Add extra params to model_params dictionary to pass to running the model</span>
    <span class="k">if</span> <span class="n">extra_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">extra_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">model_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
    
    <span class="c1"># Type of split</span>
    <span class="n">num_splits</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">=</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Num of classes in the output</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">3</span>
    
    <span class="c1"># train and test predictions (could be added as stacking features later)</span>
    <span class="n">pred_test</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">pred_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x_train_raw</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_classes</span><span class="p">))</span>
    
    <span class="c1"># val_scores</span>
    <span class="n">val_scores</span> <span class="o">=</span> <span class="p">[]</span>
    
   
    <span class="c1"># Kfold training and prediction</span>
    <span class="n">fold_num</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">train_folds_idx</span><span class="p">,</span> <span class="n">val_fold_idx</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x_train_raw</span><span class="p">):</span>

        <span class="n">fold_num</span><span class="o">+=</span><span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running fold&quot;</span><span class="p">,</span> <span class="n">fold_num</span><span class="p">)</span>
        
        <span class="c1"># Get the train and validation folds (Dataframe idx needs .loc)</span>
        <span class="k">if</span> <span class="n">kfold_idx_not_on_df</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x_train_folds</span><span class="p">,</span> <span class="n">x_val_fold</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">train_folds_idx</span><span class="p">],</span> <span class="n">x_train</span><span class="p">[</span><span class="n">val_fold_idx</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_train_folds</span><span class="p">,</span> <span class="n">x_val_fold</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train_folds_idx</span><span class="p">],</span> <span class="n">x_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">val_fold_idx</span><span class="p">]</span>
        <span class="n">y_train_folds</span><span class="p">,</span> <span class="n">y_val_fold</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">train_folds_idx</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">val_fold_idx</span><span class="p">]</span>
        
        <span class="c1"># Fit </span>
        <span class="p">(</span><span class="n">pred_val_fold</span><span class="p">,</span> <span class="n">pred_test_per_fold_training</span><span class="p">,</span> 
         <span class="n">model</span><span class="p">)</span> <span class="o">=</span> <span class="n">run_model</span><span class="p">(</span><span class="n">x_train_folds</span><span class="p">,</span> <span class="n">y_train_folds</span><span class="p">,</span> 
                            <span class="n">x_val_fold</span><span class="p">,</span> <span class="n">y_val_fold</span><span class="p">,</span>
                            <span class="n">x_test</span><span class="p">,</span>
                            <span class="n">model_params</span><span class="p">)</span>
        
        <span class="c1"># Compile predictions on validation fold (i.e. unseen data)</span>
        <span class="n">pred_train</span><span class="p">[</span><span class="n">val_fold_idx</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">pred_val_fold</span>
        <span class="c1"># Compile predictions on test set using current training folds model</span>
        <span class="n">pred_test</span> <span class="o">+=</span> <span class="n">pred_test_per_fold_training</span>
        <span class="n">val_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_val_fold</span><span class="p">,</span> <span class="n">pred_val_fold</span><span class="p">))</span>

    <span class="n">pred_test</span> <span class="o">=</span> <span class="n">pred_test</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">num_splits</span><span class="p">)</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">===== Model: </span><span class="si">{}</span><span class="s2">  ========:&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; Cross-val log losses are:&quot;</span><span class="p">,</span> <span class="n">val_scores</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;====== Mean cross-val log loss is: </span><span class="si">{}</span><span class="s2"> =========</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">val_scores</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span>

<span class="c1">#########################################</span>
<span class="c1"># Function to add predictions as features</span>
<span class="c1">#########################################</span>
<span class="k">def</span> <span class="nf">add_pred_features</span><span class="p">(</span><span class="n">modelname</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">):</span>
    
    <span class="n">feature_eap</span> <span class="o">=</span> <span class="n">modelname</span> <span class="o">+</span> <span class="s1">&#39;_eap&#39;</span>
    <span class="n">feature_hpl</span> <span class="o">=</span> <span class="n">modelname</span> <span class="o">+</span> <span class="s1">&#39;_hpl&#39;</span>
    <span class="n">feature_mws</span> <span class="o">=</span> <span class="n">modelname</span> <span class="o">+</span> <span class="s1">&#39;_mws&#39;</span>
    <span class="n">x_train</span><span class="p">[</span><span class="n">feature_eap</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_train</span><span class="p">[</span><span class="n">feature_hpl</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x_train</span><span class="p">[</span><span class="n">feature_mws</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_train</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">x_test</span><span class="p">[</span><span class="n">feature_eap</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_test</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_test</span><span class="p">[</span><span class="n">feature_hpl</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_test</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x_test</span><span class="p">[</span><span class="n">feature_mws</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_test</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Pre-processing-and-Supervised-Machine-learning-algorithms:">Pre-processing and Supervised Machine learning algorithms:<a class="anchor-link" href="#Pre-processing-and-Supervised-Machine-learning-algorithms:">&#182;</a></h3><ul>
<li>CountVectorizer, TfidfVectorizer</li>
<li>Multinominal Naive Bayes (MNB), Logistic Regression (LR)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tfidf vectorizer with MNB:</span>
<span class="k">def</span> <span class="nf">gen_tfidf</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">,</span> <span class="n">x_test_text</span><span class="p">):</span>
    <span class="n">ngram</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="c1">#vect = TfidfVectorizer(stop_words=&#39;english&#39;, ngram_range = (1,ngram)).fit(x_train)</span>
    <span class="n">vect</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">ngram_range</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">ngram</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">)</span>
    <span class="n">x_train_doc_term</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">)</span>
    <span class="n">x_test_doc_term</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_test_text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_train_doc_term</span><span class="p">,</span> <span class="n">x_test_doc_term</span>

<span class="c1"># Run mnb on tfidf</span>
<span class="k">def</span> <span class="nf">run_mnb_tfidf</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Separate out text</span>
    <span class="n">x_train_text</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="n">x_test_text</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="c1"># Get tfidf doc term matrix</span>
    <span class="n">x_train_doc_term</span><span class="p">,</span> <span class="n">x_test_doc_term</span> <span class="o">=</span> <span class="n">gen_tfidf</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">,</span> <span class="n">x_test_text</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">x_train_doc_term</span><span class="p">,</span> <span class="n">x_test_doc_term</span><span class="p">,</span> <span class="kc">None</span>

<span class="c1"># Multinomial Naive bayes algorithm</span>
<span class="k">def</span> <span class="nf">run_mnb</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">pred_val</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_val</span><span class="p">)</span>
    <span class="n">pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pred_val</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span>   

<span class="c1"># Run MNB tfidf and add predictions as features:</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;mnb_tfidf&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_mnb_tfidf</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_mnb</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">0.01</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;VECT&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>                                           
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
<span class="c1">#display(train_raw.head(1))</span>
<span class="c1">#display(test_raw.head(1))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Running kfold training with model mnb_tfidf
Shapes: x_train_raw.shape (19579, 6), y_train_raw.shape (19579,) , x_test_raw.shape (8392, 5)
Running preprocess: &lt;function run_mnb_tfidf at 0x7f64f5860e18&gt;
Shapes: x_train.shape (19579, 249062), y_train.shape (19579,) , x_test.shape (8392, 249062)
Running fold 1
Running fold 2
Running fold 3
Running fold 4
Running fold 5


===== Model: mnb_tfidf  ========:
 Cross-val log losses are: [0.37908346999014125, 0.37106918023241087, 0.37236379382643092, 0.37617292752863873, 0.36991758602498825]
====== Mean cross-val log loss is: 0.373721391520522 =========


</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Count vectorizer with MNB:</span>
<span class="k">def</span> <span class="nf">gen_count</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">,</span> <span class="n">x_test_text</span><span class="p">):</span>
    <span class="n">ngram</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">,</span> <span class="n">ngram_range</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">ngram</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">)</span>
    <span class="n">x_train_doc_term</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">)</span>
    <span class="n">x_test_doc_term</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_test_text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_train_doc_term</span><span class="p">,</span> <span class="n">x_test_doc_term</span>

<span class="c1"># Run mnb on tfidf</span>
<span class="k">def</span> <span class="nf">run_mnb_count</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Separate out text</span>
    <span class="n">x_train_text</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="n">x_test_text</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="c1"># Get tfidf doc term matrix</span>
    <span class="n">x_train_doc_term</span><span class="p">,</span> <span class="n">x_test_doc_term</span> <span class="o">=</span> <span class="n">gen_count</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">,</span> <span class="n">x_test_text</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">x_train_doc_term</span><span class="p">,</span> <span class="n">x_test_doc_term</span><span class="p">,</span> <span class="kc">None</span>

<span class="c1"># Multinomial Naive bayes algorithm</span>
<span class="k">def</span> <span class="nf">run_mnb</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">pred_val</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_val</span><span class="p">)</span>
    <span class="n">pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pred_val</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> 

<span class="c1"># Run MNB count and add predictions as features:</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;mnb_count&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_mnb_count</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_mnb</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">1.0</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;VECT&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>                                           
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">train_raw</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">display</span><span class="p">(</span><span class="n">test_raw</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Running kfold training with model mnb_count
Shapes: x_train_raw.shape (19579, 9), y_train_raw.shape (19579,) , x_test_raw.shape (8392, 8)
Running preprocess: &lt;function run_mnb_count at 0x7f64a2b12510&gt;
Shapes: x_train.shape (19579, 212575), y_train.shape (19579,) , x_test.shape (8392, 212575)
Running fold 1
Running fold 2
Running fold 3
Running fold 4
Running fold 5


===== Model: mnb_count  ========:
 Cross-val log losses are: [0.46875532098312317, 0.45116954365731066, 0.43660145956478208, 0.46107245572976219, 0.44702080768041885]
====== Mean cross-val log loss is: 0.45292391752307937 =========


</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>author</th>
      <th>num_punctuations</th>
      <th>num_stopwords</th>
      <th>num_words</th>
      <th>mnb_tfidf_eap</th>
      <th>mnb_tfidf_hpl</th>
      <th>mnb_tfidf_mws</th>
      <th>mnb_count_eap</th>
      <th>mnb_count_hpl</th>
      <th>mnb_count_mws</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id26305</td>
      <td>This process, however, afforded me no means of...</td>
      <td>EAP</td>
      <td>7</td>
      <td>19</td>
      <td>41</td>
      <td>0.942005</td>
      <td>0.01129</td>
      <td>0.046705</td>
      <td>0.999989</td>
      <td>0.000007</td>
      <td>0.000004</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>num_punctuations</th>
      <th>num_stopwords</th>
      <th>num_words</th>
      <th>mnb_tfidf_eap</th>
      <th>mnb_tfidf_hpl</th>
      <th>mnb_tfidf_mws</th>
      <th>mnb_count_eap</th>
      <th>mnb_count_hpl</th>
      <th>mnb_count_mws</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id02310</td>
      <td>Still, as I urged our leaving Ireland with suc...</td>
      <td>3</td>
      <td>9</td>
      <td>19</td>
      <td>0.07408</td>
      <td>0.004957</td>
      <td>0.920963</td>
      <td>0.038465</td>
      <td>0.000992</td>
      <td>0.960543</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[39]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tfidf vectorizer with MNB at char level:</span>
<span class="k">def</span> <span class="nf">gen_tfidf_char</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">,</span> <span class="n">x_test_text</span><span class="p">):</span>
    <span class="n">ngram</span> <span class="o">=</span> <span class="mi">7</span>
    <span class="c1">#vect = TfidfVectorizer(stop_words=&#39;english&#39;, ngram_range = (1,ngram)).fit(x_train)</span>
    <span class="n">vect</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">ngram_range</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">ngram</span><span class="p">),</span> <span class="n">analyzer</span><span class="o">=</span><span class="s1">&#39;char&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">)</span>
    <span class="n">x_train_doc_term</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">)</span>
    <span class="n">x_test_doc_term</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_test_text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_train_doc_term</span><span class="p">,</span> <span class="n">x_test_doc_term</span>

<span class="c1"># Run mnb on tfidf</span>
<span class="k">def</span> <span class="nf">run_mnb_tfidf_char</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Separate out text</span>
    <span class="n">x_train_text</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="n">x_test_text</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="c1"># Get tfidf doc term matrix</span>
    <span class="n">x_train_doc_term</span><span class="p">,</span> <span class="n">x_test_doc_term</span> <span class="o">=</span> <span class="n">gen_tfidf_char</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">,</span> <span class="n">x_test_text</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">x_train_doc_term</span><span class="p">,</span> <span class="n">x_test_doc_term</span><span class="p">,</span> <span class="kc">None</span>

<span class="c1"># Multinomial Naive bayes algorithm</span>
<span class="k">def</span> <span class="nf">run_mnb</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">pred_val</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_val</span><span class="p">)</span>
    <span class="n">pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pred_val</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span>  

<span class="c1"># Run MNB tfidf chars and add predictions as features:</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;mnb_tfidf_char&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_mnb_tfidf_char</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_mnb</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">0.03</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;VECT&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>                                           
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>


<span class="n">display</span><span class="p">(</span><span class="n">train_raw</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">display</span><span class="p">(</span><span class="n">test_raw</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Running kfold training with model mnb_tfidf_char
Shapes: x_train_raw.shape (19579, 12), y_train_raw.shape (19579,) , x_test_raw.shape (8392, 11)
Running preprocess: &lt;function run_mnb_tfidf_char at 0x7f64a2b7f6a8&gt;
Shapes: x_train.shape (19579, 1151052), y_train.shape (19579,) , x_test.shape (8392, 1151052)
Running fold 1
Running fold 2
Running fold 3
Running fold 4
Running fold 5


===== Model: mnb_tfidf_char  ========:
 Cross-val log losses are: [0.38824197963183649, 0.3890223276436871, 0.39628563464291233, 0.3847979523816813, 0.37640460557993577]
====== Mean cross-val log loss is: 0.3869504999760106 =========


</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>author</th>
      <th>num_punctuations</th>
      <th>num_stopwords</th>
      <th>num_words</th>
      <th>mnb_tfidf_eap</th>
      <th>mnb_tfidf_hpl</th>
      <th>mnb_tfidf_mws</th>
      <th>mnb_count_eap</th>
      <th>mnb_count_hpl</th>
      <th>mnb_count_mws</th>
      <th>mnb_tfidf_char_eap</th>
      <th>mnb_tfidf_char_hpl</th>
      <th>mnb_tfidf_char_mws</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id26305</td>
      <td>This process, however, afforded me no means of...</td>
      <td>EAP</td>
      <td>7</td>
      <td>19</td>
      <td>41</td>
      <td>0.942005</td>
      <td>0.01129</td>
      <td>0.046705</td>
      <td>0.999989</td>
      <td>0.000007</td>
      <td>0.000004</td>
      <td>0.999451</td>
      <td>0.000059</td>
      <td>0.00049</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>num_punctuations</th>
      <th>num_stopwords</th>
      <th>num_words</th>
      <th>mnb_tfidf_eap</th>
      <th>mnb_tfidf_hpl</th>
      <th>mnb_tfidf_mws</th>
      <th>mnb_count_eap</th>
      <th>mnb_count_hpl</th>
      <th>mnb_count_mws</th>
      <th>mnb_tfidf_char_eap</th>
      <th>mnb_tfidf_char_hpl</th>
      <th>mnb_tfidf_char_mws</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id02310</td>
      <td>Still, as I urged our leaving Ireland with suc...</td>
      <td>3</td>
      <td>9</td>
      <td>19</td>
      <td>0.07408</td>
      <td>0.004957</td>
      <td>0.920963</td>
      <td>0.038465</td>
      <td>0.000992</td>
      <td>0.960543</td>
      <td>0.00035</td>
      <td>0.000036</td>
      <td>0.999614</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[40]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Count vectorizer with Logistic regression:</span>
<span class="k">def</span> <span class="nf">gen_count</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">,</span> <span class="n">x_test_text</span><span class="p">):</span>
    <span class="n">ngram</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">ngram</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">)</span>
    <span class="n">x_train_doc_term</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">)</span>
    <span class="n">x_test_doc_term</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_test_text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_train_doc_term</span><span class="p">,</span> <span class="n">x_test_doc_term</span>

<span class="c1"># Run mnb on tfidf</span>
<span class="k">def</span> <span class="nf">run_lr_count</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Separate out text</span>
    <span class="n">x_train_text</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="n">x_test_text</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="c1"># Get tfidf doc term matrix</span>
    <span class="n">x_train_doc_term</span><span class="p">,</span> <span class="n">x_test_doc_term</span> <span class="o">=</span> <span class="n">gen_count</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">,</span> <span class="n">x_test_text</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">x_train_doc_term</span><span class="p">,</span> <span class="n">x_test_doc_term</span><span class="p">,</span> <span class="kc">None</span>

<span class="c1"># LR algorithm</span>
<span class="k">def</span> <span class="nf">run_lr</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">pred_val</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_val</span><span class="p">)</span>
    <span class="n">pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pred_val</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span>  

<span class="c1"># Run LR count and add predictions as features:</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;lr_count&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_lr_count</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_lr</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="mf">1.0</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;VECT&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>                                           
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>

<span class="c1">#display(train_raw.head(1))</span>
<span class="c1">#display(test_raw.head(1))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Running kfold training with model lr_count
Shapes: x_train_raw.shape (19579, 15), y_train_raw.shape (19579,) , x_test_raw.shape (8392, 14)
Running preprocess: &lt;function run_lr_count at 0x7f649b474158&gt;
Shapes: x_train.shape (19579, 249062), y_train.shape (19579,) , x_test.shape (8392, 249062)
Running fold 1
Running fold 2
Running fold 3
Running fold 4
Running fold 5


===== Model: lr_count  ========:
 Cross-val log losses are: [0.4636133436069198, 0.46855246411426921, 0.4412857462490028, 0.45436025990786655, 0.44846678129836448]
====== Mean cross-val log loss is: 0.45525571903528456 =========


</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="XGBoost-model-definition-and-Run-Stacking-model:">XGBoost model definition and Run Stacking model:<a class="anchor-link" href="#XGBoost-model-definition-and-Run-Stacking-model:">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[41]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">## Display the columns in the training and test data at this point</span>
<span class="n">display</span><span class="p">(</span><span class="n">train_raw</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">display</span><span class="p">(</span><span class="n">test_raw</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>author</th>
      <th>num_punctuations</th>
      <th>num_stopwords</th>
      <th>num_words</th>
      <th>mnb_tfidf_eap</th>
      <th>mnb_tfidf_hpl</th>
      <th>mnb_tfidf_mws</th>
      <th>mnb_count_eap</th>
      <th>mnb_count_hpl</th>
      <th>mnb_count_mws</th>
      <th>mnb_tfidf_char_eap</th>
      <th>mnb_tfidf_char_hpl</th>
      <th>mnb_tfidf_char_mws</th>
      <th>lr_count_eap</th>
      <th>lr_count_hpl</th>
      <th>lr_count_mws</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id26305</td>
      <td>This process, however, afforded me no means of...</td>
      <td>EAP</td>
      <td>7</td>
      <td>19</td>
      <td>41</td>
      <td>0.942005</td>
      <td>0.01129</td>
      <td>0.046705</td>
      <td>0.999989</td>
      <td>0.000007</td>
      <td>0.000004</td>
      <td>0.999451</td>
      <td>0.000059</td>
      <td>0.00049</td>
      <td>0.996793</td>
      <td>0.001718</td>
      <td>0.001489</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>num_punctuations</th>
      <th>num_stopwords</th>
      <th>num_words</th>
      <th>mnb_tfidf_eap</th>
      <th>mnb_tfidf_hpl</th>
      <th>mnb_tfidf_mws</th>
      <th>mnb_count_eap</th>
      <th>mnb_count_hpl</th>
      <th>mnb_count_mws</th>
      <th>mnb_tfidf_char_eap</th>
      <th>mnb_tfidf_char_hpl</th>
      <th>mnb_tfidf_char_mws</th>
      <th>lr_count_eap</th>
      <th>lr_count_hpl</th>
      <th>lr_count_mws</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id02310</td>
      <td>Still, as I urged our leaving Ireland with suc...</td>
      <td>3</td>
      <td>9</td>
      <td>19</td>
      <td>0.07408</td>
      <td>0.004957</td>
      <td>0.920963</td>
      <td>0.038465</td>
      <td>0.000992</td>
      <td>0.960543</td>
      <td>0.00035</td>
      <td>0.000036</td>
      <td>0.999614</td>
      <td>0.173632</td>
      <td>0.008389</td>
      <td>0.817979</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[42]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">### Run XGboost on features so far ###</span>

<span class="c1"># Drop the necessary columns</span>
<span class="k">def</span> <span class="nf">run_xgb_preprocess</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    
    <span class="n">features_drop</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;features_drop&#39;</span><span class="p">]</span>
    
    <span class="c1"># train data</span>
    <span class="n">drop_columns_train</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span><span class="s1">&#39;text&#39;</span><span class="p">,</span><span class="s1">&#39;author&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">features_drop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">drop_columns_train</span> <span class="o">=</span> <span class="n">drop_columns_train</span> <span class="o">+</span> <span class="n">features_drop</span>
    <span class="n">x_train_features</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">drop_columns_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="c1"># test data</span>
    <span class="n">drop_columns_test</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">features_drop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">drop_columns_test</span> <span class="o">=</span> <span class="n">drop_columns_test</span> <span class="o">+</span> <span class="n">features_drop</span>
    <span class="n">x_test_features</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">drop_columns_test</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>    
    
    <span class="k">return</span> <span class="n">x_train_features</span><span class="p">,</span> <span class="n">x_test_features</span><span class="p">,</span> <span class="kc">None</span>

<span class="c1">#####################</span>
<span class="c1">## XGboost:</span>
<span class="c1">#####################</span>
<span class="k">def</span> <span class="nf">run_xgb</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> 
                      <span class="n">val_X</span><span class="p">,</span> <span class="n">val_y</span><span class="p">,</span> 
                      <span class="n">test_X</span><span class="p">,</span> 
                      <span class="n">model_params</span><span class="p">):</span>
    
    <span class="c1"># Extra params</span>
    <span class="n">seed_val</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;seed_val&#39;</span><span class="p">]</span>
    <span class="n">child</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;child&#39;</span><span class="p">]</span>
    <span class="n">colsample</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;colsample&#39;</span><span class="p">]</span>
    
    <span class="c1"># Params list</span>
    <span class="n">param</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">param</span><span class="p">[</span><span class="s1">&#39;objective&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;multi:softprob&#39;</span>
    <span class="n">param</span><span class="p">[</span><span class="s1">&#39;eta&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">param</span><span class="p">[</span><span class="s1">&#39;max_depth&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">param</span><span class="p">[</span><span class="s1">&#39;silent&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">param</span><span class="p">[</span><span class="s1">&#39;num_class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">param</span><span class="p">[</span><span class="s1">&#39;eval_metric&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mlogloss&quot;</span>
    <span class="n">param</span><span class="p">[</span><span class="s1">&#39;min_child_weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">child</span>
    <span class="n">param</span><span class="p">[</span><span class="s1">&#39;subsample&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.8</span>
    <span class="n">param</span><span class="p">[</span><span class="s1">&#39;colsample_bytree&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">colsample</span>
    <span class="n">param</span><span class="p">[</span><span class="s1">&#39;seed&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">seed_val</span>
    <span class="n">early_stopping_rounds</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">num_rounds</span> <span class="o">=</span> <span class="mi">2000</span>
    <span class="n">param_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
   
    <span class="c1"># Train </span>
    <span class="n">d_train</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="n">train_y</span><span class="p">)</span>
    <span class="n">d_val</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">val_X</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="n">val_y</span><span class="p">)</span>
    <span class="n">watchlist</span> <span class="o">=</span> <span class="p">[</span> <span class="p">(</span><span class="n">d_train</span><span class="p">,</span><span class="s1">&#39;train&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">d_val</span><span class="p">,</span> <span class="s1">&#39;cross-valid&#39;</span><span class="p">)</span> <span class="p">]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">param_list</span><span class="p">,</span> <span class="n">d_train</span><span class="p">,</span> <span class="n">num_rounds</span><span class="p">,</span> 
                      <span class="n">watchlist</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span> <span class="n">early_stopping_rounds</span><span class="p">,</span> 
                      <span class="n">verbose_eval</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    
    <span class="c1"># Get validation predictions </span>
    <span class="c1">#pred_train = model.predict(d_train, </span>
    <span class="c1">#                            ntree_limit = model.best_ntree_limit)</span>
    <span class="n">pred_val</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">d_val</span><span class="p">,</span> 
                                <span class="n">ntree_limit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">best_ntree_limit</span><span class="p">)</span>
    
    <span class="c1"># Get test set predictions.</span>
    <span class="n">d_test</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">test_X</span><span class="p">)</span>
    <span class="n">pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">d_test</span><span class="p">,</span> 
                                 <span class="n">ntree_limit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">best_ntree_limit</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pred_val</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span>

<span class="c1"># Run XGboost and add predictions as features:</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;xgboost_stacking_1&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_xgb_preprocess</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_xgb</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;seed_val&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;child&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;colsample&#39;</span><span class="p">:</span> <span class="mf">0.3</span> <span class="p">}</span>
<span class="c1">#features_drop = [&#39;num_punctuations&#39;,&#39;num_stopwords&#39;,&#39;num_words&#39;]</span>
<span class="n">features_drop</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;features_drop&#39;</span><span class="p">:</span> <span class="n">features_drop</span> <span class="p">}</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;VECT&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> 
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="n">xgb</span><span class="o">.</span><span class="n">plot_importance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Running kfold training with model xgboost_stacking_1
Shapes: x_train_raw.shape (19579, 18), y_train_raw.shape (19579,) , x_test_raw.shape (8392, 17)
Running preprocess: &lt;function run_xgb_preprocess at 0x7f64a2b12158&gt;
Shapes: x_train.shape (19579, 15), y_train.shape (19579,) , x_test.shape (8392, 15)
Running fold 1
[0]	train-mlogloss:0.999664	cross-valid-mlogloss:1.00074
Multiple eval metrics have been passed: &#39;cross-valid-mlogloss&#39; will be used for early stopping.

Will train until cross-valid-mlogloss hasn&#39;t improved in 50 rounds.
[20]	train-mlogloss:0.383147	cross-valid-mlogloss:0.395538
[40]	train-mlogloss:0.308226	cross-valid-mlogloss:0.325671
[60]	train-mlogloss:0.290174	cross-valid-mlogloss:0.31099
[80]	train-mlogloss:0.281545	cross-valid-mlogloss:0.306877
[100]	train-mlogloss:0.275509	cross-valid-mlogloss:0.305253
[120]	train-mlogloss:0.269675	cross-valid-mlogloss:0.304423
[140]	train-mlogloss:0.264935	cross-valid-mlogloss:0.304101
[160]	train-mlogloss:0.260306	cross-valid-mlogloss:0.303982
[180]	train-mlogloss:0.256018	cross-valid-mlogloss:0.304124
[200]	train-mlogloss:0.251718	cross-valid-mlogloss:0.303537
[220]	train-mlogloss:0.247239	cross-valid-mlogloss:0.303689
[240]	train-mlogloss:0.243359	cross-valid-mlogloss:0.303824
Stopping. Best iteration:
[203]	train-mlogloss:0.251067	cross-valid-mlogloss:0.303406

Running fold 2
[0]	train-mlogloss:0.999933	cross-valid-mlogloss:1.00036
Multiple eval metrics have been passed: &#39;cross-valid-mlogloss&#39; will be used for early stopping.

Will train until cross-valid-mlogloss hasn&#39;t improved in 50 rounds.
[20]	train-mlogloss:0.383909	cross-valid-mlogloss:0.392427
[40]	train-mlogloss:0.308679	cross-valid-mlogloss:0.323086
[60]	train-mlogloss:0.290174	cross-valid-mlogloss:0.309287
[80]	train-mlogloss:0.281121	cross-valid-mlogloss:0.305519
[100]	train-mlogloss:0.275056	cross-valid-mlogloss:0.304575
[120]	train-mlogloss:0.269389	cross-valid-mlogloss:0.304092
[140]	train-mlogloss:0.264492	cross-valid-mlogloss:0.304471
[160]	train-mlogloss:0.259924	cross-valid-mlogloss:0.304657
Stopping. Best iteration:
[111]	train-mlogloss:0.27183	cross-valid-mlogloss:0.303924

Running fold 3
[0]	train-mlogloss:1.00009	cross-valid-mlogloss:1.00079
Multiple eval metrics have been passed: &#39;cross-valid-mlogloss&#39; will be used for early stopping.

Will train until cross-valid-mlogloss hasn&#39;t improved in 50 rounds.
[20]	train-mlogloss:0.384732	cross-valid-mlogloss:0.390998
[40]	train-mlogloss:0.309888	cross-valid-mlogloss:0.317788
[60]	train-mlogloss:0.291875	cross-valid-mlogloss:0.30292
[80]	train-mlogloss:0.282877	cross-valid-mlogloss:0.298574
[100]	train-mlogloss:0.276846	cross-valid-mlogloss:0.297413
[120]	train-mlogloss:0.271184	cross-valid-mlogloss:0.297027
[140]	train-mlogloss:0.266093	cross-valid-mlogloss:0.296948
[160]	train-mlogloss:0.261045	cross-valid-mlogloss:0.297053
[180]	train-mlogloss:0.25676	cross-valid-mlogloss:0.296568
[200]	train-mlogloss:0.252349	cross-valid-mlogloss:0.29664
[220]	train-mlogloss:0.248352	cross-valid-mlogloss:0.296514
Stopping. Best iteration:
[184]	train-mlogloss:0.256062	cross-valid-mlogloss:0.296448

Running fold 4
[0]	train-mlogloss:1.00058	cross-valid-mlogloss:1.00069
Multiple eval metrics have been passed: &#39;cross-valid-mlogloss&#39; will be used for early stopping.

Will train until cross-valid-mlogloss hasn&#39;t improved in 50 rounds.
[20]	train-mlogloss:0.384244	cross-valid-mlogloss:0.390903
[40]	train-mlogloss:0.308641	cross-valid-mlogloss:0.32172
[60]	train-mlogloss:0.290232	cross-valid-mlogloss:0.308447
[80]	train-mlogloss:0.281686	cross-valid-mlogloss:0.305206
[100]	train-mlogloss:0.275316	cross-valid-mlogloss:0.303804
[120]	train-mlogloss:0.26974	cross-valid-mlogloss:0.303346
[140]	train-mlogloss:0.264704	cross-valid-mlogloss:0.303374
[160]	train-mlogloss:0.260258	cross-valid-mlogloss:0.303342
[180]	train-mlogloss:0.25579	cross-valid-mlogloss:0.303336
[200]	train-mlogloss:0.251712	cross-valid-mlogloss:0.30332
Stopping. Best iteration:
[164]	train-mlogloss:0.259251	cross-valid-mlogloss:0.303142

Running fold 5
[0]	train-mlogloss:0.99835	cross-valid-mlogloss:0.998813
Multiple eval metrics have been passed: &#39;cross-valid-mlogloss&#39; will be used for early stopping.

Will train until cross-valid-mlogloss hasn&#39;t improved in 50 rounds.
[20]	train-mlogloss:0.382279	cross-valid-mlogloss:0.386316
[40]	train-mlogloss:0.308401	cross-valid-mlogloss:0.316768
[60]	train-mlogloss:0.291033	cross-valid-mlogloss:0.304198
[80]	train-mlogloss:0.282189	cross-valid-mlogloss:0.30023
[100]	train-mlogloss:0.275981	cross-valid-mlogloss:0.299054
[120]	train-mlogloss:0.270792	cross-valid-mlogloss:0.298777
[140]	train-mlogloss:0.265665	cross-valid-mlogloss:0.298761
[160]	train-mlogloss:0.260892	cross-valid-mlogloss:0.299008
Stopping. Best iteration:
[123]	train-mlogloss:0.270081	cross-valid-mlogloss:0.298649



===== Model: xgboost_stacking_1  ========:
 Cross-val log losses are: [0.30340605295119594, 0.30392434009540742, 0.29644798686117763, 0.30314216150289319, 0.29864875083094305]
====== Mean cross-val log loss is: 0.30111385844832345 =========


</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAysAAALJCAYAAAC0rLxhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Cell-used-for-initial-training-to-search-for-good-models-and-hyperparameters:---Disabled-now">Cell used for initial training to search for good models and hyperparameters: - Disabled now<a class="anchor-link" href="#Cell-used-for-initial-training-to-search-for-good-models-and-hyperparameters:---Disabled-now">&#182;</a></h3><ul>
<li>Split the data into training and test set</li>
<li>Tokenize the input and try Count vectorizer, Tf-idf vectorizer</li>
<li>Try MNB, LR and GridSearchCV for hyperparameters.</li>
<li>Try Stemming + Lemmatize, with / without stopwords, various n-grams for Tf-idf etc.</li>
</ul>

</div>
</div>
</div>## Disable this cell:
if False:

    ########################################################################################
    # Create inputs for training and Split into train and test sets 
    ########################################################################################
    X_train = train_raw['text']
    #y_tr = train_raw['author']

    # Split into train and test data (0.05 -> 0.2 -> .02)
    X_train, X_test, y_train, y_test = train_test_split(X_tr,
                                                        y_tr,
                                                        test_size=0.1,
                                                        stratify=y_tr,
                                                        random_state = 2)

    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)
    print(X_train.head())
    print(y_train[0:20])
    display(train_raw.loc[[17794,13109,16951,11267,11267]])

    ## Exclude stopwords

    ngram = 1

    # Count vectorizer
    vect = CountVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)
    #print(len(vect.get_feature_names()))
    #print(vect.get_feature_names()[::100])
    # Transform reg vectorizer
    X_train_doc_term = vect.transform(X_train)
    print("Count doc_term_matrix shape with {}-gram features is {}\n".format(ngram, X_train_doc_term.shape))

    # Tf idf vectorizer
    tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)
    #print(len(tfidf_vect.get_feature_names()))
    print(tfidf_vect.get_feature_names()[::200])
    print(tfidf_vect.get_feature_names()[:30])
    # Transform tfidf vectorizer
    X_train_doc_term_tfidf = tfidf_vect.transform(X_train)
    print("Tf-idf count doc_term_matrix shape with {}-gram features is {}\n".format(ngram, X_train_doc_term_tfidf.shape))


    ###############################################################################

    ## Include stop words

    ngram = 1

    # Count vectorizer
    vect = CountVectorizer(ngram_range = (1,ngram)).fit(X_train)
    #print(len(vect.get_feature_names()))
    #print(vect.get_feature_names()[::500])
    # Transform reg vectorizer
    X_train_doc_term = vect.transform(X_train)
    print("Count doc_term_matrix shape with {}-gram features is {}\n".format(ngram, X_train_doc_term.shape))

    # Tf idf vectorizer
    tfidf_vect = TfidfVectorizer(ngram_range = (1,ngram)).fit(X_train)
    #print(len(tfidf_vect.get_feature_names()))
    print(tfidf_vect.get_feature_names()[::200])
    print(tfidf_vect.get_feature_names()[:30])
    # Transform tfidf vectorizer
    X_train_doc_term_tfidf = tfidf_vect.transform(X_train)
    print("Tf-idf count doc_term_matrix shape with {}-gram features is {}\n".format(ngram, X_train_doc_term_tfidf.shape))


    ########################################################################################
    Add Stemming and Lemmatization
    ########################################################################################

    # Stem and Lemmatize the tokens since sklearn doesnt support it.
    # Idea borrowed from https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial

    from nltk.stem import WordNetLemmatizer
    from nltk.stem import PorterStemmer

    stemmer = PorterStemmer()
    lemmatizer = WordNetLemmatizer()
    class LCountVectorizer(CountVectorizer):
        def build_analyzer(self):
            analyzer= super(LCountVectorizer, self).build_analyzer()
            return lambda document: (lemmatizer.lemmatize(stemmer.stem(token)) for token in analyzer(document))

    class LTfidfVectorizer(TfidfVectorizer):
        def build_analyzer(self):
            analyzer= super(LTfidfVectorizer, self).build_analyzer()
            return lambda document: (lemmatizer.lemmatize(stemmer.stem(token)) for token in analyzer(document))

    ngram = 1

    if False:
        # Count vectorizer
        vect = LCountVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)
        #print(len(vect.get_feature_names()))
        #print(vect.get_feature_names()[::100])
        # Transform reg vectorizer
        X_train_doc_term = vect.transform(X_train)
        print("Count doc_term_matrix shape with {}-gram features is {}\n".format(ngram, X_train_doc_term.shape))

        # Tf idf vectorizer
        tfidf_vect = LTfidfVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)
        #print(len(tfidf_vect.get_feature_names()))
        print(tfidf_vect.get_feature_names()[::200])
        print(tfidf_vect.get_feature_names()[:30])
        # Transform tfidf vectorizer
        X_train_doc_term_tfidf = tfidf_vect.transform(X_train)
        print("Tf-idf count doc_term_matrix shape with {}-gram features is {}\n".format(ngram, X_train_doc_term_tfidf.shape))

    ###############################################################################################
    # Grid search Count vect / Tfidf vect 
    # with/without stop words
    # with/without stemlemmatize
    # Logistic regression and Multinomial NB
    # For various ngrams
    ###############################################################################################

    #ngrams = [1,2,3,4]
    ngrams = [7]

    count_based = 1
    tf_idf_based = 1
    LR = 0
    MNB = 1
    char_based = 1

    for ngram in ngrams:

        print("\n\n\n\n ======= ngram used is {} ========= \n\n\n\n".format(ngram))

        if count_based:
            ##---------------------------------------
            ## -- Count vectorizer without stopwords --
            ##---------------------------------------
            # Stem+Lemmatize version
            #vect = LCountVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)
            # No Stem+Lemmatize i.e. default version
            if char_based:
                vect = CountVectorizer(stop_words='english', analyzer = 'char', ngram_range = (1,ngram)).fit(X_train)
            else:
                vect = CountVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)

            ##---------------------------------------
            ## -- Count vectorizer with stopwords --
            ##---------------------------------------
            # Stem+Lemmatize version
            #vect = LCountVectorizer(ngram_range = (1,ngram)).fit(X_train)
            # No Stem+Lemmatize i.e. default version
            #vect = CountVectorizer(ngram_range = (1,ngram)).fit(X_train)
            ##---------------------------------------
            print(len(vect.get_feature_names()))
            print(vect.get_feature_names()[::2000])
            # Transform reg vectorizer
            X_train_doc_term = vect.transform(X_train)
            print("Count doc_term_matrix shape with {}-gram features is {}\n".format(ngram, X_train_doc_term.shape))

        if tf_idf_based:
            ##---------------------------------------
            ## Tf idf vectorizer without stopwords --
            ##---------------------------------------
            # Stem+Lemmatize version
            #tfidf_vect = LTfidfVectorizer(stop_words='english', ngram_range =(1,ngram)).fit(X_train)
            # No Stem+Lemmatize i.e. default version
            if char_based:
                tfidf_vect = TfidfVectorizer(stop_words='english', analyzer = 'char', ngram_range = (1,ngram)).fit(X_train)
            else:
                tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)
            #---------------------------------------
            ##---------------------------------------
            ## Tf idf vectorizer with stopwords --
            ##--------------------------------------
            # Stem+Lemmatize version
            #tfidf_vect = LTfidfVectorizer(ngram_range =(1,ngram)).fit(X_train)
            # No Stem+Lemmatize i.e. default version
            #tfidf_vect = TfidfVectorizer(ngram_range = (1,ngram)).fit(X_train)
            ##---------------------------------------
            print(len(tfidf_vect.get_feature_names()))
            print(tfidf_vect.get_feature_names()[::2000])
            # Transform tfidf vectorizer
            X_train_doc_term_tfidf = tfidf_vect.transform(X_train)
            print("Tf-idf count doc_term_matrix shape with {}-gram features is {}\n".format(ngram, X_train_doc_term_tfidf.shape))


        if LR:
            ##########################
            ## Logistic regression
            ##########################
            if count_based:
                # Count vectorizer
                param_grid = {'C': [0.1, 1.0, 10.0, 100.0]}
                model = LogisticRegression()
                print(model)
                skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)
                grid_obj = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'neg_log_loss', cv = skfold, verbose = 1)
                grid_obj_fit = grid_obj.fit(X_train_doc_term,y_train)
                best_model = grid_obj_fit.best_estimator_
                cv_results = grid_obj_fit.cv_results_
                for mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):
                    print(params, mean_score)
                predictions_prob = best_model.predict_proba(tfidf_vect.transform(X_test))
                logloss = log_loss(y_test,predictions_prob)
                print("best params is " + str(grid_obj_fit.best_params_))
                print("\n=== logloss on trainset for logistic regression & count vectorizer: {:.5f}=== \n".format(grid_obj_fit.best_score_))
                print("\n=== logloss on testset for logistic regression & count vectorizer: {:.5f}=== \n".format(logloss))

            if tf_idf_based:
                # Tf-idf vectorizer
                param_grid = {'C': [0.1, 1.0, 10.0, 100.0]}
                model = LogisticRegression()
                skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)
                grid_obj = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'neg_log_loss', cv = skfold, verbose = 1)
                grid_obj_fit = grid_obj.fit(X_train_doc_term_tfidf,y_train)
                best_model = grid_obj_fit.best_estimator_
                cv_results = grid_obj_fit.cv_results_
                for mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):
                    print(params, mean_score)
                predictions_prob = best_model.predict_proba(tfidf_vect.transform(X_test))
                logloss = log_loss(y_test,predictions_prob)
                print("best params is " + str(grid_obj_fit.best_params_))
                print("\n=== logloss on trainset for logistic regression & tf-idf vectorizer: {:.5f}=== \n".format(grid_obj_fit.best_score_))
                print("\n=== logloss on testset for logistic regression & tf-idf vectorizer: {:.5f}=== \n".format(logloss))


        if MNB:
            ####################################################
            ## Multinomial Naive Bayes 
            ####################################################

            if count_based:
                # Count vectorizer
                param_grid = {'alpha': [0.01, 0.03, 0.1, 0.3, 0.6, 1.0]}
                #scoring = make_scorer(log_loss, labels = np.array([0,1,2]))
                model = MultinomialNB()
                skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)
                #grid_obj = GridSearchCV(estimator = model, param_grid = param_grid, scoring = scoring)
                grid_obj = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'neg_log_loss', cv = skfold, verbose = 1)
                grid_obj_fit = grid_obj.fit(X_train_doc_term,y_train)
                best_model = grid_obj_fit.best_estimator_
                cv_results = grid_obj_fit.cv_results_
                for mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):
                    print(params, mean_score)
                #print(pd.DataFrame(grid_obj_fit.cv_results_))
                predictions_prob = best_model.predict_proba(vect.transform(X_test))
                logloss = log_loss(y_test,predictions_prob)
                print("best params is " + str(grid_obj_fit.best_params_))
                print("\n=== logloss on trainset for MNB & count vectorizer: {:.5f}=== \n".format(grid_obj_fit.best_score_))
                print("\n=== logloss on testset for MNB & count vectorizer: {:.5f}=== \n".format(logloss))

            if tf_idf_based:
                # Tf-idf vectorizer
                param_grid = {'alpha': [0.01, 0.03, 0.1, 0.3, 0.6, 1.0]}
                model = MultinomialNB()
                skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)
                grid_obj = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'neg_log_loss', cv = skfold, verbose = 1)
                grid_obj_fit = grid_obj.fit(X_train_doc_term_tfidf,y_train)
                best_model = grid_obj_fit.best_estimator_
                cv_results = grid_obj_fit.cv_results_
                for mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):
                    print(params, mean_score)
                predictions_prob = best_model.predict_proba(tfidf_vect.transform(X_test))
                logloss = log_loss(y_test,predictions_prob)
                print("best params is " + str(grid_obj_fit.best_params_))
                print("\n=== logloss on trainset for MNB & tf idf vectorizer: {:.5f}=== \n".format(grid_obj_fit.best_score_))
                print("\n=== logloss on testset for MNB & tf idf vectorizer: {:.5f}=== \n".format(logloss))
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Word-embeddings-+-Neural-networks-/-Deep-learning">Word embeddings + Neural networks / Deep learning<a class="anchor-link" href="#Word-embeddings-+-Neural-networks-/-Deep-learning">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[43]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Convert labels to one hot to train NN</span>
<span class="n">y_train_one_hot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train_one_hot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_train_one_hot</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>(19579, 3)
0 [ 1.  0.  0.]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="The-following-cells-are-used-for-pre-processing-text-for-NN-networks">The following cells are used for pre-processing text for NN networks<a class="anchor-link" href="#The-following-cells-are-used-for-pre-processing-text-for-NN-networks">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[44]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">###################################</span>
<span class="c1"># Function to create word2vec from </span>
<span class="c1"># given texts using gensim</span>
<span class="c1">###################################</span>

<span class="k">def</span> <span class="nf">create_gensim_wordvec</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">wordvecsize</span><span class="p">):</span>
    
    <span class="kn">from</span> <span class="nn">gensim.models</span> <span class="k">import</span> <span class="n">Word2Vec</span>
    <span class="kn">from</span> <span class="nn">gensim.utils</span> <span class="k">import</span> <span class="n">simple_preprocess</span>
    
    <span class="n">ngrams_max</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">x_train</span><span class="p">:</span>
        <span class="n">simple</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">simple</span><span class="p">:</span>
            <span class="c1"># only tokenize</span>
            <span class="c1">#print(sent)</span>
            <span class="n">sent_temp</span> <span class="o">=</span> <span class="n">simple_preprocess</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>
            <span class="n">sentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sent_temp</span><span class="p">)</span>
            <span class="c1">#print(sent_temp)</span>
            <span class="c1">#break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># separate punctuation</span>
            <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&#39; &quot;</span><span class="p">,</span> <span class="s2">&quot; &#39; &quot;</span><span class="p">)</span>
            <span class="n">punct</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="s1">&#39;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[</span><span class="se">\\</span><span class="s1">]^_`{|}~&#39;</span><span class="p">)</span>
            <span class="n">prods</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">punct</span>
            <span class="c1">#print(&quot;sent is {}&quot;.format(sent))</span>
            <span class="c1">##print(&quot;punct is {}&quot;.format(punct))</span>
            <span class="c1">#print(&quot;prods is {}&quot;.format(prods))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">prods</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">for</span> <span class="n">sign</span> <span class="ow">in</span> <span class="n">prods</span><span class="p">:</span>
                <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">sign</span><span class="p">,</span> <span class="s1">&#39; </span><span class="si">{}</span><span class="s1"> &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sign</span><span class="p">))</span>
            <span class="c1">#sent = sent.split()</span>
            
            <span class="c1"># Add n-grams</span>
            <span class="n">sent_words</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
            <span class="n">ngrams</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">nidx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ngrams_max</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">widx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sent_words</span><span class="p">)</span> <span class="o">-</span> <span class="n">nidx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>                    
                    <span class="n">ngrams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;--&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sent_words</span><span class="p">[</span><span class="n">widx</span><span class="p">:</span><span class="n">widx</span><span class="o">+</span><span class="n">nidx</span><span class="p">]))</span>
                    <span class="c1">#break</span>
            <span class="n">ngrams_string</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ngrams</span><span class="p">)</span>
            <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">ngrams_string</span>
            <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
            <span class="n">sentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>
            <span class="c1">#print(sent)</span>
            <span class="c1">#break</span>
            
    <span class="nb">print</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">wordvecsize</span> <span class="o">=</span> <span class="n">wordvecsize</span>
    <span class="n">gensim_model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> 
                            <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">wordvecsize</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Get some details</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">gensim_model</span><span class="p">)</span>
    <span class="c1"># Get the most and least common words</span>
    <span class="n">vocab_size_gensim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">vocab_size_gensim</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> 
          <span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
          <span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">vocab_size_gensim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> 
          <span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">vocab_size_gensim</span> <span class="o">-</span> <span class="mi">2</span><span class="p">],</span> 
          <span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">vocab_size_gensim</span> <span class="o">-</span> <span class="mi">3</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Index of &quot;of&quot; is: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;of&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;the&#39;</span><span class="p">])</span>

    <span class="c1"># Write out vectors file</span>
    <span class="c1">#weights = gensim_model.syn0 </span>
    <span class="n">gensim_embeddings</span> <span class="o">=</span> <span class="s1">&#39;../scratch/gensim.&#39;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">wordvecsize</span><span class="p">)</span> <span class="o">+</span><span class="s1">&#39;.txt&#39;</span>
    <span class="n">word_vectors</span> <span class="o">=</span> <span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span>
    <span class="n">word_vectors</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">gensim_embeddings</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">gensim_embeddings</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">gensim_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
        <span class="c1">#print(&quot;{} {}&quot;.format(w,v))</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">vi</span> <span class="ow">in</span> <span class="n">v</span><span class="p">:</span>
            <span class="c1">#print(&quot;{} &quot;.format(vi))</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">vi</span><span class="p">))</span>
        <span class="c1">#break</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="c1"># Disabled here</span>
<span class="k">if</span> <span class="kc">False</span><span class="p">:</span>
    <span class="c1">###################################</span>
    <span class="c1"># Create gensim wordvectors from given text </span>
    <span class="c1">###################################</span>
    <span class="n">create_gensim_wordvectors</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">create_gensim_wordvectors</span><span class="p">:</span>
        <span class="n">wordvecsize</span> <span class="o">=</span> <span class="mi">20</span>
        <span class="n">create_gensim_wordvec</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">wordvecsize</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[45]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">######################################################</span>
<span class="c1"># Function to Preprocess to separate out punctuation and n-grams </span>
<span class="c1"># since Tokenizer isnt doing good.</span>
<span class="c1">#######################################################</span>
<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">x_train</span><span class="p">):</span>
    
    <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">separate_punctuation</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># Fast text: ngrams_max = 2</span>
    <span class="n">ngrams_max</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">x_train</span><span class="p">:</span>
        <span class="c1">#print(type(sent))</span>

        <span class="k">if</span> <span class="n">separate_punctuation</span><span class="p">:</span>
            <span class="c1"># Separate punctuation</span>
            <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&#39; &quot;</span><span class="p">,</span> <span class="s2">&quot; &#39; &quot;</span><span class="p">)</span>
            <span class="n">punct</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="s1">&#39;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[</span><span class="se">\\</span><span class="s1">]^_`{|}~&#39;</span><span class="p">)</span>
            <span class="n">prods</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">punct</span>
            <span class="c1">#print(&quot;sent is {}&quot;.format(sent))</span>
            <span class="c1">##print(&quot;punct is {}&quot;.format(punct))</span>
            <span class="c1">#print(&quot;prods is {}&quot;.format(prods))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">prods</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">for</span> <span class="n">sign</span> <span class="ow">in</span> <span class="n">prods</span><span class="p">:</span>
                <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">sign</span><span class="p">,</span> <span class="s1">&#39; </span><span class="si">{}</span><span class="s1"> &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sign</span><span class="p">))</span>
            <span class="c1">#print(&quot;new sent is {}&quot;.format(sent))</span>
            <span class="n">x_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sent</span>

        <span class="k">if</span> <span class="kc">False</span><span class="p">:</span>
            <span class="c1"># Remove punctuation before ngrams</span>
            <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&#39; &quot;</span><span class="p">,</span> <span class="s2">&quot; &#39; &quot;</span><span class="p">)</span>
            <span class="n">punct</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="s1">&#39;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[</span><span class="se">\\</span><span class="s1">]^_`{|}~&#39;</span><span class="p">)</span>
            <span class="n">prods</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">punct</span>
            <span class="c1">#print(&quot;sent is {}&quot;.format(sent))</span>
            <span class="c1">##print(&quot;punct is {}&quot;.format(punct))</span>
            <span class="c1">#print(&quot;prods is {}&quot;.format(prods))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">prods</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">for</span> <span class="n">sign</span> <span class="ow">in</span> <span class="n">prods</span><span class="p">:</span>
                <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">sign</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sign</span><span class="p">))</span>
            <span class="c1">#print(&quot;new sent is {}&quot;.format(sent))</span>
            <span class="n">x_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sent</span>
            
        <span class="k">if</span> <span class="n">ngrams_max</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Add n-grams</span>
            <span class="n">sent_words</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
            <span class="n">ngrams</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">nidx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ngrams_max</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">widx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sent_words</span><span class="p">)</span> <span class="o">-</span> <span class="n">nidx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>                    
                    <span class="n">ngrams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;--&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sent_words</span><span class="p">[</span><span class="n">widx</span><span class="p">:</span><span class="n">widx</span><span class="o">+</span><span class="n">nidx</span><span class="p">]))</span>
                    <span class="c1">#break</span>
            <span class="n">ngrams_string</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ngrams</span><span class="p">)</span>
            <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">ngrams_string</span>
            <span class="c1">#print(&quot;new sent is {}&quot;.format(sent))</span>
            <span class="n">x_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sent</span>

        <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1">#break</span>
        
    <span class="k">return</span> <span class="n">x_train</span>

<span class="c1"># Disabled code</span>
<span class="k">if</span> <span class="kc">False</span><span class="p">:</span>
    <span class="c1">###################################</span>
    <span class="c1"># Preprocess X_train and X_test data:</span>
    <span class="c1"># Separate punctuation, add ngrams</span>
    <span class="c1">###################################</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1">#print(X_train_orig.head())</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[46]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">## Convert text to integers that can be input to Embedding layer</span>

<span class="k">def</span> <span class="nf">create_tokenizer</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">char_level</span><span class="p">):</span>
    
    <span class="c1"># This is for word vectors to be learnt by embedding layer</span>
    <span class="k">if</span> <span class="n">char_level</span><span class="p">:</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">char_level</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
        <span class="c1">#tokenizer = Tokenizer()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

    <span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">sentences_encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No of unique words found is &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">))</span>

    <span class="n">min_num_occ</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">num_words_atleastocc</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">c</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_counts</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">c</span> <span class="o">&gt;=</span> <span class="n">min_num_occ</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No of words occuring more than </span><span class="si">%d</span><span class="s2"> times is </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">min_num_occ</span><span class="p">,</span> <span class="n">num_words_atleastocc</span><span class="p">))</span>
    <span class="c1"># hardcode to 10000</span>
    <span class="c1">#num_words_atleastocc = 15000</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">num_words_atleastocc</span><span class="p">,</span><span class="n">filters</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">sentences_encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No of unique words found is after capping is &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">))</span>

    <span class="c1"># Cap the length of sentence, and pad with 0 if sentences less than that length</span>
    <span class="n">sent_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences_encoded</span><span class="p">]</span>
    <span class="n">sent_lengths_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;max sentence length is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">sentences_encoded</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="nb">len</span><span class="p">))))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mean, median, 99th perc length of sentence is </span><span class="si">%f</span><span class="s2"> </span><span class="si">%f</span><span class="s2"> </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sent_lengths_arr</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">sent_lengths_arr</span><span class="p">),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">sent_lengths_arr</span><span class="p">,</span><span class="mi">99</span><span class="p">)))</span>
    <span class="c1">#plt.figure()</span>
    <span class="c1">#sns.violinplot(y=np.array(sent_lengths))</span>
    <span class="c1"># Fasttext: sentence_maxlength_cap = 256</span>
    <span class="n">sentence_maxlength_cap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">sent_lengths_arr</span><span class="p">,</span><span class="mi">99</span><span class="p">))</span>
    <span class="c1">#sentence_maxlength_cap = 140</span>
    <span class="n">sentences_encoded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sentences_encoded</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">sentence_maxlength_cap</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After padding: max sentence size is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">sentences_encoded</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="nb">len</span><span class="p">))))</span>
    <span class="n">sent_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences_encoded</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After padding: mean, median length of sentence is </span><span class="si">%f</span><span class="s2"> </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">)),</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">))))</span>

    <span class="c1"># dimension of word vector output from Embedding layer</span>
    <span class="c1"># Fasttext: wordvecdim = 20</span>
    <span class="n">wordvecdim</span> <span class="o">=</span> <span class="mi">20</span>
    
    <span class="k">return</span>  <span class="n">wordvecdim</span><span class="p">,</span> <span class="n">sentence_maxlength_cap</span><span class="p">,</span> <span class="n">sentences_encoded</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">vocab_size</span>

<span class="c1"># Disabled code</span>
<span class="k">if</span> <span class="kc">False</span><span class="p">:</span>
    <span class="c1"># Create tokenizer and encoded sentences X_train:</span>
    <span class="c1">#wordvecdim, sentence_maxlength_cap, sentences_encoded, tokenizer, vocab_size = create_tokenizer(x_train, 0)</span>
    <span class="p">(</span><span class="n">wordvecdim</span><span class="p">,</span> <span class="n">sentence_maxlength_cap</span><span class="p">,</span> 
     <span class="n">sentences_encoded</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span> <span class="o">=</span> <span class="n">create_tokenizer</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1">### debug ###</span>
    <span class="k">if</span> <span class="kc">True</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
        <span class="c1">#print(tokenizer.word_index)</span>
        <span class="n">firstfewwords</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]}</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;first few words are&quot;</span><span class="p">,</span> <span class="n">firstfewwords</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">sentences_encoded</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">document_count</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences_encoded</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>  
        <span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)[:</span><span class="mi">100</span><span class="p">])</span>  
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[47]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create word embedding matrix for word vectors:</span>
<span class="k">def</span> <span class="nf">create_word_embedding_matrix</span><span class="p">(</span><span class="n">word_vector_type</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    
    <span class="k">if</span> <span class="n">word_vector_type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Creating word embedding matrix for </span><span class="si">{}</span><span class="s2"> vectors&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">word_vector_type</span><span class="p">))</span>
    
    <span class="k">if</span> <span class="n">word_vector_type</span> <span class="o">==</span> <span class="s2">&quot;glove&quot;</span><span class="p">:</span> 
        <span class="c1"># Load pre-trained Glove vectors. </span>
        <span class="c1"># Convert them as word to word vector mapping</span>
        <span class="c1"># https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html</span>
        <span class="n">wordvecdim</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../scratch/glove.6B.100d.txt&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">word_vector_type</span> <span class="o">==</span> <span class="s2">&quot;gensim&quot;</span><span class="p">:</span>
        <span class="n">wordvecdim</span> <span class="o">=</span> <span class="mi">20</span>
        <span class="n">gensim_embeddings</span> <span class="o">=</span> <span class="s1">&#39;../scratch/gensim.&#39;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">wordvecdim</span><span class="p">)</span> <span class="o">+</span><span class="s1">&#39;.txt&#39;</span>
        <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">gensim_embeddings</span><span class="p">)</span>


    <span class="n">embeddings_index</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
        <span class="n">embeddings_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Found </span><span class="si">%s</span><span class="s1"> word vectors.&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">))</span>


    <span class="c1"># Create pretrained weight matrix to be used as Embedded layer input </span>
    <span class="c1"># Each word (in input data) to its pretrained word vector mapping</span>
    <span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span><span class="n">wordvecdim</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">embeddings_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">wordvecdim</span><span class="p">)</span>
            <span class="c1">#break</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="n">idx</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">embedding_vector</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">wordvecdim</span><span class="p">,</span> <span class="n">embedding_matrix</span>

<span class="c1"># Disabled code:</span>
<span class="k">if</span> <span class="kc">False</span><span class="p">:</span>
    <span class="n">gs_wordvecdim</span><span class="p">,</span> <span class="n">gs_embedding_matrix</span> <span class="o">=</span> <span class="n">create_word_embedding_matrix</span><span class="p">(</span><span class="s2">&quot;gensim&quot;</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    <span class="n">gl_wordvecdim</span><span class="p">,</span> <span class="n">gl_embedding_matrix</span> <span class="o">=</span> <span class="n">create_word_embedding_matrix</span><span class="p">(</span><span class="s2">&quot;glove&quot;</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Cell-used-for-tuning-hyper-parameters---Disabled-now">Cell used for tuning hyper-parameters - Disabled now<a class="anchor-link" href="#Cell-used-for-tuning-hyper-parameters---Disabled-now">&#182;</a></h3>
</div>
</div>
</div>## Disable the cell
if False:
    ## FastText model

    def create_Fasttext(vocab_size, wordvecdim, sentence_maxlength_cap, embedding_matrix, word_vector_type):  

        print("Creating Fasttext model vocab_size: {}, wordvecdim {}, sentence_maxlength_cap {}, word_vector_type {} ".format( 
              vocab_size, wordvecdim, sentence_maxlength_cap, word_vector_type))

        # Train the Fasttext NN model (1 hidden layer)
        model = Sequential()
        if word_vector_type == "glove":
            embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = False)
            #embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = True)
        elif word_vector_type == "gensim":
            embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = True)
        else:
            embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap)
        model.add(embedded_layer)
        #model.add(Flatten())
        #model.add(Dense(20,activation='relu'))
        #model.add(Dropout(0.75))
        #model.add(Flatten())
        model.add(GlobalAveragePooling1D())
        model.add(Dense(3,activation='softmax'))
        model.summary()
        return model

    # CNN model
    def create_CNN(vocab_size, wordvecdim, sentence_maxlength_cap, embedding_matrix, word_vector_type):

        print("Creating CNN model vocab_size: {}, wordvecdim {}, sentence_maxlength_cap {}, word_vector_type {} ".format( 
              vocab_size, wordvecdim, sentence_maxlength_cap, word_vector_type))

        # Train CNN model (1 hidden layer)
        model = Sequential()
        if word_vector_type == "glove":
            embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = False)
            #embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = True)
        elif word_vector_type == "gensim":
            embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = True)
        else:
            embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap)
        model.add(embedded_layer)
        #model.add(Flatten())
        #model.add(Dense(20,activation='relu'))
        model.add(Conv1D(filters = 32, kernel_size = 4, activation='relu'))
        model.add(MaxPooling1D(pool_size=2))
        model.add(Dropout(0.8))
        #model.add(Conv1D(filters = 16, kernel_size = 4, activation='relu'))
        #model.add(MaxPooling1D(pool_size=2))
        #model.add(Dropout(0.8))
        #model.add(Flatten())
        model.add(GlobalAveragePooling1D())
        model.add(Dense(3,activation='softmax'))
        model.summary()
        return model

    # Function to train the models
    def run_model(model, tokenizer, 
                  sentence_maxlength_cap, sentences_encoded, 
                  y_train_one_hot, X_train, X_test, train_raw, test_raw):

        # Compile the model
        model.compile(optimizer='adam',
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])

        # Pre training loss
        loss, score = model.evaluate(sentences_encoded, y_train_one_hot)
        print("Before training loss, score are: {} {}".format(loss,score))

        # Fit the model to train data
        #num_epochs = 200
        num_epochs = 500
        checkpointer = EarlyStopping(patience=3, monitor='val_loss')
        model.fit(sentences_encoded, y_train_one_hot,
                  epochs= num_epochs, 
                  batch_size=128, 
                  verbose=1, 
                  validation_split=0.05, shuffle = True,
                  callbacks=[checkpointer])

        # Training loss and predictions:
        loss, score = model.evaluate(sentences_encoded, y_train_one_hot)
        print("training loss, score are: {} {}".format(loss,score))
        pred_train = model.predict(sentences_encoded)

        # Test loss and predictions:
        sentences_encoded_test = tokenizer.texts_to_sequences(X_test) 
        sentences_encoded_test = pad_sequences(sentences_encoded_test, maxlen=sentence_maxlength_cap)
        loss, score = model.evaluate(sentences_encoded_test, y_test_one_hot)
        print("Test loss, score are: {} {}".format(loss,score))
        pred_test = model.predict(sentences_encoded_test)

        # Final submission test data predictions:
        X_final_test = test_raw['text'].copy()
        X_final_test = preprocess(X_final_test)
        sentences_encoded_ftest = tokenizer.texts_to_sequences(X_final_test) 
        sentences_encoded_ftest = pad_sequences(sentences_encoded_ftest, maxlen=sentence_maxlength_cap)
        pred_ftest = model.predict(sentences_encoded_ftest)

        return pred_train, pred_test, pred_ftest

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Pre-processing-for-NN---top-level-function">Pre-processing for NN - top level function<a class="anchor-link" href="#Pre-processing-for-NN---top-level-function">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[48]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Preprocessing (top level function) needed for Neural networks</span>

<span class="k">def</span> <span class="nf">run_NN_preprocess</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>    
    
    <span class="c1"># Make a copy</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">test_raw</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    
    <span class="c1"># Preprocess X_train and X_test data: Separate punctuation, add ngrams</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="c1">#print(x_train.head())</span>
    <span class="c1">#print(x_train.iloc[0])</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="c1">#print(x_test.head())</span>
    <span class="c1">#print(x_test.iloc[0])</span>
    
    <span class="c1"># Word vector type:</span>
    <span class="n">word_vector_type</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;word_vector_type&#39;</span><span class="p">]</span>
    
    <span class="c1"># Char level tokenize</span>
    <span class="n">char_level_tokenize</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;char_level&#39;</span><span class="p">]</span>
    
    <span class="c1"># Create tokenizer and encoded sentences:</span>
    <span class="p">(</span><span class="n">wordvecdim</span><span class="p">,</span> <span class="n">sentence_maxlength_cap</span><span class="p">,</span> 
     <span class="n">sentences_encoded_train</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span> <span class="o">=</span> <span class="n">create_tokenizer</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># For debug </span>
    <span class="k">if</span> <span class="kc">False</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
        <span class="c1">#print(tokenizer.word_index)</span>
        <span class="n">firstfewwords</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]}</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;first few words are&quot;</span><span class="p">,</span> <span class="n">firstfewwords</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">sentences_encoded_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">document_count</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences_encoded_train</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>  
        <span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)[:</span><span class="mi">100</span><span class="p">])</span> 
    
    <span class="c1"># Create encoded sentences for test set:</span>
    <span class="n">sentences_encoded_test</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span> 
    <span class="n">sentences_encoded_test</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sentences_encoded_test</span><span class="p">,</span> 
                                            <span class="n">maxlen</span><span class="o">=</span><span class="n">sentence_maxlength_cap</span><span class="p">)</span>    
    
    <span class="c1"># Create word2vec using gensim:</span>
    <span class="k">if</span> <span class="n">word_vector_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">wordvecdim</span><span class="p">,</span> <span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">create_word_embedding_matrix</span><span class="p">(</span><span class="n">word_vector_type</span><span class="p">,</span> 
                                                                      <span class="n">vocab_size</span><span class="p">,</span>
                                                                      <span class="n">tokenizer</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">embedding_matrix</span> <span class="o">=</span> <span class="kc">None</span>
        
    <span class="c1"># Store the output params</span>
    <span class="n">out_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">:</span> <span class="n">vocab_size</span><span class="p">,</span>
                 <span class="s1">&#39;wordvecdim&#39;</span><span class="p">:</span> <span class="n">wordvecdim</span><span class="p">,</span>
                 <span class="s1">&#39;sentence_maxlength_cap&#39;</span><span class="p">:</span> <span class="n">sentence_maxlength_cap</span><span class="p">,</span>
                 <span class="s1">&#39;embedding_matrix&#39;</span><span class="p">:</span> <span class="n">embedding_matrix</span><span class="p">,</span>
                 <span class="s1">&#39;word_vector_type&#39;</span><span class="p">:</span> <span class="n">word_vector_type</span><span class="p">,</span>
                 <span class="s1">&#39;tokenizer&#39;</span><span class="p">:</span> <span class="n">tokenizer</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">sentences_encoded_train</span><span class="p">,</span> <span class="n">sentences_encoded_test</span><span class="p">,</span> <span class="n">out_params</span>
    
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="FastText-model-definition-and-running:">FastText model definition and running:<a class="anchor-link" href="#FastText-model-definition-and-running:">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[49]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Fast text model </span>

<span class="k">def</span> <span class="nf">run_Fasttext</span><span class="p">(</span><span class="n">sentences_encoded_train</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span>
                 <span class="n">sentences_encoded_val</span><span class="p">,</span> <span class="n">y_val_one_hot</span><span class="p">,</span>
                 <span class="n">sentences_encoded_test</span><span class="p">,</span> 
                 <span class="n">model_params</span><span class="p">):</span>  

    <span class="c1"># All model params</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">]</span> 
    <span class="n">wordvecdim</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;wordvecdim&#39;</span><span class="p">]</span>
    <span class="n">sentence_maxlength_cap</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;sentence_maxlength_cap&#39;</span><span class="p">]</span>
    <span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;embedding_matrix&#39;</span><span class="p">]</span>
    <span class="n">word_vector_type</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;word_vector_type&#39;</span><span class="p">]</span>
    <span class="n">tokenizer</span> <span class="o">=</span>  <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;tokenizer&#39;</span><span class="p">]</span>    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Creating Fasttext model vocab_size: </span><span class="si">{}</span><span class="s2">, wordvecdim </span><span class="si">{}</span><span class="s2">, sentence_maxlength_cap </span><span class="si">{}</span><span class="s2">, word_vector_type </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span> 
          <span class="n">vocab_size</span><span class="p">,</span> <span class="n">wordvecdim</span><span class="p">,</span> <span class="n">sentence_maxlength_cap</span><span class="p">,</span> <span class="n">word_vector_type</span><span class="p">))</span>
    
    <span class="c1"># Train the Fasttext NN model (1 hidden layer)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">word_vector_type</span> <span class="o">==</span> <span class="s2">&quot;glove&quot;</span><span class="p">:</span>
        <span class="n">embedded_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">wordvecdim</span><span class="p">,</span> 
                                   <span class="n">input_length</span> <span class="o">=</span> <span class="n">sentence_maxlength_cap</span><span class="p">,</span> 
                                   <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">embedding_matrix</span><span class="p">],</span> <span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1">#embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = True)</span>
    <span class="k">elif</span> <span class="n">word_vector_type</span> <span class="o">==</span> <span class="s2">&quot;gensim&quot;</span><span class="p">:</span>
        <span class="n">embedded_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">wordvecdim</span><span class="p">,</span> 
                                   <span class="n">input_length</span> <span class="o">=</span> <span class="n">sentence_maxlength_cap</span><span class="p">,</span> 
                                   <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">embedding_matrix</span><span class="p">],</span> <span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">embedded_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">wordvecdim</span><span class="p">,</span> 
                                   <span class="n">input_length</span> <span class="o">=</span> <span class="n">sentence_maxlength_cap</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">embedded_layer</span><span class="p">)</span>
    <span class="c1">#model.add(Flatten())</span>
    <span class="c1">#model.add(Dense(20,activation=&#39;relu&#39;))</span>
    <span class="c1">#model.add(Conv1D(filters = 8, kernel_size = 5,activation=&#39;relu&#39;))</span>
    <span class="c1">#model.add(MaxPooling1D(pool_size=10))</span>
    <span class="c1">#model.add(Dropout(0.75))</span>
    <span class="c1">#model.add(Flatten())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">GlobalAveragePooling1D</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

    <span class="c1"># Compile the model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                  <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

    <span class="c1"># Pre training loss</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">sentences_encoded_train</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before training loss, score are: </span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">score</span><span class="p">))</span>

    <span class="c1"># Fit the model to train data</span>
    <span class="c1">#num_epochs = 200</span>
    <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">150</span>
    <span class="n">validation_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">sentences_encoded_val</span><span class="p">,</span> <span class="n">y_val_one_hot</span><span class="p">)</span>
    <span class="n">checkpointer</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">sentences_encoded_train</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span>
              <span class="n">epochs</span><span class="o">=</span> <span class="n">num_epochs</span><span class="p">,</span> 
              <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> 
              <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
              <span class="n">validation_data</span> <span class="o">=</span> <span class="n">validation_data</span><span class="p">,</span>
              <span class="c1">#validation_split = 0.05, shuffle = True,              </span>
              <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpointer</span><span class="p">])</span>

    <span class="c1"># Training loss and predictions:</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">sentences_encoded_train</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;==== Training loss, score are: </span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2"> =======&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">score</span><span class="p">))</span>
    <span class="n">pred_train</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sentences_encoded_train</span><span class="p">)</span>

    <span class="c1"># Val loss and predictions:</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">sentences_encoded_val</span><span class="p">,</span> <span class="n">y_val_one_hot</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;==== CV loss, score are: </span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2"> =======&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">score</span><span class="p">))</span>
    <span class="n">pred_val</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sentences_encoded_val</span><span class="p">)</span>
    
    <span class="c1"># Test loss and predictions:</span>
    <span class="n">pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sentences_encoded_test</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pred_val</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[50]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Run Fast text and add predictions as features:</span>

<span class="c1"># No pre-trained vectors:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Timestamp: {:%Y-%b-</span><span class="si">%d</span><span class="s1"> %H:%M:%S}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()))</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;fast_text_none&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_NN_preprocess</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_Fasttext</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;temp&#39;</span><span class="p">:</span> <span class="mi">0</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;char_level&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;word_vector_type&#39;</span><span class="p">:</span> <span class="kc">None</span> <span class="p">}</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;NN&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>

<span class="c1"># Pre-trained word2vec using vectors:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Timestamp: {:%Y-%b-</span><span class="si">%d</span><span class="s1"> %H:%M:%S}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()))</span>
<span class="n">wordvecsize</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">create_gensim_wordvectors</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">if</span> <span class="n">create_gensim_wordvectors</span><span class="p">:</span>
    <span class="n">create_gensim_wordvec</span><span class="p">(</span><span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">wordvecsize</span><span class="p">)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;fast_text_gensim&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_NN_preprocess</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_Fasttext</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;temp&#39;</span><span class="p">:</span> <span class="mi">0</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;char_level&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;word_vector_type&#39;</span><span class="p">:</span> <span class="s2">&quot;gensim&quot;</span> <span class="p">}</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;NN&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
    
<span class="c1"># Glove vectors:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Timestamp: {:%Y-%b-</span><span class="si">%d</span><span class="s1"> %H:%M:%S}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()))</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;fast_text_glove&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_NN_preprocess</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_Fasttext</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;temp&#39;</span><span class="p">:</span> <span class="mi">0</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;char_level&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;word_vector_type&#39;</span><span class="p">:</span> <span class="s2">&quot;glove&quot;</span> <span class="p">}</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;NN&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Timestamp: 2018-Jan-14 02:51:50
Running kfold training with model fast_text_none
Shapes: x_train_raw.shape (19579, 18), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 17)
Running preprocess: &lt;function run_NN_preprocess at 0x7f620ee8eea0&gt;
No of unique words found is 247346
No of words occuring more than 2 times is 75104
No of unique words found is after capping is 247346
max sentence length is 1439
mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000
After padding: max sentence size is 161
After padding: mean, median length of sentence is 161.000000 161.000000
Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)
Running fold 1
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_4 (Embedding)      (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_4 ( (None, 20)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 0s 26us/step
Before training loss, score are: 1.0999214982100542 0.3116261252728731
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 54us/step - loss: 1.0817 - acc: 0.4067 - val_loss: 1.0771 - val_acc: 0.3927
Epoch 2/150
15663/15663 [==============================] - 1s 46us/step - loss: 1.0559 - acc: 0.4184 - val_loss: 1.0477 - val_acc: 0.4162
Epoch 3/150
15663/15663 [==============================] - 1s 46us/step - loss: 1.0115 - acc: 0.4684 - val_loss: 0.9995 - val_acc: 0.5005
Epoch 4/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.9472 - acc: 0.5881 - val_loss: 0.9409 - val_acc: 0.5534
Epoch 5/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.8723 - acc: 0.6940 - val_loss: 0.8768 - val_acc: 0.6402
Epoch 6/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.7958 - acc: 0.7617 - val_loss: 0.8149 - val_acc: 0.7086
Epoch 7/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.7237 - acc: 0.8058 - val_loss: 0.7601 - val_acc: 0.7388
Epoch 8/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.6581 - acc: 0.8340 - val_loss: 0.7109 - val_acc: 0.7684
Epoch 9/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.6010 - acc: 0.8569 - val_loss: 0.6698 - val_acc: 0.7778
Epoch 10/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.5500 - acc: 0.8710 - val_loss: 0.6376 - val_acc: 0.7735
Epoch 11/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.5054 - acc: 0.8841 - val_loss: 0.6039 - val_acc: 0.7972
Epoch 12/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.4658 - acc: 0.8959 - val_loss: 0.5771 - val_acc: 0.8077
Epoch 13/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.4302 - acc: 0.9066 - val_loss: 0.5518 - val_acc: 0.8169
Epoch 14/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3983 - acc: 0.9169 - val_loss: 0.5316 - val_acc: 0.8192
Epoch 15/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3698 - acc: 0.9252 - val_loss: 0.5138 - val_acc: 0.8218
Epoch 16/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3436 - acc: 0.9310 - val_loss: 0.4964 - val_acc: 0.8276
Epoch 17/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3197 - acc: 0.9389 - val_loss: 0.4805 - val_acc: 0.8366
Epoch 18/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2979 - acc: 0.9443 - val_loss: 0.4677 - val_acc: 0.8361
Epoch 19/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2782 - acc: 0.9490 - val_loss: 0.4555 - val_acc: 0.8407
Epoch 20/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2599 - acc: 0.9536 - val_loss: 0.4444 - val_acc: 0.8404
Epoch 21/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.2432 - acc: 0.9588 - val_loss: 0.4352 - val_acc: 0.8412
Epoch 22/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2275 - acc: 0.9621 - val_loss: 0.4262 - val_acc: 0.8435
Epoch 23/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2132 - acc: 0.9653 - val_loss: 0.4189 - val_acc: 0.8481
Epoch 24/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.1999 - acc: 0.9688 - val_loss: 0.4143 - val_acc: 0.8453
Epoch 25/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1875 - acc: 0.9703 - val_loss: 0.4059 - val_acc: 0.8460
Epoch 26/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1760 - acc: 0.9732 - val_loss: 0.3975 - val_acc: 0.8529
Epoch 27/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1651 - acc: 0.9756 - val_loss: 0.3922 - val_acc: 0.8539
Epoch 28/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1551 - acc: 0.9781 - val_loss: 0.3858 - val_acc: 0.8557
Epoch 29/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1458 - acc: 0.9799 - val_loss: 0.3846 - val_acc: 0.8527
Epoch 30/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1372 - acc: 0.9808 - val_loss: 0.3793 - val_acc: 0.8542
Epoch 31/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1290 - acc: 0.9826 - val_loss: 0.3731 - val_acc: 0.8583
Epoch 32/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1216 - acc: 0.9842 - val_loss: 0.3721 - val_acc: 0.8560
Epoch 33/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1144 - acc: 0.9857 - val_loss: 0.3701 - val_acc: 0.8560
Epoch 34/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1080 - acc: 0.9870 - val_loss: 0.3648 - val_acc: 0.8565
Epoch 35/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1018 - acc: 0.9879 - val_loss: 0.3636 - val_acc: 0.8565
Epoch 36/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0960 - acc: 0.9888 - val_loss: 0.3627 - val_acc: 0.8565
Epoch 37/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0904 - acc: 0.9902 - val_loss: 0.3592 - val_acc: 0.8580
Epoch 38/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0852 - acc: 0.9906 - val_loss: 0.3587 - val_acc: 0.8588
Epoch 39/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0804 - acc: 0.9911 - val_loss: 0.3555 - val_acc: 0.8596
Epoch 40/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0761 - acc: 0.9917 - val_loss: 0.3526 - val_acc: 0.8616
Epoch 41/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0720 - acc: 0.9922 - val_loss: 0.3549 - val_acc: 0.8593
Epoch 42/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0677 - acc: 0.9928 - val_loss: 0.3507 - val_acc: 0.8613
Epoch 43/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0641 - acc: 0.9931 - val_loss: 0.3515 - val_acc: 0.8593
Epoch 44/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0604 - acc: 0.9938 - val_loss: 0.3495 - val_acc: 0.8596
Epoch 45/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0571 - acc: 0.9941 - val_loss: 0.3479 - val_acc: 0.8616
Epoch 46/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0540 - acc: 0.9950 - val_loss: 0.3510 - val_acc: 0.8598
Epoch 47/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0512 - acc: 0.9953 - val_loss: 0.3486 - val_acc: 0.8603
Epoch 48/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0483 - acc: 0.9959 - val_loss: 0.3561 - val_acc: 0.8555
15663/15663 [==============================] - 0s 20us/step
==== Training loss, score are: 0.0470402358821782 0.9943178190640363 =======
3916/3916 [==============================] - 0s 21us/step
==== CV loss, score are: 0.3561393990427773 0.855464759959142 =======
Running fold 2
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_5 (Embedding)      (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_5 ( (None, 20)                0         
_________________________________________________________________
dense_5 (Dense)              (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 0s 25us/step
Before training loss, score are: 1.0954445080153445 0.4033071570170228
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 53us/step - loss: 1.0814 - acc: 0.4059 - val_loss: 1.0767 - val_acc: 0.3953
Epoch 2/150
15663/15663 [==============================] - 1s 46us/step - loss: 1.0565 - acc: 0.4172 - val_loss: 1.0492 - val_acc: 0.4147
Epoch 3/150
15663/15663 [==============================] - 1s 46us/step - loss: 1.0120 - acc: 0.4737 - val_loss: 1.0031 - val_acc: 0.4737
Epoch 4/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.9472 - acc: 0.5951 - val_loss: 0.9434 - val_acc: 0.5991
Epoch 5/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.8717 - acc: 0.7018 - val_loss: 0.8806 - val_acc: 0.6900
Epoch 6/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.7953 - acc: 0.7695 - val_loss: 0.8209 - val_acc: 0.7441
Epoch 7/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.7242 - acc: 0.8080 - val_loss: 0.7677 - val_acc: 0.7482
Epoch 8/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.6592 - acc: 0.8313 - val_loss: 0.7205 - val_acc: 0.7704
Epoch 9/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.6024 - acc: 0.8507 - val_loss: 0.6800 - val_acc: 0.7806
Epoch 10/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.5518 - acc: 0.8688 - val_loss: 0.6451 - val_acc: 0.7878
Epoch 11/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5074 - acc: 0.8838 - val_loss: 0.6135 - val_acc: 0.8029
Epoch 12/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4678 - acc: 0.8936 - val_loss: 0.5866 - val_acc: 0.8141
Epoch 13/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4323 - acc: 0.9061 - val_loss: 0.5628 - val_acc: 0.8159
Epoch 14/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.4000 - acc: 0.9150 - val_loss: 0.5423 - val_acc: 0.8207
Epoch 15/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3712 - acc: 0.9244 - val_loss: 0.5224 - val_acc: 0.8304
Epoch 16/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3452 - acc: 0.9325 - val_loss: 0.5053 - val_acc: 0.8307
Epoch 17/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3213 - acc: 0.9384 - val_loss: 0.4899 - val_acc: 0.8368
Epoch 18/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2993 - acc: 0.9457 - val_loss: 0.4759 - val_acc: 0.8427
Epoch 19/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2794 - acc: 0.9513 - val_loss: 0.4639 - val_acc: 0.8419
Epoch 20/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2610 - acc: 0.9551 - val_loss: 0.4529 - val_acc: 0.8453
Epoch 21/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2440 - acc: 0.9593 - val_loss: 0.4418 - val_acc: 0.8501
Epoch 22/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2280 - acc: 0.9641 - val_loss: 0.4330 - val_acc: 0.8504
Epoch 23/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2136 - acc: 0.9669 - val_loss: 0.4235 - val_acc: 0.8516
Epoch 24/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2003 - acc: 0.9701 - val_loss: 0.4156 - val_acc: 0.8524
Epoch 25/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1878 - acc: 0.9725 - val_loss: 0.4091 - val_acc: 0.8547
Epoch 26/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1762 - acc: 0.9747 - val_loss: 0.4021 - val_acc: 0.8544
Epoch 27/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1655 - acc: 0.9771 - val_loss: 0.3961 - val_acc: 0.8562
Epoch 28/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1554 - acc: 0.9780 - val_loss: 0.3907 - val_acc: 0.8573
Epoch 29/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1460 - acc: 0.9797 - val_loss: 0.3858 - val_acc: 0.8578
Epoch 30/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1374 - acc: 0.9821 - val_loss: 0.3812 - val_acc: 0.8588
Epoch 31/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1291 - acc: 0.9827 - val_loss: 0.3780 - val_acc: 0.8603
Epoch 32/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1215 - acc: 0.9845 - val_loss: 0.3744 - val_acc: 0.8606
Epoch 33/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1144 - acc: 0.9856 - val_loss: 0.3698 - val_acc: 0.8611
Epoch 34/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1076 - acc: 0.9868 - val_loss: 0.3675 - val_acc: 0.8613
Epoch 35/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1014 - acc: 0.9874 - val_loss: 0.3662 - val_acc: 0.8603
Epoch 36/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0957 - acc: 0.9882 - val_loss: 0.3629 - val_acc: 0.8629
Epoch 37/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.0901 - acc: 0.9894 - val_loss: 0.3603 - val_acc: 0.8652
Epoch 38/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0850 - acc: 0.9900 - val_loss: 0.3585 - val_acc: 0.8647
Epoch 39/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0802 - acc: 0.9908 - val_loss: 0.3556 - val_acc: 0.8629
Epoch 40/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0756 - acc: 0.9922 - val_loss: 0.3568 - val_acc: 0.8621
Epoch 41/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0715 - acc: 0.9923 - val_loss: 0.3535 - val_acc: 0.8636
Epoch 42/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0675 - acc: 0.9933 - val_loss: 0.3524 - val_acc: 0.8631
Epoch 43/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0635 - acc: 0.9937 - val_loss: 0.3506 - val_acc: 0.8647
Epoch 44/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0600 - acc: 0.9943 - val_loss: 0.3508 - val_acc: 0.8629
Epoch 45/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0566 - acc: 0.9946 - val_loss: 0.3525 - val_acc: 0.8621
Epoch 46/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0534 - acc: 0.9950 - val_loss: 0.3493 - val_acc: 0.8626
Epoch 47/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0505 - acc: 0.9951 - val_loss: 0.3490 - val_acc: 0.8634
Epoch 48/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0477 - acc: 0.9955 - val_loss: 0.3487 - val_acc: 0.8626
Epoch 49/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0451 - acc: 0.9959 - val_loss: 0.3493 - val_acc: 0.8616
Epoch 50/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0427 - acc: 0.9962 - val_loss: 0.3494 - val_acc: 0.8639
Epoch 51/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0403 - acc: 0.9967 - val_loss: 0.3494 - val_acc: 0.8636
15663/15663 [==============================] - 0s 21us/step
==== Training loss, score are: 0.038422147226279964 0.9968077635191215 =======
3916/3916 [==============================] - 0s 21us/step
==== CV loss, score are: 0.34937799246306317 0.8636363636363636 =======
Running fold 3
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_6 (Embedding)      (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_6 ( (None, 20)                0         
_________________________________________________________________
dense_6 (Dense)              (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 0s 27us/step
Before training loss, score are: 1.1024929768721172 0.2903658303073682
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 57us/step - loss: 1.0847 - acc: 0.3955 - val_loss: 1.0753 - val_acc: 0.3996
Epoch 2/150
15663/15663 [==============================] - 1s 47us/step - loss: 1.0591 - acc: 0.4112 - val_loss: 1.0491 - val_acc: 0.4150
Epoch 3/150
15663/15663 [==============================] - 1s 46us/step - loss: 1.0167 - acc: 0.4551 - val_loss: 1.0046 - val_acc: 0.5015
Epoch 4/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.9533 - acc: 0.5809 - val_loss: 0.9458 - val_acc: 0.6065
Epoch 5/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.8774 - acc: 0.6940 - val_loss: 0.8824 - val_acc: 0.6627
Epoch 6/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.7988 - acc: 0.7743 - val_loss: 0.8208 - val_acc: 0.7137
Epoch 7/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.7250 - acc: 0.8109 - val_loss: 0.7650 - val_acc: 0.7549
Epoch 8/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.6593 - acc: 0.8392 - val_loss: 0.7163 - val_acc: 0.7804
Epoch 9/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6013 - acc: 0.8565 - val_loss: 0.6745 - val_acc: 0.7829
Epoch 10/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5499 - acc: 0.8714 - val_loss: 0.6391 - val_acc: 0.8026
Epoch 11/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5053 - acc: 0.8870 - val_loss: 0.6071 - val_acc: 0.8003
Epoch 12/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4656 - acc: 0.8973 - val_loss: 0.5798 - val_acc: 0.8151
Epoch 13/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4300 - acc: 0.9061 - val_loss: 0.5560 - val_acc: 0.8289
Epoch 14/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3981 - acc: 0.9179 - val_loss: 0.5344 - val_acc: 0.8304
Epoch 15/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3693 - acc: 0.9259 - val_loss: 0.5155 - val_acc: 0.8338
Epoch 16/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3430 - acc: 0.9325 - val_loss: 0.4982 - val_acc: 0.8327
Epoch 17/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3196 - acc: 0.9387 - val_loss: 0.4830 - val_acc: 0.8396
Epoch 18/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2979 - acc: 0.9434 - val_loss: 0.4695 - val_acc: 0.8432
Epoch 19/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2782 - acc: 0.9503 - val_loss: 0.4571 - val_acc: 0.8414
Epoch 20/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2600 - acc: 0.9537 - val_loss: 0.4472 - val_acc: 0.8475
Epoch 21/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2430 - acc: 0.9586 - val_loss: 0.4352 - val_acc: 0.8516
Epoch 22/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2275 - acc: 0.9624 - val_loss: 0.4251 - val_acc: 0.8555
Epoch 23/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2134 - acc: 0.9655 - val_loss: 0.4175 - val_acc: 0.8567
Epoch 24/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1999 - acc: 0.9685 - val_loss: 0.4089 - val_acc: 0.8552
Epoch 25/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1874 - acc: 0.9708 - val_loss: 0.4028 - val_acc: 0.8588
Epoch 26/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1762 - acc: 0.9740 - val_loss: 0.3951 - val_acc: 0.8565
Epoch 27/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1654 - acc: 0.9759 - val_loss: 0.3890 - val_acc: 0.8580
Epoch 28/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1556 - acc: 0.9775 - val_loss: 0.3829 - val_acc: 0.8601
Epoch 29/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.1463 - acc: 0.9794 - val_loss: 0.3785 - val_acc: 0.8585
Epoch 30/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1376 - acc: 0.9814 - val_loss: 0.3730 - val_acc: 0.8639
Epoch 31/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1295 - acc: 0.9832 - val_loss: 0.3692 - val_acc: 0.8618
Epoch 32/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1218 - acc: 0.9845 - val_loss: 0.3646 - val_acc: 0.8641
Epoch 33/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1149 - acc: 0.9857 - val_loss: 0.3614 - val_acc: 0.8657
Epoch 34/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1079 - acc: 0.9867 - val_loss: 0.3574 - val_acc: 0.8680
Epoch 35/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.1018 - acc: 0.9879 - val_loss: 0.3567 - val_acc: 0.8659
Epoch 36/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0962 - acc: 0.9884 - val_loss: 0.3523 - val_acc: 0.8687
Epoch 37/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0906 - acc: 0.9902 - val_loss: 0.3521 - val_acc: 0.8664
Epoch 38/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0854 - acc: 0.9907 - val_loss: 0.3470 - val_acc: 0.8680
Epoch 39/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0806 - acc: 0.9913 - val_loss: 0.3450 - val_acc: 0.8693
Epoch 40/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0761 - acc: 0.9920 - val_loss: 0.3431 - val_acc: 0.8687
Epoch 41/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0718 - acc: 0.9921 - val_loss: 0.3421 - val_acc: 0.8698
Epoch 42/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0679 - acc: 0.9932 - val_loss: 0.3412 - val_acc: 0.8710
Epoch 43/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.0640 - acc: 0.9936 - val_loss: 0.3400 - val_acc: 0.8716
Epoch 44/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.0605 - acc: 0.9941 - val_loss: 0.3393 - val_acc: 0.8695
Epoch 45/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0571 - acc: 0.9946 - val_loss: 0.3371 - val_acc: 0.8713
Epoch 46/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0539 - acc: 0.9949 - val_loss: 0.3366 - val_acc: 0.8710
Epoch 47/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.0511 - acc: 0.9952 - val_loss: 0.3352 - val_acc: 0.8700
Epoch 48/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0483 - acc: 0.9956 - val_loss: 0.3342 - val_acc: 0.8718
Epoch 49/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0456 - acc: 0.9961 - val_loss: 0.3338 - val_acc: 0.8718
Epoch 50/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0433 - acc: 0.9963 - val_loss: 0.3348 - val_acc: 0.8713
Epoch 51/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0407 - acc: 0.9963 - val_loss: 0.3348 - val_acc: 0.8705
Epoch 52/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0387 - acc: 0.9967 - val_loss: 0.3349 - val_acc: 0.8703
15663/15663 [==============================] - 0s 20us/step
==== Training loss, score are: 0.036956799540566215 0.9964885398710337 =======
3916/3916 [==============================] - 0s 20us/step
==== CV loss, score are: 0.3349311209309454 0.8702757916849895 =======
Running fold 4
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_7 (Embedding)      (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_7 ( (None, 20)                0         
_________________________________________________________________
dense_7 (Dense)              (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 0s 28us/step
Before training loss, score are: 1.096575716221796 0.4000510758065267
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 54us/step - loss: 1.0830 - acc: 0.4022 - val_loss: 1.0706 - val_acc: 0.4134
Epoch 2/150
15663/15663 [==============================] - 1s 46us/step - loss: 1.0583 - acc: 0.4156 - val_loss: 1.0432 - val_acc: 0.4331
Epoch 3/150
15663/15663 [==============================] - 1s 46us/step - loss: 1.0152 - acc: 0.4713 - val_loss: 0.9988 - val_acc: 0.5130
Epoch 4/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.9534 - acc: 0.5846 - val_loss: 0.9427 - val_acc: 0.6193
Epoch 5/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.8811 - acc: 0.6966 - val_loss: 0.8821 - val_acc: 0.6828
Epoch 6/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.8069 - acc: 0.7678 - val_loss: 0.8230 - val_acc: 0.7153
Epoch 7/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7364 - acc: 0.7994 - val_loss: 0.7701 - val_acc: 0.7594
Epoch 8/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.6716 - acc: 0.8319 - val_loss: 0.7226 - val_acc: 0.7707
Epoch 9/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.6137 - acc: 0.8497 - val_loss: 0.6816 - val_acc: 0.7840
Epoch 10/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5626 - acc: 0.8674 - val_loss: 0.6465 - val_acc: 0.8001
Epoch 11/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.5176 - acc: 0.8838 - val_loss: 0.6145 - val_acc: 0.8018
Epoch 12/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.4772 - acc: 0.8937 - val_loss: 0.5857 - val_acc: 0.8069
Epoch 13/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.4410 - acc: 0.9048 - val_loss: 0.5618 - val_acc: 0.8141
Epoch 14/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.4083 - acc: 0.9136 - val_loss: 0.5402 - val_acc: 0.8174
Epoch 15/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3790 - acc: 0.9224 - val_loss: 0.5222 - val_acc: 0.8269
Epoch 16/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.3525 - acc: 0.9321 - val_loss: 0.5044 - val_acc: 0.8241
Epoch 17/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3283 - acc: 0.9373 - val_loss: 0.4896 - val_acc: 0.8330
Epoch 18/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3062 - acc: 0.9434 - val_loss: 0.4756 - val_acc: 0.8373
Epoch 19/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2858 - acc: 0.9499 - val_loss: 0.4623 - val_acc: 0.8409
Epoch 20/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.2671 - acc: 0.9530 - val_loss: 0.4516 - val_acc: 0.8453
Epoch 21/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2499 - acc: 0.9579 - val_loss: 0.4401 - val_acc: 0.8473
Epoch 22/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2339 - acc: 0.9621 - val_loss: 0.4309 - val_acc: 0.8509
Epoch 23/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2193 - acc: 0.9650 - val_loss: 0.4216 - val_acc: 0.8488
Epoch 24/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2055 - acc: 0.9690 - val_loss: 0.4136 - val_acc: 0.8532
Epoch 25/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1929 - acc: 0.9716 - val_loss: 0.4061 - val_acc: 0.8542
Epoch 26/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1812 - acc: 0.9738 - val_loss: 0.4002 - val_acc: 0.8567
Epoch 27/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1704 - acc: 0.9749 - val_loss: 0.3934 - val_acc: 0.8562
Epoch 28/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1602 - acc: 0.9771 - val_loss: 0.3880 - val_acc: 0.8547
Epoch 29/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1506 - acc: 0.9787 - val_loss: 0.3831 - val_acc: 0.8573
Epoch 30/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1418 - acc: 0.9806 - val_loss: 0.3776 - val_acc: 0.8611
Epoch 31/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1336 - acc: 0.9824 - val_loss: 0.3737 - val_acc: 0.8618
Epoch 32/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1258 - acc: 0.9836 - val_loss: 0.3695 - val_acc: 0.8624
Epoch 33/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1186 - acc: 0.9850 - val_loss: 0.3670 - val_acc: 0.8588
Epoch 34/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1117 - acc: 0.9857 - val_loss: 0.3624 - val_acc: 0.8634
Epoch 35/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1055 - acc: 0.9871 - val_loss: 0.3599 - val_acc: 0.8608
Epoch 36/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0994 - acc: 0.9879 - val_loss: 0.3564 - val_acc: 0.8667
Epoch 37/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0938 - acc: 0.9887 - val_loss: 0.3539 - val_acc: 0.8654
Epoch 38/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0883 - acc: 0.9899 - val_loss: 0.3524 - val_acc: 0.8682
Epoch 39/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0834 - acc: 0.9906 - val_loss: 0.3493 - val_acc: 0.8672
Epoch 40/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0787 - acc: 0.9911 - val_loss: 0.3476 - val_acc: 0.8670
Epoch 41/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0744 - acc: 0.9916 - val_loss: 0.3459 - val_acc: 0.8662
Epoch 42/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0703 - acc: 0.9924 - val_loss: 0.3456 - val_acc: 0.8639
Epoch 43/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0665 - acc: 0.9930 - val_loss: 0.3436 - val_acc: 0.8682
Epoch 44/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0627 - acc: 0.9936 - val_loss: 0.3420 - val_acc: 0.8664
Epoch 45/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0596 - acc: 0.9941 - val_loss: 0.3410 - val_acc: 0.8685
Epoch 46/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0560 - acc: 0.9948 - val_loss: 0.3402 - val_acc: 0.8685
Epoch 47/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0531 - acc: 0.9948 - val_loss: 0.3394 - val_acc: 0.8687
Epoch 48/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0501 - acc: 0.9957 - val_loss: 0.3399 - val_acc: 0.8659
Epoch 49/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0474 - acc: 0.9957 - val_loss: 0.3386 - val_acc: 0.8690
Epoch 50/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0449 - acc: 0.9960 - val_loss: 0.3384 - val_acc: 0.8682
Epoch 51/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0424 - acc: 0.9965 - val_loss: 0.3380 - val_acc: 0.8695
Epoch 52/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0402 - acc: 0.9971 - val_loss: 0.3392 - val_acc: 0.8672
Epoch 53/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0380 - acc: 0.9969 - val_loss: 0.3384 - val_acc: 0.8703
Epoch 54/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0360 - acc: 0.9970 - val_loss: 0.3378 - val_acc: 0.8680
Epoch 55/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0341 - acc: 0.9976 - val_loss: 0.3387 - val_acc: 0.8672
Epoch 56/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0323 - acc: 0.9973 - val_loss: 0.3395 - val_acc: 0.8675
Epoch 57/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0305 - acc: 0.9976 - val_loss: 0.3396 - val_acc: 0.8695
15663/15663 [==============================] - 0s 20us/step
==== Training loss, score are: 0.029169710141814797 0.9976377450041499 =======
3916/3916 [==============================] - 0s 20us/step
==== CV loss, score are: 0.3396222192463032 0.8695097037793666 =======
Running fold 5
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_8 (Embedding)      (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_8 ( (None, 20)                0         
_________________________________________________________________
dense_8 (Dense)              (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15664/15664 [==============================] - 0s 29us/step
Before training loss, score are: 1.101763122902462 0.31205311542390196
Train on 15664 samples, validate on 3915 samples
Epoch 1/150
15664/15664 [==============================] - 1s 56us/step - loss: 1.0860 - acc: 0.3878 - val_loss: 1.0697 - val_acc: 0.4212
Epoch 2/150
15664/15664 [==============================] - 1s 45us/step - loss: 1.0614 - acc: 0.4044 - val_loss: 1.0443 - val_acc: 0.4383
Epoch 3/150
15664/15664 [==============================] - 1s 46us/step - loss: 1.0187 - acc: 0.4578 - val_loss: 0.9989 - val_acc: 0.5188
Epoch 4/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.9545 - acc: 0.5830 - val_loss: 0.9405 - val_acc: 0.6542
Epoch 5/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.8779 - acc: 0.7065 - val_loss: 0.8758 - val_acc: 0.7195
Epoch 6/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.7991 - acc: 0.7811 - val_loss: 0.8124 - val_acc: 0.7280
Epoch 7/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.7248 - acc: 0.8181 - val_loss: 0.7567 - val_acc: 0.7678
Epoch 8/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.6583 - acc: 0.8412 - val_loss: 0.7086 - val_acc: 0.7750
Epoch 9/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.6003 - acc: 0.8573 - val_loss: 0.6665 - val_acc: 0.7985
Epoch 10/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.5490 - acc: 0.8753 - val_loss: 0.6304 - val_acc: 0.8095
Epoch 11/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.5045 - acc: 0.8889 - val_loss: 0.6004 - val_acc: 0.8156
Epoch 12/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.4648 - acc: 0.8982 - val_loss: 0.5732 - val_acc: 0.8232
Epoch 13/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.4294 - acc: 0.9088 - val_loss: 0.5501 - val_acc: 0.8309
Epoch 14/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.3977 - acc: 0.9185 - val_loss: 0.5287 - val_acc: 0.8324
Epoch 15/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.3695 - acc: 0.9248 - val_loss: 0.5108 - val_acc: 0.8396
Epoch 16/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.3437 - acc: 0.9333 - val_loss: 0.4944 - val_acc: 0.8437
Epoch 17/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.3202 - acc: 0.9400 - val_loss: 0.4791 - val_acc: 0.8437
Epoch 18/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.2985 - acc: 0.9457 - val_loss: 0.4675 - val_acc: 0.8511
Epoch 19/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.2788 - acc: 0.9508 - val_loss: 0.4539 - val_acc: 0.8506
Epoch 20/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.2608 - acc: 0.9554 - val_loss: 0.4429 - val_acc: 0.8531
Epoch 21/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.2441 - acc: 0.9580 - val_loss: 0.4330 - val_acc: 0.8526
Epoch 22/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.2288 - acc: 0.9622 - val_loss: 0.4238 - val_acc: 0.8572
Epoch 23/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.2144 - acc: 0.9651 - val_loss: 0.4165 - val_acc: 0.8587
Epoch 24/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.2012 - acc: 0.9683 - val_loss: 0.4079 - val_acc: 0.8554
Epoch 25/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.1890 - acc: 0.9710 - val_loss: 0.4010 - val_acc: 0.8554
Epoch 26/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.1774 - acc: 0.9734 - val_loss: 0.3943 - val_acc: 0.8590
Epoch 27/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.1667 - acc: 0.9746 - val_loss: 0.3886 - val_acc: 0.8595
Epoch 28/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.1567 - acc: 0.9771 - val_loss: 0.3829 - val_acc: 0.8600
Epoch 29/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.1476 - acc: 0.9787 - val_loss: 0.3782 - val_acc: 0.8616
Epoch 30/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.1389 - acc: 0.9813 - val_loss: 0.3740 - val_acc: 0.8590
Epoch 31/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.1309 - acc: 0.9826 - val_loss: 0.3699 - val_acc: 0.8649
Epoch 32/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.1232 - acc: 0.9843 - val_loss: 0.3657 - val_acc: 0.8626
Epoch 33/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.1161 - acc: 0.9854 - val_loss: 0.3630 - val_acc: 0.8633
Epoch 34/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.1095 - acc: 0.9860 - val_loss: 0.3598 - val_acc: 0.8674
Epoch 35/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.1033 - acc: 0.9874 - val_loss: 0.3577 - val_acc: 0.8613
Epoch 36/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0975 - acc: 0.9884 - val_loss: 0.3538 - val_acc: 0.8649
Epoch 37/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0920 - acc: 0.9900 - val_loss: 0.3518 - val_acc: 0.8667
Epoch 38/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0869 - acc: 0.9904 - val_loss: 0.3495 - val_acc: 0.8644
Epoch 39/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0819 - acc: 0.9913 - val_loss: 0.3474 - val_acc: 0.8669
Epoch 40/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0773 - acc: 0.9917 - val_loss: 0.3459 - val_acc: 0.8651
Epoch 41/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0731 - acc: 0.9924 - val_loss: 0.3449 - val_acc: 0.8685
Epoch 42/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0692 - acc: 0.9928 - val_loss: 0.3436 - val_acc: 0.8641
Epoch 43/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0654 - acc: 0.9930 - val_loss: 0.3424 - val_acc: 0.8646
Epoch 44/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0618 - acc: 0.9938 - val_loss: 0.3420 - val_acc: 0.8656
Epoch 45/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0584 - acc: 0.9945 - val_loss: 0.3407 - val_acc: 0.8651
Epoch 46/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0554 - acc: 0.9948 - val_loss: 0.3399 - val_acc: 0.8679
Epoch 47/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0523 - acc: 0.9949 - val_loss: 0.3393 - val_acc: 0.8651
Epoch 48/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0495 - acc: 0.9955 - val_loss: 0.3384 - val_acc: 0.8672
Epoch 49/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0468 - acc: 0.9959 - val_loss: 0.3385 - val_acc: 0.8667
Epoch 50/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.0443 - acc: 0.9960 - val_loss: 0.3382 - val_acc: 0.8659
Epoch 51/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0420 - acc: 0.9962 - val_loss: 0.3389 - val_acc: 0.8649
Epoch 52/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.0399 - acc: 0.9964 - val_loss: 0.3386 - val_acc: 0.8623
Epoch 53/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0377 - acc: 0.9965 - val_loss: 0.3388 - val_acc: 0.8628
15664/15664 [==============================] - 0s 20us/step
==== Training loss, score are: 0.03597937589672996 0.9966164453524005 =======
3915/3915 [==============================] - 0s 20us/step
==== CV loss, score are: 0.3387813589962658 0.862835249118269 =======


===== Model: fast_text_none  ========:
 Cross-val log losses are: [0.35613939757900015, 0.34937798852393853, 0.33493111868839998, 0.33962221644782714, 0.3387813549996288]
====== Mean cross-val log loss is: 0.34377041524775886 =========


Timestamp: 2018-Jan-14 02:55:16
Running kfold training with model fast_text_gensim
Shapes: x_train_raw.shape (19579, 21), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 20)
Running preprocess: &lt;function run_NN_preprocess at 0x7f620ee8eea0&gt;
No of unique words found is 247346
No of words occuring more than 2 times is 75104
No of unique words found is after capping is 247346
max sentence length is 1439
mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000
After padding: max sentence size is 161
After padding: mean, median length of sentence is 161.000000 161.000000
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>9584it [00:00, 95826.61it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Creating word embedding matrix for gensim vectors
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>76597it [00:00, 96042.27it/s]
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Found 76597 word vectors.
of--mirth 247345
None
(247346, 20)
Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)
Running fold 1
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_9 (Embedding)      (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_9 ( (None, 20)                0         
_________________________________________________________________
dense_9 (Dense)              (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 0s 29us/step
Before training loss, score are: 1.1140749234772045 0.3340356253610293
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 57us/step - loss: 1.0628 - acc: 0.4271 - val_loss: 1.0470 - val_acc: 0.4341
Epoch 2/150
15663/15663 [==============================] - 1s 46us/step - loss: 1.0204 - acc: 0.4891 - val_loss: 1.0104 - val_acc: 0.4844
Epoch 3/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.9839 - acc: 0.5383 - val_loss: 0.9759 - val_acc: 0.5406
Epoch 4/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.9479 - acc: 0.5811 - val_loss: 0.9433 - val_acc: 0.5896
Epoch 5/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.9118 - acc: 0.6154 - val_loss: 0.9101 - val_acc: 0.6162
Epoch 6/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.8749 - acc: 0.6483 - val_loss: 0.8788 - val_acc: 0.6254
Epoch 7/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.8383 - acc: 0.6741 - val_loss: 0.8478 - val_acc: 0.6466
Epoch 8/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.8018 - acc: 0.6966 - val_loss: 0.8178 - val_acc: 0.6739
Epoch 9/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.7660 - acc: 0.7171 - val_loss: 0.7901 - val_acc: 0.6862
Epoch 10/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.7314 - acc: 0.7358 - val_loss: 0.7635 - val_acc: 0.7056
Epoch 11/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.6977 - acc: 0.7522 - val_loss: 0.7386 - val_acc: 0.7199
Epoch 12/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.6655 - acc: 0.7696 - val_loss: 0.7155 - val_acc: 0.7255
Epoch 13/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.6346 - acc: 0.7841 - val_loss: 0.6939 - val_acc: 0.7337
Epoch 14/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.6052 - acc: 0.8002 - val_loss: 0.6733 - val_acc: 0.7441
Epoch 15/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.5768 - acc: 0.8116 - val_loss: 0.6537 - val_acc: 0.7551
Epoch 16/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.5501 - acc: 0.8235 - val_loss: 0.6361 - val_acc: 0.7615
Epoch 17/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.5245 - acc: 0.8368 - val_loss: 0.6189 - val_acc: 0.7699
Epoch 18/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.5002 - acc: 0.8481 - val_loss: 0.6052 - val_acc: 0.7699
Epoch 19/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.4771 - acc: 0.8550 - val_loss: 0.5910 - val_acc: 0.7771
Epoch 20/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.4552 - acc: 0.8650 - val_loss: 0.5760 - val_acc: 0.7835
Epoch 21/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.4343 - acc: 0.8722 - val_loss: 0.5623 - val_acc: 0.7906
Epoch 22/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.4140 - acc: 0.8819 - val_loss: 0.5500 - val_acc: 0.7972
Epoch 23/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3954 - acc: 0.8894 - val_loss: 0.5393 - val_acc: 0.7978
Epoch 24/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3769 - acc: 0.8956 - val_loss: 0.5297 - val_acc: 0.8008
Epoch 25/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3601 - acc: 0.9013 - val_loss: 0.5185 - val_acc: 0.8080
Epoch 26/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3436 - acc: 0.9088 - val_loss: 0.5087 - val_acc: 0.8092
Epoch 27/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3278 - acc: 0.9137 - val_loss: 0.5010 - val_acc: 0.8108
Epoch 28/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.3130 - acc: 0.9197 - val_loss: 0.4934 - val_acc: 0.8126
Epoch 29/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2989 - acc: 0.9250 - val_loss: 0.4839 - val_acc: 0.8172
Epoch 30/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.2852 - acc: 0.9300 - val_loss: 0.4764 - val_acc: 0.8200
Epoch 31/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2723 - acc: 0.9343 - val_loss: 0.4699 - val_acc: 0.8225
Epoch 32/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2602 - acc: 0.9383 - val_loss: 0.4642 - val_acc: 0.8233
Epoch 33/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2484 - acc: 0.9420 - val_loss: 0.4587 - val_acc: 0.8212
Epoch 34/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2371 - acc: 0.9468 - val_loss: 0.4522 - val_acc: 0.8241
Epoch 35/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2264 - acc: 0.9492 - val_loss: 0.4462 - val_acc: 0.8281
Epoch 36/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2163 - acc: 0.9536 - val_loss: 0.4407 - val_acc: 0.8294
Epoch 37/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2064 - acc: 0.9568 - val_loss: 0.4366 - val_acc: 0.8292
Epoch 38/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1972 - acc: 0.9591 - val_loss: 0.4313 - val_acc: 0.8320
Epoch 39/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1884 - acc: 0.9612 - val_loss: 0.4272 - val_acc: 0.8340
Epoch 40/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1800 - acc: 0.9628 - val_loss: 0.4252 - val_acc: 0.8322
Epoch 41/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1719 - acc: 0.9658 - val_loss: 0.4202 - val_acc: 0.8348
Epoch 42/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1642 - acc: 0.9678 - val_loss: 0.4153 - val_acc: 0.8355
Epoch 43/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1569 - acc: 0.9697 - val_loss: 0.4126 - val_acc: 0.8358
Epoch 44/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1500 - acc: 0.9714 - val_loss: 0.4091 - val_acc: 0.8384
Epoch 45/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1432 - acc: 0.9736 - val_loss: 0.4090 - val_acc: 0.8376
Epoch 46/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1369 - acc: 0.9744 - val_loss: 0.4046 - val_acc: 0.8389
Epoch 47/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1308 - acc: 0.9767 - val_loss: 0.4044 - val_acc: 0.8389
Epoch 48/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1251 - acc: 0.9779 - val_loss: 0.3994 - val_acc: 0.8417
Epoch 49/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1195 - acc: 0.9785 - val_loss: 0.3975 - val_acc: 0.8417
Epoch 50/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1143 - acc: 0.9805 - val_loss: 0.3967 - val_acc: 0.8424
Epoch 51/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1092 - acc: 0.9810 - val_loss: 0.3935 - val_acc: 0.8424
Epoch 52/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1043 - acc: 0.9827 - val_loss: 0.3933 - val_acc: 0.8445
Epoch 53/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0997 - acc: 0.9836 - val_loss: 0.3906 - val_acc: 0.8440
Epoch 54/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0953 - acc: 0.9848 - val_loss: 0.3898 - val_acc: 0.8475
Epoch 55/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0914 - acc: 0.9856 - val_loss: 0.3876 - val_acc: 0.8463
Epoch 56/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0872 - acc: 0.9861 - val_loss: 0.3862 - val_acc: 0.8473
Epoch 57/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0833 - acc: 0.9872 - val_loss: 0.3860 - val_acc: 0.8473
Epoch 58/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0797 - acc: 0.9877 - val_loss: 0.3848 - val_acc: 0.8504
Epoch 59/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0762 - acc: 0.9886 - val_loss: 0.3852 - val_acc: 0.8498
Epoch 60/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0728 - acc: 0.9891 - val_loss: 0.3835 - val_acc: 0.8498
Epoch 61/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0697 - acc: 0.9897 - val_loss: 0.3827 - val_acc: 0.8498
Epoch 62/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0667 - acc: 0.9900 - val_loss: 0.3824 - val_acc: 0.8501
Epoch 63/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0638 - acc: 0.9907 - val_loss: 0.3821 - val_acc: 0.8509
Epoch 64/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0610 - acc: 0.9910 - val_loss: 0.3825 - val_acc: 0.8511
Epoch 65/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0583 - acc: 0.9921 - val_loss: 0.3817 - val_acc: 0.8514
Epoch 66/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0558 - acc: 0.9923 - val_loss: 0.3855 - val_acc: 0.8498
Epoch 67/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0534 - acc: 0.9928 - val_loss: 0.3831 - val_acc: 0.8519
Epoch 68/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.0511 - acc: 0.9934 - val_loss: 0.3826 - val_acc: 0.8514
15663/15663 [==============================] - 0s 20us/step
==== Training loss, score are: 0.04918092788635737 0.9938709059567132 =======
3916/3916 [==============================] - 0s 20us/step
==== CV loss, score are: 0.38258479510465365 0.8513789581205311 =======
Running fold 2
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_10 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_10  (None, 20)                0         
_________________________________________________________________
dense_10 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 0s 30us/step
Before training loss, score are: 1.120552753242262 0.2824490838376436
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 57us/step - loss: 1.0913 - acc: 0.3937 - val_loss: 1.0707 - val_acc: 0.4193
Epoch 2/150
15663/15663 [==============================] - 1s 46us/step - loss: 1.0492 - acc: 0.4501 - val_loss: 1.0366 - val_acc: 0.4448
Epoch 3/150
15663/15663 [==============================] - 1s 45us/step - loss: 1.0120 - acc: 0.4997 - val_loss: 1.0031 - val_acc: 0.5074
Epoch 4/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.9763 - acc: 0.5468 - val_loss: 0.9716 - val_acc: 0.5603
Epoch 5/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.9414 - acc: 0.5905 - val_loss: 0.9426 - val_acc: 0.5751
Epoch 6/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.9063 - acc: 0.6205 - val_loss: 0.9122 - val_acc: 0.6085
Epoch 7/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.8713 - acc: 0.6494 - val_loss: 0.8835 - val_acc: 0.6272
Epoch 8/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.8365 - acc: 0.6746 - val_loss: 0.8557 - val_acc: 0.6522
Epoch 9/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.8026 - acc: 0.6984 - val_loss: 0.8292 - val_acc: 0.6596
Epoch 10/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.7687 - acc: 0.7170 - val_loss: 0.8035 - val_acc: 0.6759
Epoch 11/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.7361 - acc: 0.7366 - val_loss: 0.7788 - val_acc: 0.6938
Epoch 12/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.7039 - acc: 0.7526 - val_loss: 0.7565 - val_acc: 0.7025
Epoch 13/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.6730 - acc: 0.7678 - val_loss: 0.7341 - val_acc: 0.7114
Epoch 14/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.6433 - acc: 0.7820 - val_loss: 0.7133 - val_acc: 0.7194
Epoch 15/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.6145 - acc: 0.7943 - val_loss: 0.6931 - val_acc: 0.7291
Epoch 16/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.5868 - acc: 0.8078 - val_loss: 0.6750 - val_acc: 0.7360
Epoch 17/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5602 - acc: 0.8169 - val_loss: 0.6567 - val_acc: 0.7490
Epoch 18/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.5349 - acc: 0.8309 - val_loss: 0.6412 - val_acc: 0.7505
Epoch 19/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.5105 - acc: 0.8401 - val_loss: 0.6237 - val_acc: 0.7615
Epoch 20/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4874 - acc: 0.8513 - val_loss: 0.6088 - val_acc: 0.7684
Epoch 21/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4651 - acc: 0.8592 - val_loss: 0.5950 - val_acc: 0.7697
Epoch 22/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4440 - acc: 0.8704 - val_loss: 0.5809 - val_acc: 0.7760
Epoch 23/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.4234 - acc: 0.8779 - val_loss: 0.5682 - val_acc: 0.7801
Epoch 24/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4042 - acc: 0.8846 - val_loss: 0.5562 - val_acc: 0.7840
Epoch 25/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3858 - acc: 0.8921 - val_loss: 0.5446 - val_acc: 0.7911
Epoch 26/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3682 - acc: 0.8999 - val_loss: 0.5337 - val_acc: 0.7939
Epoch 27/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3514 - acc: 0.9051 - val_loss: 0.5248 - val_acc: 0.8018
Epoch 28/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.3352 - acc: 0.9129 - val_loss: 0.5139 - val_acc: 0.8067
Epoch 29/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3198 - acc: 0.9194 - val_loss: 0.5047 - val_acc: 0.8103
Epoch 30/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3053 - acc: 0.9252 - val_loss: 0.4964 - val_acc: 0.8126
Epoch 31/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2914 - acc: 0.9301 - val_loss: 0.4880 - val_acc: 0.8154
Epoch 32/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2780 - acc: 0.9351 - val_loss: 0.4808 - val_acc: 0.8195
Epoch 33/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2655 - acc: 0.9386 - val_loss: 0.4733 - val_acc: 0.8195
Epoch 34/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.2534 - acc: 0.9427 - val_loss: 0.4669 - val_acc: 0.8233
Epoch 35/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.2421 - acc: 0.9467 - val_loss: 0.4605 - val_acc: 0.8228
Epoch 36/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.2309 - acc: 0.9504 - val_loss: 0.4548 - val_acc: 0.8246
Epoch 37/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2205 - acc: 0.9528 - val_loss: 0.4489 - val_acc: 0.8266
Epoch 38/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.2106 - acc: 0.9549 - val_loss: 0.4436 - val_acc: 0.8297
Epoch 39/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.2009 - acc: 0.9588 - val_loss: 0.4390 - val_acc: 0.8299
Epoch 40/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1920 - acc: 0.9608 - val_loss: 0.4331 - val_acc: 0.8335
Epoch 41/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1834 - acc: 0.9640 - val_loss: 0.4301 - val_acc: 0.8327
Epoch 42/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1751 - acc: 0.9653 - val_loss: 0.4247 - val_acc: 0.8343
Epoch 43/150
15663/15663 [==============================] - 1s 46us/step - loss: 0.1673 - acc: 0.9678 - val_loss: 0.4210 - val_acc: 0.8353
Epoch 44/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1597 - acc: 0.9701 - val_loss: 0.4179 - val_acc: 0.8348
Epoch 45/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1525 - acc: 0.9715 - val_loss: 0.4134 - val_acc: 0.8407
Epoch 46/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1458 - acc: 0.9738 - val_loss: 0.4099 - val_acc: 0.8389
Epoch 47/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1393 - acc: 0.9749 - val_loss: 0.4070 - val_acc: 0.8396
Epoch 48/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1329 - acc: 0.9770 - val_loss: 0.4054 - val_acc: 0.8414
Epoch 49/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1272 - acc: 0.9780 - val_loss: 0.4017 - val_acc: 0.8427
Epoch 50/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1215 - acc: 0.9795 - val_loss: 0.3987 - val_acc: 0.8447
Epoch 51/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1160 - acc: 0.9804 - val_loss: 0.3963 - val_acc: 0.8463
Epoch 52/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1109 - acc: 0.9817 - val_loss: 0.3943 - val_acc: 0.8468
Epoch 53/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1057 - acc: 0.9831 - val_loss: 0.3933 - val_acc: 0.8455
Epoch 54/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1011 - acc: 0.9838 - val_loss: 0.3908 - val_acc: 0.8470
Epoch 55/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0964 - acc: 0.9850 - val_loss: 0.3894 - val_acc: 0.8473
Epoch 56/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0923 - acc: 0.9856 - val_loss: 0.3878 - val_acc: 0.8470
Epoch 57/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0881 - acc: 0.9860 - val_loss: 0.3861 - val_acc: 0.8486
Epoch 58/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0842 - acc: 0.9868 - val_loss: 0.3851 - val_acc: 0.8486
Epoch 59/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0804 - acc: 0.9879 - val_loss: 0.3840 - val_acc: 0.8486
Epoch 60/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0767 - acc: 0.9887 - val_loss: 0.3840 - val_acc: 0.8506
Epoch 61/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0735 - acc: 0.9893 - val_loss: 0.3820 - val_acc: 0.8496
Epoch 62/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0702 - acc: 0.9902 - val_loss: 0.3813 - val_acc: 0.8491
Epoch 63/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0671 - acc: 0.9905 - val_loss: 0.3801 - val_acc: 0.8483
Epoch 64/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0640 - acc: 0.9909 - val_loss: 0.3794 - val_acc: 0.8496
Epoch 65/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0613 - acc: 0.9913 - val_loss: 0.3790 - val_acc: 0.8483
Epoch 66/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0585 - acc: 0.9923 - val_loss: 0.3786 - val_acc: 0.8501
Epoch 67/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0560 - acc: 0.9928 - val_loss: 0.3787 - val_acc: 0.8509
Epoch 68/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0535 - acc: 0.9933 - val_loss: 0.3782 - val_acc: 0.8501
Epoch 69/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0512 - acc: 0.9936 - val_loss: 0.3780 - val_acc: 0.8501
Epoch 70/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0489 - acc: 0.9939 - val_loss: 0.3785 - val_acc: 0.8504
Epoch 71/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0467 - acc: 0.9942 - val_loss: 0.3783 - val_acc: 0.8516
Epoch 72/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0446 - acc: 0.9946 - val_loss: 0.3789 - val_acc: 0.8498
15663/15663 [==============================] - 0s 20us/step
==== Training loss, score are: 0.0429314952176025 0.9947647321713593 =======
3916/3916 [==============================] - 0s 20us/step
==== CV loss, score are: 0.3788707141400114 0.849846782431052 =======
Running fold 3
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_11 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_11  (None, 20)                0         
_________________________________________________________________
dense_11 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 0s 30us/step
Before training loss, score are: 1.2773297683021951 0.30837004406618246
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 57us/step - loss: 1.1382 - acc: 0.3461 - val_loss: 1.0863 - val_acc: 0.3864
Epoch 2/150
15663/15663 [==============================] - 1s 44us/step - loss: 1.0679 - acc: 0.4151 - val_loss: 1.0496 - val_acc: 0.4489
Epoch 3/150
15663/15663 [==============================] - 1s 45us/step - loss: 1.0291 - acc: 0.4800 - val_loss: 1.0163 - val_acc: 0.4962
Epoch 4/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.9923 - acc: 0.5299 - val_loss: 0.9844 - val_acc: 0.5472
Epoch 5/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.9562 - acc: 0.5737 - val_loss: 0.9533 - val_acc: 0.5802
Epoch 6/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.9201 - acc: 0.6119 - val_loss: 0.9233 - val_acc: 0.5955
Epoch 7/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.8840 - acc: 0.6391 - val_loss: 0.8930 - val_acc: 0.6364
Epoch 8/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.8481 - acc: 0.6651 - val_loss: 0.8642 - val_acc: 0.6438
Epoch 9/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.8126 - acc: 0.6881 - val_loss: 0.8379 - val_acc: 0.6486
Epoch 10/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.7782 - acc: 0.7087 - val_loss: 0.8095 - val_acc: 0.6790
Epoch 11/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.7441 - acc: 0.7273 - val_loss: 0.7849 - val_acc: 0.6813
Epoch 12/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.7118 - acc: 0.7456 - val_loss: 0.7601 - val_acc: 0.6984
Epoch 13/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.6802 - acc: 0.7600 - val_loss: 0.7369 - val_acc: 0.7148
Epoch 14/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.6498 - acc: 0.7770 - val_loss: 0.7152 - val_acc: 0.7165
Epoch 15/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.6206 - acc: 0.7903 - val_loss: 0.6944 - val_acc: 0.7324
Epoch 16/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.5923 - acc: 0.8042 - val_loss: 0.6752 - val_acc: 0.7357
Epoch 17/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.5656 - acc: 0.8145 - val_loss: 0.6560 - val_acc: 0.7477
Epoch 18/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.5395 - acc: 0.8270 - val_loss: 0.6385 - val_acc: 0.7541
Epoch 19/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.5148 - acc: 0.8391 - val_loss: 0.6216 - val_acc: 0.7610
Epoch 20/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.4909 - acc: 0.8493 - val_loss: 0.6062 - val_acc: 0.7648
Epoch 21/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.4683 - acc: 0.8573 - val_loss: 0.5910 - val_acc: 0.7748
Epoch 22/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.4468 - acc: 0.8687 - val_loss: 0.5777 - val_acc: 0.7689
Epoch 23/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.4262 - acc: 0.8742 - val_loss: 0.5634 - val_acc: 0.7850
Epoch 24/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.4063 - acc: 0.8851 - val_loss: 0.5504 - val_acc: 0.7891
Epoch 25/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.3874 - acc: 0.8911 - val_loss: 0.5387 - val_acc: 0.7934
Epoch 26/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.3695 - acc: 0.8989 - val_loss: 0.5273 - val_acc: 0.7967
Epoch 27/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.3524 - acc: 0.9051 - val_loss: 0.5166 - val_acc: 0.8008
Epoch 28/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.3361 - acc: 0.9110 - val_loss: 0.5071 - val_acc: 0.8067
Epoch 29/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.3207 - acc: 0.9172 - val_loss: 0.4971 - val_acc: 0.8108
Epoch 30/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.3058 - acc: 0.9224 - val_loss: 0.4879 - val_acc: 0.8131
Epoch 31/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.2917 - acc: 0.9273 - val_loss: 0.4788 - val_acc: 0.8159
Epoch 32/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.2783 - acc: 0.9321 - val_loss: 0.4710 - val_acc: 0.8151
Epoch 33/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.2655 - acc: 0.9356 - val_loss: 0.4634 - val_acc: 0.8179
Epoch 34/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.2533 - acc: 0.9404 - val_loss: 0.4556 - val_acc: 0.8235
Epoch 35/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.2417 - acc: 0.9440 - val_loss: 0.4486 - val_acc: 0.8281
Epoch 36/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.2308 - acc: 0.9469 - val_loss: 0.4422 - val_acc: 0.8302
Epoch 37/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.2202 - acc: 0.9514 - val_loss: 0.4356 - val_acc: 0.8338
Epoch 38/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.2100 - acc: 0.9554 - val_loss: 0.4306 - val_acc: 0.8340
Epoch 39/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.2007 - acc: 0.9581 - val_loss: 0.4259 - val_acc: 0.8350
Epoch 40/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1916 - acc: 0.9612 - val_loss: 0.4199 - val_acc: 0.8378
Epoch 41/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1829 - acc: 0.9628 - val_loss: 0.4142 - val_acc: 0.8401
Epoch 42/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1746 - acc: 0.9660 - val_loss: 0.4094 - val_acc: 0.8432
Epoch 43/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1667 - acc: 0.9684 - val_loss: 0.4052 - val_acc: 0.8458
Epoch 44/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1592 - acc: 0.9699 - val_loss: 0.4022 - val_acc: 0.8437
Epoch 45/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1520 - acc: 0.9718 - val_loss: 0.3980 - val_acc: 0.8478
Epoch 46/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1452 - acc: 0.9741 - val_loss: 0.3936 - val_acc: 0.8506
Epoch 47/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1388 - acc: 0.9754 - val_loss: 0.3899 - val_acc: 0.8521
Epoch 48/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1324 - acc: 0.9771 - val_loss: 0.3874 - val_acc: 0.8521
Epoch 49/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1267 - acc: 0.9784 - val_loss: 0.3839 - val_acc: 0.8537
Epoch 50/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1209 - acc: 0.9801 - val_loss: 0.3814 - val_acc: 0.8529
Epoch 51/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1154 - acc: 0.9814 - val_loss: 0.3782 - val_acc: 0.8547
Epoch 52/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1102 - acc: 0.9822 - val_loss: 0.3757 - val_acc: 0.8552
Epoch 53/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1052 - acc: 0.9834 - val_loss: 0.3733 - val_acc: 0.8580
Epoch 54/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1006 - acc: 0.9845 - val_loss: 0.3713 - val_acc: 0.8573
Epoch 55/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0962 - acc: 0.9859 - val_loss: 0.3703 - val_acc: 0.8562
Epoch 56/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0919 - acc: 0.9861 - val_loss: 0.3683 - val_acc: 0.8608
Epoch 57/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0880 - acc: 0.9872 - val_loss: 0.3667 - val_acc: 0.8585
Epoch 58/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0839 - acc: 0.9881 - val_loss: 0.3642 - val_acc: 0.8608
Epoch 59/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0802 - acc: 0.9887 - val_loss: 0.3624 - val_acc: 0.8616
Epoch 60/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0766 - acc: 0.9894 - val_loss: 0.3611 - val_acc: 0.8613
Epoch 61/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0733 - acc: 0.9899 - val_loss: 0.3601 - val_acc: 0.8603
Epoch 62/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0699 - acc: 0.9901 - val_loss: 0.3587 - val_acc: 0.8621
Epoch 63/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0669 - acc: 0.9906 - val_loss: 0.3584 - val_acc: 0.8603
Epoch 64/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0639 - acc: 0.9913 - val_loss: 0.3571 - val_acc: 0.8618
Epoch 65/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0611 - acc: 0.9916 - val_loss: 0.3584 - val_acc: 0.8636
Epoch 66/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0584 - acc: 0.9919 - val_loss: 0.3556 - val_acc: 0.8639
Epoch 67/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0558 - acc: 0.9923 - val_loss: 0.3546 - val_acc: 0.8611
Epoch 68/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0533 - acc: 0.9927 - val_loss: 0.3544 - val_acc: 0.8649
Epoch 69/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0510 - acc: 0.9928 - val_loss: 0.3549 - val_acc: 0.8639
Epoch 70/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0487 - acc: 0.9936 - val_loss: 0.3543 - val_acc: 0.8649
Epoch 71/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0465 - acc: 0.9947 - val_loss: 0.3553 - val_acc: 0.8649
Epoch 72/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0445 - acc: 0.9946 - val_loss: 0.3545 - val_acc: 0.8636
Epoch 73/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0426 - acc: 0.9951 - val_loss: 0.3540 - val_acc: 0.8672
Epoch 74/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0407 - acc: 0.9953 - val_loss: 0.3535 - val_acc: 0.8636
Epoch 75/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0389 - acc: 0.9958 - val_loss: 0.3543 - val_acc: 0.8659
Epoch 76/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0373 - acc: 0.9958 - val_loss: 0.3542 - val_acc: 0.8672
Epoch 77/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0357 - acc: 0.9959 - val_loss: 0.3541 - val_acc: 0.8675
15663/15663 [==============================] - 0s 20us/step
==== Training loss, score are: 0.034232947076361334 0.9964246951414161 =======
3916/3916 [==============================] - 0s 20us/step
==== CV loss, score are: 0.35414784215200423 0.8674668028600613 =======
Running fold 4
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_12 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_12  (None, 20)                0         
_________________________________________________________________
dense_12 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 0s 31us/step
Before training loss, score are: 1.1268716988751983 0.3182659771464409
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 58us/step - loss: 1.0819 - acc: 0.3912 - val_loss: 1.0605 - val_acc: 0.4277
Epoch 2/150
15663/15663 [==============================] - 1s 45us/step - loss: 1.0375 - acc: 0.4562 - val_loss: 1.0222 - val_acc: 0.4780
Epoch 3/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.9988 - acc: 0.5181 - val_loss: 0.9869 - val_acc: 0.5332
Epoch 4/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.9609 - acc: 0.5667 - val_loss: 0.9528 - val_acc: 0.5720
Epoch 5/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.9228 - acc: 0.6160 - val_loss: 0.9193 - val_acc: 0.6210
Epoch 6/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.8838 - acc: 0.6479 - val_loss: 0.8861 - val_acc: 0.6443
Epoch 7/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.8448 - acc: 0.6817 - val_loss: 0.8542 - val_acc: 0.6601
Epoch 8/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.8063 - acc: 0.7008 - val_loss: 0.8237 - val_acc: 0.6846
Epoch 9/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.7690 - acc: 0.7234 - val_loss: 0.7946 - val_acc: 0.6925
Epoch 10/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.7330 - acc: 0.7391 - val_loss: 0.7679 - val_acc: 0.7053
Epoch 11/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.6985 - acc: 0.7587 - val_loss: 0.7426 - val_acc: 0.7155
Epoch 12/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.6658 - acc: 0.7723 - val_loss: 0.7200 - val_acc: 0.7316
Epoch 13/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.6347 - acc: 0.7883 - val_loss: 0.6982 - val_acc: 0.7388
Epoch 14/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.6053 - acc: 0.8013 - val_loss: 0.6768 - val_acc: 0.7444
Epoch 15/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.5773 - acc: 0.8136 - val_loss: 0.6605 - val_acc: 0.7533
Epoch 16/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.5507 - acc: 0.8241 - val_loss: 0.6400 - val_acc: 0.7582
Epoch 17/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.5254 - acc: 0.8335 - val_loss: 0.6229 - val_acc: 0.7640
Epoch 18/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.5013 - acc: 0.8449 - val_loss: 0.6070 - val_acc: 0.7730
Epoch 19/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.4787 - acc: 0.8546 - val_loss: 0.5923 - val_acc: 0.7758
Epoch 20/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.4568 - acc: 0.8631 - val_loss: 0.5790 - val_acc: 0.7860
Epoch 21/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.4360 - acc: 0.8714 - val_loss: 0.5661 - val_acc: 0.7886
Epoch 22/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.4160 - acc: 0.8823 - val_loss: 0.5532 - val_acc: 0.7944
Epoch 23/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.3972 - acc: 0.8911 - val_loss: 0.5407 - val_acc: 0.7947
Epoch 24/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.3792 - acc: 0.8964 - val_loss: 0.5299 - val_acc: 0.8031
Epoch 25/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.3620 - acc: 0.9031 - val_loss: 0.5190 - val_acc: 0.8057
Epoch 26/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.3455 - acc: 0.9085 - val_loss: 0.5091 - val_acc: 0.8115
Epoch 27/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.3301 - acc: 0.9143 - val_loss: 0.5007 - val_acc: 0.8164
Epoch 28/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.3150 - acc: 0.9199 - val_loss: 0.4910 - val_acc: 0.8144
Epoch 29/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.3009 - acc: 0.9256 - val_loss: 0.4825 - val_acc: 0.8184
Epoch 30/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.2875 - acc: 0.9306 - val_loss: 0.4746 - val_acc: 0.8225
Epoch 31/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.2745 - acc: 0.9346 - val_loss: 0.4671 - val_acc: 0.8274
Epoch 32/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.2622 - acc: 0.9385 - val_loss: 0.4599 - val_acc: 0.8287
Epoch 33/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.2504 - acc: 0.9418 - val_loss: 0.4539 - val_acc: 0.8302
Epoch 34/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.2393 - acc: 0.9457 - val_loss: 0.4465 - val_acc: 0.8315
Epoch 35/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.2286 - acc: 0.9485 - val_loss: 0.4406 - val_acc: 0.8332
Epoch 36/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.2182 - acc: 0.9523 - val_loss: 0.4371 - val_acc: 0.8366
Epoch 37/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.2088 - acc: 0.9558 - val_loss: 0.4297 - val_acc: 0.8361
Epoch 38/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1993 - acc: 0.9583 - val_loss: 0.4254 - val_acc: 0.8376
Epoch 39/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1906 - acc: 0.9604 - val_loss: 0.4199 - val_acc: 0.8386
Epoch 40/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1823 - acc: 0.9641 - val_loss: 0.4152 - val_acc: 0.8414
Epoch 41/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1743 - acc: 0.9659 - val_loss: 0.4114 - val_acc: 0.8419
Epoch 42/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1666 - acc: 0.9681 - val_loss: 0.4073 - val_acc: 0.8404
Epoch 43/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1592 - acc: 0.9700 - val_loss: 0.4031 - val_acc: 0.8455
Epoch 44/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1522 - acc: 0.9724 - val_loss: 0.4002 - val_acc: 0.8468
Epoch 45/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1455 - acc: 0.9737 - val_loss: 0.3965 - val_acc: 0.8478
Epoch 46/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1391 - acc: 0.9754 - val_loss: 0.3928 - val_acc: 0.8493
Epoch 47/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1330 - acc: 0.9767 - val_loss: 0.3900 - val_acc: 0.8493
Epoch 48/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1271 - acc: 0.9781 - val_loss: 0.3870 - val_acc: 0.8498
Epoch 49/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1216 - acc: 0.9795 - val_loss: 0.3849 - val_acc: 0.8519
Epoch 50/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1163 - acc: 0.9807 - val_loss: 0.3818 - val_acc: 0.8506
Epoch 51/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.1111 - acc: 0.9824 - val_loss: 0.3796 - val_acc: 0.8514
Epoch 52/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1062 - acc: 0.9833 - val_loss: 0.3778 - val_acc: 0.8542
Epoch 53/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.1016 - acc: 0.9837 - val_loss: 0.3755 - val_acc: 0.8527
Epoch 54/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0974 - acc: 0.9849 - val_loss: 0.3740 - val_acc: 0.8527
Epoch 55/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0930 - acc: 0.9857 - val_loss: 0.3721 - val_acc: 0.8532
Epoch 56/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0889 - acc: 0.9867 - val_loss: 0.3706 - val_acc: 0.8534
Epoch 57/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0850 - acc: 0.9872 - val_loss: 0.3691 - val_acc: 0.8562
Epoch 58/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0813 - acc: 0.9875 - val_loss: 0.3674 - val_acc: 0.8557
Epoch 59/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0778 - acc: 0.9886 - val_loss: 0.3666 - val_acc: 0.8550
Epoch 60/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0744 - acc: 0.9891 - val_loss: 0.3650 - val_acc: 0.8583
Epoch 61/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0711 - acc: 0.9897 - val_loss: 0.3640 - val_acc: 0.8565
Epoch 62/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0680 - acc: 0.9903 - val_loss: 0.3636 - val_acc: 0.8567
Epoch 63/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0651 - acc: 0.9905 - val_loss: 0.3621 - val_acc: 0.8585
Epoch 64/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0622 - acc: 0.9909 - val_loss: 0.3616 - val_acc: 0.8603
Epoch 65/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0596 - acc: 0.9915 - val_loss: 0.3610 - val_acc: 0.8588
Epoch 66/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0569 - acc: 0.9918 - val_loss: 0.3604 - val_acc: 0.8601
Epoch 67/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0545 - acc: 0.9918 - val_loss: 0.3605 - val_acc: 0.8575
Epoch 68/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0521 - acc: 0.9926 - val_loss: 0.3595 - val_acc: 0.8596
Epoch 69/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0500 - acc: 0.9933 - val_loss: 0.3605 - val_acc: 0.8590
Epoch 70/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0477 - acc: 0.9936 - val_loss: 0.3598 - val_acc: 0.8593
Epoch 71/150
15663/15663 [==============================] - 1s 44us/step - loss: 0.0457 - acc: 0.9937 - val_loss: 0.3591 - val_acc: 0.8585
Epoch 72/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0437 - acc: 0.9944 - val_loss: 0.3589 - val_acc: 0.8611
Epoch 73/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0418 - acc: 0.9946 - val_loss: 0.3586 - val_acc: 0.8585
Epoch 74/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0400 - acc: 0.9950 - val_loss: 0.3587 - val_acc: 0.8583
Epoch 75/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0382 - acc: 0.9956 - val_loss: 0.3599 - val_acc: 0.8562
Epoch 76/150
15663/15663 [==============================] - 1s 45us/step - loss: 0.0366 - acc: 0.9960 - val_loss: 0.3592 - val_acc: 0.8585
15663/15663 [==============================] - 0s 20us/step
==== Training loss, score are: 0.0352395822113952 0.9963608504117986 =======
3916/3916 [==============================] - 0s 21us/step
==== CV loss, score are: 0.3592084593713223 0.8585291113989832 =======
Running fold 5
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_13 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_13  (None, 20)                0         
_________________________________________________________________
dense_13 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15664/15664 [==============================] - 1s 33us/step
Before training loss, score are: 1.305587020909092 0.287985188968335
Train on 15664 samples, validate on 3915 samples
Epoch 1/150
15664/15664 [==============================] - 1s 60us/step - loss: 1.1599 - acc: 0.3168 - val_loss: 1.0961 - val_acc: 0.3898
Epoch 2/150
15664/15664 [==============================] - 1s 45us/step - loss: 1.0843 - acc: 0.4070 - val_loss: 1.0592 - val_acc: 0.4506
Epoch 3/150
15664/15664 [==============================] - 1s 45us/step - loss: 1.0468 - acc: 0.4595 - val_loss: 1.0265 - val_acc: 0.5027
Epoch 4/150
15664/15664 [==============================] - 1s 45us/step - loss: 1.0107 - acc: 0.5057 - val_loss: 0.9931 - val_acc: 0.5423
Epoch 5/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.9746 - acc: 0.5522 - val_loss: 0.9607 - val_acc: 0.5954
Epoch 6/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.9379 - acc: 0.5999 - val_loss: 0.9271 - val_acc: 0.6238
Epoch 7/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.9007 - acc: 0.6440 - val_loss: 0.8942 - val_acc: 0.6455
Epoch 8/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.8635 - acc: 0.6710 - val_loss: 0.8631 - val_acc: 0.6746
Epoch 9/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.8263 - acc: 0.6990 - val_loss: 0.8323 - val_acc: 0.6912
Epoch 10/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.7900 - acc: 0.7191 - val_loss: 0.8032 - val_acc: 0.7055
Epoch 11/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.7546 - acc: 0.7393 - val_loss: 0.7749 - val_acc: 0.7160
Epoch 12/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.7203 - acc: 0.7562 - val_loss: 0.7494 - val_acc: 0.7290
Epoch 13/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.6874 - acc: 0.7703 - val_loss: 0.7242 - val_acc: 0.7367
Epoch 14/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.6558 - acc: 0.7866 - val_loss: 0.7016 - val_acc: 0.7436
Epoch 15/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.6257 - acc: 0.7993 - val_loss: 0.6792 - val_acc: 0.7533
Epoch 16/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.5966 - acc: 0.8128 - val_loss: 0.6590 - val_acc: 0.7596
Epoch 17/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.5695 - acc: 0.8228 - val_loss: 0.6410 - val_acc: 0.7699
Epoch 18/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.5429 - acc: 0.8325 - val_loss: 0.6237 - val_acc: 0.7752
Epoch 19/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.5177 - acc: 0.8442 - val_loss: 0.6059 - val_acc: 0.7808
Epoch 20/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.4939 - acc: 0.8528 - val_loss: 0.5906 - val_acc: 0.7844
Epoch 21/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.4710 - acc: 0.8625 - val_loss: 0.5757 - val_acc: 0.7888
Epoch 22/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.4493 - acc: 0.8697 - val_loss: 0.5616 - val_acc: 0.7923
Epoch 23/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.4288 - acc: 0.8784 - val_loss: 0.5498 - val_acc: 0.7997
Epoch 24/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.4088 - acc: 0.8855 - val_loss: 0.5366 - val_acc: 0.8026
Epoch 25/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.3900 - acc: 0.8931 - val_loss: 0.5259 - val_acc: 0.8059
Epoch 26/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.3723 - acc: 0.8987 - val_loss: 0.5161 - val_acc: 0.8146
Epoch 27/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.3548 - acc: 0.9056 - val_loss: 0.5052 - val_acc: 0.8181
Epoch 28/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.3386 - acc: 0.9129 - val_loss: 0.4950 - val_acc: 0.8202
Epoch 29/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.3229 - acc: 0.9189 - val_loss: 0.4870 - val_acc: 0.8230
Epoch 30/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.3083 - acc: 0.9247 - val_loss: 0.4768 - val_acc: 0.8250
Epoch 31/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.2942 - acc: 0.9284 - val_loss: 0.4702 - val_acc: 0.8263
Epoch 32/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.2808 - acc: 0.9334 - val_loss: 0.4636 - val_acc: 0.8286
Epoch 33/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.2680 - acc: 0.9376 - val_loss: 0.4540 - val_acc: 0.8314
Epoch 34/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.2559 - acc: 0.9418 - val_loss: 0.4481 - val_acc: 0.8335
Epoch 35/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.2441 - acc: 0.9462 - val_loss: 0.4410 - val_acc: 0.8342
Epoch 36/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.2329 - acc: 0.9497 - val_loss: 0.4349 - val_acc: 0.8368
Epoch 37/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.2224 - acc: 0.9523 - val_loss: 0.4295 - val_acc: 0.8388
Epoch 38/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.2124 - acc: 0.9556 - val_loss: 0.4235 - val_acc: 0.8406
Epoch 39/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.2029 - acc: 0.9584 - val_loss: 0.4197 - val_acc: 0.8427
Epoch 40/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1936 - acc: 0.9611 - val_loss: 0.4134 - val_acc: 0.8450
Epoch 41/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.1848 - acc: 0.9636 - val_loss: 0.4091 - val_acc: 0.8450
Epoch 42/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.1767 - acc: 0.9656 - val_loss: 0.4046 - val_acc: 0.8465
Epoch 43/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.1686 - acc: 0.9676 - val_loss: 0.4009 - val_acc: 0.8465
Epoch 44/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.1610 - acc: 0.9692 - val_loss: 0.3992 - val_acc: 0.8490
Epoch 45/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1538 - acc: 0.9713 - val_loss: 0.3936 - val_acc: 0.8496
Epoch 46/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.1470 - acc: 0.9727 - val_loss: 0.3909 - val_acc: 0.8511
Epoch 47/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1402 - acc: 0.9748 - val_loss: 0.3883 - val_acc: 0.8511
Epoch 48/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1341 - acc: 0.9756 - val_loss: 0.3847 - val_acc: 0.8508
Epoch 49/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.1281 - acc: 0.9778 - val_loss: 0.3821 - val_acc: 0.8529
Epoch 50/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.1224 - acc: 0.9794 - val_loss: 0.3794 - val_acc: 0.8536
Epoch 51/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.1168 - acc: 0.9802 - val_loss: 0.3767 - val_acc: 0.8570
Epoch 52/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.1117 - acc: 0.9814 - val_loss: 0.3750 - val_acc: 0.8544
Epoch 53/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.1067 - acc: 0.9826 - val_loss: 0.3718 - val_acc: 0.8559
Epoch 54/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.1019 - acc: 0.9833 - val_loss: 0.3707 - val_acc: 0.8549
Epoch 55/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0974 - acc: 0.9841 - val_loss: 0.3701 - val_acc: 0.8554
Epoch 56/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0931 - acc: 0.9848 - val_loss: 0.3674 - val_acc: 0.8562
Epoch 57/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.0891 - acc: 0.9860 - val_loss: 0.3650 - val_acc: 0.8564
Epoch 58/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0851 - acc: 0.9870 - val_loss: 0.3639 - val_acc: 0.8570
Epoch 59/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0812 - acc: 0.9877 - val_loss: 0.3631 - val_acc: 0.8559
Epoch 60/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0777 - acc: 0.9887 - val_loss: 0.3614 - val_acc: 0.8559
Epoch 61/150
15664/15664 [==============================] - 1s 45us/step - loss: 0.0743 - acc: 0.9888 - val_loss: 0.3603 - val_acc: 0.8564
Epoch 62/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.0710 - acc: 0.9896 - val_loss: 0.3604 - val_acc: 0.8564
Epoch 63/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0678 - acc: 0.9903 - val_loss: 0.3585 - val_acc: 0.8562
Epoch 64/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0648 - acc: 0.9910 - val_loss: 0.3579 - val_acc: 0.8577
Epoch 65/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.0621 - acc: 0.9916 - val_loss: 0.3582 - val_acc: 0.8590
Epoch 66/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0594 - acc: 0.9918 - val_loss: 0.3575 - val_acc: 0.8575
Epoch 67/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0567 - acc: 0.9926 - val_loss: 0.3565 - val_acc: 0.8582
Epoch 68/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0543 - acc: 0.9928 - val_loss: 0.3562 - val_acc: 0.8593
Epoch 69/150
15664/15664 [==============================] - 1s 46us/step - loss: 0.0520 - acc: 0.9930 - val_loss: 0.3564 - val_acc: 0.8577
Epoch 70/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.0495 - acc: 0.9934 - val_loss: 0.3583 - val_acc: 0.8585
Epoch 71/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0474 - acc: 0.9938 - val_loss: 0.3566 - val_acc: 0.8587
15664/15664 [==============================] - 0s 22us/step
==== Training loss, score are: 0.04577144218058886 0.994573544433095 =======
3915/3915 [==============================] - 0s 22us/step
==== CV loss, score are: 0.35661753699240556 0.8587484035607651 =======


===== Model: fast_text_gensim  ========:
 Cross-val log losses are: [0.38271956085864439, 0.378870709472864, 0.35414783893485785, 0.35920845544401081, 0.35661753207869984]
====== Mean cross-val log loss is: 0.3663128193578154 =========


Timestamp: 2018-Jan-14 02:59:52
Running kfold training with model fast_text_glove
Shapes: x_train_raw.shape (19579, 24), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 23)
Running preprocess: &lt;function run_NN_preprocess at 0x7f620ee8eea0&gt;
No of unique words found is 247346
No of words occuring more than 2 times is 75104
No of unique words found is after capping is 247346
max sentence length is 1439
mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000
After padding: max sentence size is 161
After padding: mean, median length of sentence is 161.000000 161.000000
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>5007it [00:00, 50060.98it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Creating word embedding matrix for glove vectors
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>400000it [00:08, 49133.71it/s]
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Found 400000 word vectors.
of--mirth 247345
None
(247346, 100)
Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)
Running fold 1
Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_14 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
global_average_pooling1d_14  (None, 100)               0         
_________________________________________________________________
dense_14 (Dense)             (None, 3)                 303       
=================================================================
Total params: 24,734,903
Trainable params: 303
Non-trainable params: 24,734,600
_________________________________________________________________
15663/15663 [==============================] - 1s 37us/step
Before training loss, score are: 1.0987276865864732 0.31181765946172585
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 0s 31us/step - loss: 1.0848 - acc: 0.3996 - val_loss: 1.0793 - val_acc: 0.3907
Epoch 2/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0713 - acc: 0.4118 - val_loss: 1.0675 - val_acc: 0.4002
Epoch 3/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0599 - acc: 0.4299 - val_loss: 1.0573 - val_acc: 0.4132
Epoch 4/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0496 - acc: 0.4449 - val_loss: 1.0475 - val_acc: 0.4374
Epoch 5/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0403 - acc: 0.4615 - val_loss: 1.0386 - val_acc: 0.4535
Epoch 6/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0318 - acc: 0.4744 - val_loss: 1.0307 - val_acc: 0.4630
Epoch 7/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0238 - acc: 0.4823 - val_loss: 1.0224 - val_acc: 0.5010
Epoch 8/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0166 - acc: 0.5004 - val_loss: 1.0157 - val_acc: 0.4977
Epoch 9/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0098 - acc: 0.5074 - val_loss: 1.0094 - val_acc: 0.5056
Epoch 10/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0032 - acc: 0.5145 - val_loss: 1.0028 - val_acc: 0.5207
Epoch 11/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9971 - acc: 0.5212 - val_loss: 0.9968 - val_acc: 0.5217
Epoch 12/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9914 - acc: 0.5286 - val_loss: 0.9912 - val_acc: 0.5309
Epoch 13/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9858 - acc: 0.5380 - val_loss: 0.9866 - val_acc: 0.5299
Epoch 14/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9806 - acc: 0.5408 - val_loss: 0.9810 - val_acc: 0.5396
Epoch 15/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9757 - acc: 0.5433 - val_loss: 0.9760 - val_acc: 0.5434
Epoch 16/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9707 - acc: 0.5528 - val_loss: 0.9724 - val_acc: 0.5434
Epoch 17/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9664 - acc: 0.5536 - val_loss: 0.9668 - val_acc: 0.5554
Epoch 18/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9621 - acc: 0.5593 - val_loss: 0.9629 - val_acc: 0.5580
Epoch 19/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9578 - acc: 0.5617 - val_loss: 0.9586 - val_acc: 0.5626
Epoch 20/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9538 - acc: 0.5667 - val_loss: 0.9548 - val_acc: 0.5623
Epoch 21/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9502 - acc: 0.5687 - val_loss: 0.9511 - val_acc: 0.5666
Epoch 22/150
15663/15663 [==============================] - 0s 20us/step - loss: 0.9464 - acc: 0.5728 - val_loss: 0.9475 - val_acc: 0.5735
Epoch 23/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.9429 - acc: 0.5740 - val_loss: 0.9437 - val_acc: 0.5753
Epoch 24/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9395 - acc: 0.5794 - val_loss: 0.9409 - val_acc: 0.5723
Epoch 25/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9362 - acc: 0.5799 - val_loss: 0.9373 - val_acc: 0.5751
Epoch 26/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9331 - acc: 0.5825 - val_loss: 0.9343 - val_acc: 0.5809
Epoch 27/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9300 - acc: 0.5863 - val_loss: 0.9313 - val_acc: 0.5771
Epoch 28/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9270 - acc: 0.5879 - val_loss: 0.9285 - val_acc: 0.5840
Epoch 29/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9242 - acc: 0.5896 - val_loss: 0.9254 - val_acc: 0.5858
Epoch 30/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9214 - acc: 0.5932 - val_loss: 0.9232 - val_acc: 0.5863
Epoch 31/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9187 - acc: 0.5928 - val_loss: 0.9209 - val_acc: 0.5909
Epoch 32/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9162 - acc: 0.5939 - val_loss: 0.9179 - val_acc: 0.5899
Epoch 33/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9136 - acc: 0.5965 - val_loss: 0.9150 - val_acc: 0.5876
Epoch 34/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9113 - acc: 0.5986 - val_loss: 0.9128 - val_acc: 0.5904
Epoch 35/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9089 - acc: 0.5991 - val_loss: 0.9103 - val_acc: 0.5965
Epoch 36/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9067 - acc: 0.6003 - val_loss: 0.9080 - val_acc: 0.5968
Epoch 37/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9044 - acc: 0.6024 - val_loss: 0.9063 - val_acc: 0.5940
Epoch 38/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9023 - acc: 0.6030 - val_loss: 0.9042 - val_acc: 0.6004
Epoch 39/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9002 - acc: 0.6031 - val_loss: 0.9019 - val_acc: 0.5960
Epoch 40/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8982 - acc: 0.6061 - val_loss: 0.8998 - val_acc: 0.6014
Epoch 41/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8962 - acc: 0.6071 - val_loss: 0.8978 - val_acc: 0.6032
Epoch 42/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8943 - acc: 0.6074 - val_loss: 0.8960 - val_acc: 0.6044
Epoch 43/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8924 - acc: 0.6092 - val_loss: 0.8941 - val_acc: 0.6019
Epoch 44/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8905 - acc: 0.6099 - val_loss: 0.8926 - val_acc: 0.6052
Epoch 45/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8887 - acc: 0.6102 - val_loss: 0.8907 - val_acc: 0.6060
Epoch 46/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8870 - acc: 0.6118 - val_loss: 0.8893 - val_acc: 0.6062
Epoch 47/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8853 - acc: 0.6116 - val_loss: 0.8876 - val_acc: 0.6113
Epoch 48/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8837 - acc: 0.6132 - val_loss: 0.8857 - val_acc: 0.6118
Epoch 49/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8820 - acc: 0.6144 - val_loss: 0.8839 - val_acc: 0.6144
Epoch 50/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8805 - acc: 0.6158 - val_loss: 0.8827 - val_acc: 0.6126
Epoch 51/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8790 - acc: 0.6157 - val_loss: 0.8810 - val_acc: 0.6147
Epoch 52/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8774 - acc: 0.6171 - val_loss: 0.8793 - val_acc: 0.6159
Epoch 53/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8761 - acc: 0.6179 - val_loss: 0.8782 - val_acc: 0.6164
Epoch 54/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8746 - acc: 0.6180 - val_loss: 0.8767 - val_acc: 0.6177
Epoch 55/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8731 - acc: 0.6195 - val_loss: 0.8754 - val_acc: 0.6170
Epoch 56/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8718 - acc: 0.6195 - val_loss: 0.8738 - val_acc: 0.6175
Epoch 57/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8706 - acc: 0.6218 - val_loss: 0.8727 - val_acc: 0.6185
Epoch 58/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8692 - acc: 0.6215 - val_loss: 0.8712 - val_acc: 0.6198
Epoch 59/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8679 - acc: 0.6213 - val_loss: 0.8701 - val_acc: 0.6208
Epoch 60/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8666 - acc: 0.6222 - val_loss: 0.8693 - val_acc: 0.6190
Epoch 61/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8656 - acc: 0.6217 - val_loss: 0.8677 - val_acc: 0.6218
Epoch 62/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8644 - acc: 0.6240 - val_loss: 0.8663 - val_acc: 0.6182
Epoch 63/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8632 - acc: 0.6241 - val_loss: 0.8657 - val_acc: 0.6231
Epoch 64/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8620 - acc: 0.6257 - val_loss: 0.8644 - val_acc: 0.6223
Epoch 65/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8608 - acc: 0.6253 - val_loss: 0.8640 - val_acc: 0.6231
Epoch 66/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8598 - acc: 0.6249 - val_loss: 0.8621 - val_acc: 0.6226
Epoch 67/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8587 - acc: 0.6263 - val_loss: 0.8611 - val_acc: 0.6254
Epoch 68/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8576 - acc: 0.6269 - val_loss: 0.8603 - val_acc: 0.6221
Epoch 69/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8566 - acc: 0.6268 - val_loss: 0.8596 - val_acc: 0.6264
Epoch 70/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8557 - acc: 0.6283 - val_loss: 0.8578 - val_acc: 0.6259
Epoch 71/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8546 - acc: 0.6273 - val_loss: 0.8569 - val_acc: 0.6264
Epoch 72/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8536 - acc: 0.6289 - val_loss: 0.8563 - val_acc: 0.6264
Epoch 73/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8526 - acc: 0.6291 - val_loss: 0.8550 - val_acc: 0.6302
Epoch 74/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8518 - acc: 0.6282 - val_loss: 0.8547 - val_acc: 0.6295
Epoch 75/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8509 - acc: 0.6293 - val_loss: 0.8541 - val_acc: 0.6279
Epoch 76/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8500 - acc: 0.6286 - val_loss: 0.8526 - val_acc: 0.6315
Epoch 77/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8490 - acc: 0.6315 - val_loss: 0.8528 - val_acc: 0.6292
Epoch 78/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8483 - acc: 0.6300 - val_loss: 0.8511 - val_acc: 0.6313
Epoch 79/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8474 - acc: 0.6302 - val_loss: 0.8505 - val_acc: 0.6279
Epoch 80/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8466 - acc: 0.6307 - val_loss: 0.8495 - val_acc: 0.6310
Epoch 81/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8458 - acc: 0.6319 - val_loss: 0.8485 - val_acc: 0.6336
Epoch 82/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8450 - acc: 0.6320 - val_loss: 0.8479 - val_acc: 0.6346
Epoch 83/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8442 - acc: 0.6319 - val_loss: 0.8469 - val_acc: 0.6361
Epoch 84/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8434 - acc: 0.6327 - val_loss: 0.8464 - val_acc: 0.6318
Epoch 85/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8426 - acc: 0.6318 - val_loss: 0.8461 - val_acc: 0.6315
Epoch 86/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8420 - acc: 0.6329 - val_loss: 0.8450 - val_acc: 0.6361
Epoch 87/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8411 - acc: 0.6322 - val_loss: 0.8439 - val_acc: 0.6369
Epoch 88/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8403 - acc: 0.6326 - val_loss: 0.8435 - val_acc: 0.6356
Epoch 89/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8397 - acc: 0.6339 - val_loss: 0.8427 - val_acc: 0.6376
Epoch 90/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8392 - acc: 0.6326 - val_loss: 0.8419 - val_acc: 0.6376
Epoch 91/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8384 - acc: 0.6334 - val_loss: 0.8414 - val_acc: 0.6387
Epoch 92/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8376 - acc: 0.6334 - val_loss: 0.8406 - val_acc: 0.6374
Epoch 93/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8370 - acc: 0.6353 - val_loss: 0.8401 - val_acc: 0.6389
Epoch 94/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8363 - acc: 0.6352 - val_loss: 0.8396 - val_acc: 0.6404
Epoch 95/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8357 - acc: 0.6354 - val_loss: 0.8385 - val_acc: 0.6374
Epoch 96/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8350 - acc: 0.6365 - val_loss: 0.8379 - val_acc: 0.6404
Epoch 97/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8344 - acc: 0.6365 - val_loss: 0.8373 - val_acc: 0.6407
Epoch 98/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8338 - acc: 0.6377 - val_loss: 0.8371 - val_acc: 0.6402
Epoch 99/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8331 - acc: 0.6369 - val_loss: 0.8366 - val_acc: 0.6422
Epoch 100/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8327 - acc: 0.6370 - val_loss: 0.8358 - val_acc: 0.6417
Epoch 101/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8319 - acc: 0.6371 - val_loss: 0.8350 - val_acc: 0.6399
Epoch 102/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8315 - acc: 0.6376 - val_loss: 0.8347 - val_acc: 0.6402
Epoch 103/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8308 - acc: 0.6381 - val_loss: 0.8343 - val_acc: 0.6415
Epoch 104/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8303 - acc: 0.6380 - val_loss: 0.8335 - val_acc: 0.6399
Epoch 105/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8297 - acc: 0.6391 - val_loss: 0.8330 - val_acc: 0.6397
Epoch 106/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8292 - acc: 0.6393 - val_loss: 0.8325 - val_acc: 0.6417
Epoch 107/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8287 - acc: 0.6391 - val_loss: 0.8319 - val_acc: 0.6435
Epoch 108/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8280 - acc: 0.6407 - val_loss: 0.8316 - val_acc: 0.6438
Epoch 109/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8275 - acc: 0.6404 - val_loss: 0.8312 - val_acc: 0.6438
Epoch 110/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8270 - acc: 0.6397 - val_loss: 0.8306 - val_acc: 0.6438
Epoch 111/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8265 - acc: 0.6400 - val_loss: 0.8299 - val_acc: 0.6461
Epoch 112/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8260 - acc: 0.6412 - val_loss: 0.8297 - val_acc: 0.6448
Epoch 113/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8256 - acc: 0.6407 - val_loss: 0.8293 - val_acc: 0.6461
Epoch 114/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8250 - acc: 0.6409 - val_loss: 0.8287 - val_acc: 0.6461
Epoch 115/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8245 - acc: 0.6409 - val_loss: 0.8280 - val_acc: 0.6463
Epoch 116/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8240 - acc: 0.6418 - val_loss: 0.8277 - val_acc: 0.6463
Epoch 117/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8236 - acc: 0.6418 - val_loss: 0.8272 - val_acc: 0.6489
Epoch 118/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8231 - acc: 0.6402 - val_loss: 0.8267 - val_acc: 0.6499
Epoch 119/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8227 - acc: 0.6426 - val_loss: 0.8265 - val_acc: 0.6484
Epoch 120/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8221 - acc: 0.6427 - val_loss: 0.8259 - val_acc: 0.6491
Epoch 121/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8218 - acc: 0.6425 - val_loss: 0.8258 - val_acc: 0.6481
Epoch 122/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8213 - acc: 0.6429 - val_loss: 0.8250 - val_acc: 0.6502
Epoch 123/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8209 - acc: 0.6437 - val_loss: 0.8247 - val_acc: 0.6514
Epoch 124/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8204 - acc: 0.6431 - val_loss: 0.8247 - val_acc: 0.6496
Epoch 125/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8201 - acc: 0.6425 - val_loss: 0.8238 - val_acc: 0.6519
Epoch 126/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8196 - acc: 0.6416 - val_loss: 0.8234 - val_acc: 0.6502
Epoch 127/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8193 - acc: 0.6433 - val_loss: 0.8230 - val_acc: 0.6519
Epoch 128/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8189 - acc: 0.6432 - val_loss: 0.8228 - val_acc: 0.6522
Epoch 129/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8185 - acc: 0.6425 - val_loss: 0.8224 - val_acc: 0.6532
Epoch 130/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8179 - acc: 0.6437 - val_loss: 0.8221 - val_acc: 0.6517
Epoch 131/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8177 - acc: 0.6440 - val_loss: 0.8215 - val_acc: 0.6537
Epoch 132/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8173 - acc: 0.6439 - val_loss: 0.8218 - val_acc: 0.6504
Epoch 133/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8169 - acc: 0.6440 - val_loss: 0.8210 - val_acc: 0.6530
Epoch 134/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8165 - acc: 0.6446 - val_loss: 0.8207 - val_acc: 0.6522
Epoch 135/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8161 - acc: 0.6447 - val_loss: 0.8202 - val_acc: 0.6525
Epoch 136/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8158 - acc: 0.6436 - val_loss: 0.8202 - val_acc: 0.6514
Epoch 137/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8153 - acc: 0.6455 - val_loss: 0.8200 - val_acc: 0.6514
Epoch 138/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8150 - acc: 0.6450 - val_loss: 0.8193 - val_acc: 0.6547
Epoch 139/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8146 - acc: 0.6452 - val_loss: 0.8189 - val_acc: 0.6542
Epoch 140/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8144 - acc: 0.6451 - val_loss: 0.8187 - val_acc: 0.6535
Epoch 141/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8139 - acc: 0.6454 - val_loss: 0.8181 - val_acc: 0.6542
Epoch 142/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8137 - acc: 0.6471 - val_loss: 0.8182 - val_acc: 0.6517
Epoch 143/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8134 - acc: 0.6453 - val_loss: 0.8177 - val_acc: 0.6545
Epoch 144/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8129 - acc: 0.6459 - val_loss: 0.8173 - val_acc: 0.6563
Epoch 145/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8126 - acc: 0.6467 - val_loss: 0.8173 - val_acc: 0.6542
Epoch 146/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8123 - acc: 0.6454 - val_loss: 0.8168 - val_acc: 0.6553
Epoch 147/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8119 - acc: 0.6466 - val_loss: 0.8166 - val_acc: 0.6553
Epoch 148/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8117 - acc: 0.6460 - val_loss: 0.8162 - val_acc: 0.6553
Epoch 149/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8112 - acc: 0.6469 - val_loss: 0.8164 - val_acc: 0.6530
Epoch 150/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8110 - acc: 0.6469 - val_loss: 0.8155 - val_acc: 0.6555
15663/15663 [==============================] - 0s 22us/step
==== Training loss, score are: 0.8106370004461101 0.6479601609153567 =======
3916/3916 [==============================] - 0s 21us/step
==== CV loss, score are: 0.8155102588548359 0.6555158325430078 =======
Running fold 2
Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_15 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
global_average_pooling1d_15  (None, 100)               0         
_________________________________________________________________
dense_15 (Dense)             (None, 3)                 303       
=================================================================
Total params: 24,734,903
Trainable params: 303
Non-trainable params: 24,734,600
_________________________________________________________________
15663/15663 [==============================] - 1s 35us/step
Before training loss, score are: 1.1002171779283199 0.3076039073003067
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 0s 30us/step - loss: 1.0859 - acc: 0.3962 - val_loss: 1.0800 - val_acc: 0.3943
Epoch 2/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0720 - acc: 0.4082 - val_loss: 1.0686 - val_acc: 0.4032
Epoch 3/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0604 - acc: 0.4216 - val_loss: 1.0583 - val_acc: 0.4446
Epoch 4/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0501 - acc: 0.4483 - val_loss: 1.0489 - val_acc: 0.4538
Epoch 5/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0405 - acc: 0.4604 - val_loss: 1.0403 - val_acc: 0.4706
Epoch 6/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0319 - acc: 0.4742 - val_loss: 1.0326 - val_acc: 0.4714
Epoch 7/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0239 - acc: 0.4919 - val_loss: 1.0252 - val_acc: 0.4816
Epoch 8/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0164 - acc: 0.5004 - val_loss: 1.0182 - val_acc: 0.4944
Epoch 9/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0096 - acc: 0.5125 - val_loss: 1.0117 - val_acc: 0.5010
Epoch 10/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0030 - acc: 0.5178 - val_loss: 1.0056 - val_acc: 0.5064
Epoch 11/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9968 - acc: 0.5299 - val_loss: 0.9999 - val_acc: 0.5115
Epoch 12/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9910 - acc: 0.5326 - val_loss: 0.9945 - val_acc: 0.5199
Epoch 13/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9856 - acc: 0.5394 - val_loss: 0.9893 - val_acc: 0.5250
Epoch 14/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9804 - acc: 0.5448 - val_loss: 0.9842 - val_acc: 0.5419
Epoch 15/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9754 - acc: 0.5490 - val_loss: 0.9797 - val_acc: 0.5429
Epoch 16/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9706 - acc: 0.5550 - val_loss: 0.9751 - val_acc: 0.5465
Epoch 17/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9660 - acc: 0.5546 - val_loss: 0.9707 - val_acc: 0.5526
Epoch 18/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9618 - acc: 0.5623 - val_loss: 0.9665 - val_acc: 0.5575
Epoch 19/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9575 - acc: 0.5642 - val_loss: 0.9631 - val_acc: 0.5557
Epoch 20/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9535 - acc: 0.5680 - val_loss: 0.9586 - val_acc: 0.5656
Epoch 21/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9497 - acc: 0.5726 - val_loss: 0.9552 - val_acc: 0.5664
Epoch 22/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9458 - acc: 0.5741 - val_loss: 0.9520 - val_acc: 0.5697
Epoch 23/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9425 - acc: 0.5775 - val_loss: 0.9482 - val_acc: 0.5697
Epoch 24/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9389 - acc: 0.5799 - val_loss: 0.9455 - val_acc: 0.5687
Epoch 25/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9356 - acc: 0.5832 - val_loss: 0.9420 - val_acc: 0.5733
Epoch 26/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9325 - acc: 0.5853 - val_loss: 0.9387 - val_acc: 0.5812
Epoch 27/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9294 - acc: 0.5888 - val_loss: 0.9358 - val_acc: 0.5815
Epoch 28/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9263 - acc: 0.5891 - val_loss: 0.9329 - val_acc: 0.5822
Epoch 29/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9235 - acc: 0.5918 - val_loss: 0.9305 - val_acc: 0.5820
Epoch 30/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9207 - acc: 0.5924 - val_loss: 0.9274 - val_acc: 0.5827
Epoch 31/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9182 - acc: 0.5962 - val_loss: 0.9250 - val_acc: 0.5838
Epoch 32/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9154 - acc: 0.5962 - val_loss: 0.9226 - val_acc: 0.5876
Epoch 33/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9129 - acc: 0.5970 - val_loss: 0.9200 - val_acc: 0.5884
Epoch 34/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9105 - acc: 0.6025 - val_loss: 0.9178 - val_acc: 0.5909
Epoch 35/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9080 - acc: 0.6028 - val_loss: 0.9152 - val_acc: 0.5947
Epoch 36/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9058 - acc: 0.6038 - val_loss: 0.9131 - val_acc: 0.5942
Epoch 37/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9035 - acc: 0.6052 - val_loss: 0.9114 - val_acc: 0.5914
Epoch 38/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9014 - acc: 0.6074 - val_loss: 0.9091 - val_acc: 0.5958
Epoch 39/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8993 - acc: 0.6071 - val_loss: 0.9068 - val_acc: 0.5998
Epoch 40/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8972 - acc: 0.6088 - val_loss: 0.9052 - val_acc: 0.5983
Epoch 41/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8953 - acc: 0.6102 - val_loss: 0.9034 - val_acc: 0.5993
Epoch 42/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8933 - acc: 0.6105 - val_loss: 0.9015 - val_acc: 0.6027
Epoch 43/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8914 - acc: 0.6137 - val_loss: 0.8997 - val_acc: 0.6037
Epoch 44/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8896 - acc: 0.6135 - val_loss: 0.8980 - val_acc: 0.6037
Epoch 45/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8878 - acc: 0.6134 - val_loss: 0.8967 - val_acc: 0.6004
Epoch 46/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8860 - acc: 0.6147 - val_loss: 0.8944 - val_acc: 0.6078
Epoch 47/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8845 - acc: 0.6174 - val_loss: 0.8931 - val_acc: 0.6067
Epoch 48/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8827 - acc: 0.6157 - val_loss: 0.8915 - val_acc: 0.6083
Epoch 49/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8811 - acc: 0.6161 - val_loss: 0.8900 - val_acc: 0.6113
Epoch 50/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8795 - acc: 0.6197 - val_loss: 0.8885 - val_acc: 0.6090
Epoch 51/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8779 - acc: 0.6181 - val_loss: 0.8868 - val_acc: 0.6103
Epoch 52/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8765 - acc: 0.6197 - val_loss: 0.8852 - val_acc: 0.6124
Epoch 53/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8750 - acc: 0.6203 - val_loss: 0.8839 - val_acc: 0.6118
Epoch 54/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8735 - acc: 0.6220 - val_loss: 0.8827 - val_acc: 0.6139
Epoch 55/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8722 - acc: 0.6199 - val_loss: 0.8815 - val_acc: 0.6129
Epoch 56/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8707 - acc: 0.6216 - val_loss: 0.8799 - val_acc: 0.6162
Epoch 57/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8694 - acc: 0.6227 - val_loss: 0.8789 - val_acc: 0.6152
Epoch 58/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8681 - acc: 0.6224 - val_loss: 0.8776 - val_acc: 0.6175
Epoch 59/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8667 - acc: 0.6236 - val_loss: 0.8769 - val_acc: 0.6129
Epoch 60/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8655 - acc: 0.6251 - val_loss: 0.8753 - val_acc: 0.6177
Epoch 61/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8644 - acc: 0.6238 - val_loss: 0.8737 - val_acc: 0.6149
Epoch 62/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8632 - acc: 0.6260 - val_loss: 0.8730 - val_acc: 0.6162
Epoch 63/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8620 - acc: 0.6261 - val_loss: 0.8716 - val_acc: 0.6177
Epoch 64/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8607 - acc: 0.6255 - val_loss: 0.8705 - val_acc: 0.6187
Epoch 65/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8597 - acc: 0.6273 - val_loss: 0.8696 - val_acc: 0.6200
Epoch 66/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8584 - acc: 0.6260 - val_loss: 0.8685 - val_acc: 0.6157
Epoch 67/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8574 - acc: 0.6277 - val_loss: 0.8676 - val_acc: 0.6170
Epoch 68/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8564 - acc: 0.6291 - val_loss: 0.8667 - val_acc: 0.6193
Epoch 69/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8553 - acc: 0.6283 - val_loss: 0.8662 - val_acc: 0.6177
Epoch 70/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8543 - acc: 0.6287 - val_loss: 0.8646 - val_acc: 0.6162
Epoch 71/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8533 - acc: 0.6298 - val_loss: 0.8634 - val_acc: 0.6198
Epoch 72/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8523 - acc: 0.6295 - val_loss: 0.8625 - val_acc: 0.6203
Epoch 73/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8512 - acc: 0.6319 - val_loss: 0.8628 - val_acc: 0.6185
Epoch 74/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8504 - acc: 0.6302 - val_loss: 0.8616 - val_acc: 0.6195
Epoch 75/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8494 - acc: 0.6319 - val_loss: 0.8600 - val_acc: 0.6216
Epoch 76/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8485 - acc: 0.6323 - val_loss: 0.8590 - val_acc: 0.6221
Epoch 77/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8476 - acc: 0.6323 - val_loss: 0.8582 - val_acc: 0.6213
Epoch 78/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8468 - acc: 0.6336 - val_loss: 0.8578 - val_acc: 0.6216
Epoch 79/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8459 - acc: 0.6337 - val_loss: 0.8566 - val_acc: 0.6239
Epoch 80/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8451 - acc: 0.6342 - val_loss: 0.8559 - val_acc: 0.6244
Epoch 81/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8443 - acc: 0.6353 - val_loss: 0.8554 - val_acc: 0.6233
Epoch 82/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8434 - acc: 0.6357 - val_loss: 0.8549 - val_acc: 0.6226
Epoch 83/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8428 - acc: 0.6346 - val_loss: 0.8540 - val_acc: 0.6231
Epoch 84/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8418 - acc: 0.6358 - val_loss: 0.8530 - val_acc: 0.6239
Epoch 85/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8410 - acc: 0.6364 - val_loss: 0.8524 - val_acc: 0.6236
Epoch 86/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8403 - acc: 0.6372 - val_loss: 0.8519 - val_acc: 0.6236
Epoch 87/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8397 - acc: 0.6353 - val_loss: 0.8506 - val_acc: 0.6251
Epoch 88/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8388 - acc: 0.6384 - val_loss: 0.8503 - val_acc: 0.6241
Epoch 89/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8381 - acc: 0.6370 - val_loss: 0.8495 - val_acc: 0.6241
Epoch 90/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8374 - acc: 0.6389 - val_loss: 0.8488 - val_acc: 0.6249
Epoch 91/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8366 - acc: 0.6391 - val_loss: 0.8487 - val_acc: 0.6246
Epoch 92/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8360 - acc: 0.6382 - val_loss: 0.8474 - val_acc: 0.6249
Epoch 93/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8353 - acc: 0.6391 - val_loss: 0.8475 - val_acc: 0.6254
Epoch 94/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8347 - acc: 0.6400 - val_loss: 0.8462 - val_acc: 0.6261
Epoch 95/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8340 - acc: 0.6399 - val_loss: 0.8459 - val_acc: 0.6249
Epoch 96/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8335 - acc: 0.6400 - val_loss: 0.8450 - val_acc: 0.6259
Epoch 97/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8328 - acc: 0.6404 - val_loss: 0.8443 - val_acc: 0.6249
Epoch 98/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8321 - acc: 0.6416 - val_loss: 0.8445 - val_acc: 0.6228
Epoch 99/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8317 - acc: 0.6411 - val_loss: 0.8433 - val_acc: 0.6264
Epoch 100/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8311 - acc: 0.6409 - val_loss: 0.8428 - val_acc: 0.6264
Epoch 101/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8303 - acc: 0.6403 - val_loss: 0.8426 - val_acc: 0.6267
Epoch 102/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8297 - acc: 0.6412 - val_loss: 0.8416 - val_acc: 0.6259
Epoch 103/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8293 - acc: 0.6418 - val_loss: 0.8416 - val_acc: 0.6249
Epoch 104/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8287 - acc: 0.6411 - val_loss: 0.8409 - val_acc: 0.6267
Epoch 105/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8284 - acc: 0.6425 - val_loss: 0.8405 - val_acc: 0.6279
Epoch 106/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8276 - acc: 0.6416 - val_loss: 0.8396 - val_acc: 0.6277
Epoch 107/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8270 - acc: 0.6427 - val_loss: 0.8392 - val_acc: 0.6274
Epoch 108/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8266 - acc: 0.6430 - val_loss: 0.8389 - val_acc: 0.6267
Epoch 109/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8259 - acc: 0.6437 - val_loss: 0.8385 - val_acc: 0.6259
Epoch 110/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8254 - acc: 0.6434 - val_loss: 0.8378 - val_acc: 0.6264
Epoch 111/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8250 - acc: 0.6431 - val_loss: 0.8372 - val_acc: 0.6279
Epoch 112/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8244 - acc: 0.6436 - val_loss: 0.8367 - val_acc: 0.6272
Epoch 113/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8240 - acc: 0.6441 - val_loss: 0.8364 - val_acc: 0.6277
Epoch 114/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8234 - acc: 0.6446 - val_loss: 0.8358 - val_acc: 0.6277
Epoch 115/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8230 - acc: 0.6450 - val_loss: 0.8357 - val_acc: 0.6277
Epoch 116/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8225 - acc: 0.6447 - val_loss: 0.8350 - val_acc: 0.6267
Epoch 117/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8220 - acc: 0.6453 - val_loss: 0.8347 - val_acc: 0.6267
Epoch 118/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8216 - acc: 0.6459 - val_loss: 0.8350 - val_acc: 0.6272
Epoch 119/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8211 - acc: 0.6468 - val_loss: 0.8337 - val_acc: 0.6287
Epoch 120/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8206 - acc: 0.6467 - val_loss: 0.8336 - val_acc: 0.6269
Epoch 121/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8202 - acc: 0.6460 - val_loss: 0.8330 - val_acc: 0.6292
Epoch 122/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8199 - acc: 0.6461 - val_loss: 0.8325 - val_acc: 0.6290
Epoch 123/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8193 - acc: 0.6463 - val_loss: 0.8321 - val_acc: 0.6300
Epoch 124/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8189 - acc: 0.6476 - val_loss: 0.8318 - val_acc: 0.6284
Epoch 125/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8184 - acc: 0.6468 - val_loss: 0.8322 - val_acc: 0.6264
Epoch 126/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8181 - acc: 0.6466 - val_loss: 0.8310 - val_acc: 0.6287
Epoch 127/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8177 - acc: 0.6469 - val_loss: 0.8314 - val_acc: 0.6282
Epoch 128/150
15663/15663 [==============================] - 0s 18us/step - loss: 0.8173 - acc: 0.6480 - val_loss: 0.8304 - val_acc: 0.6295
Epoch 129/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8168 - acc: 0.6476 - val_loss: 0.8300 - val_acc: 0.6310
Epoch 130/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8164 - acc: 0.6477 - val_loss: 0.8295 - val_acc: 0.6323
Epoch 131/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8160 - acc: 0.6485 - val_loss: 0.8302 - val_acc: 0.6284
Epoch 132/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8157 - acc: 0.6484 - val_loss: 0.8289 - val_acc: 0.6302
Epoch 133/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8153 - acc: 0.6462 - val_loss: 0.8283 - val_acc: 0.6315
Epoch 134/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8151 - acc: 0.6482 - val_loss: 0.8281 - val_acc: 0.6310
Epoch 135/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8146 - acc: 0.6482 - val_loss: 0.8278 - val_acc: 0.6313
Epoch 136/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8144 - acc: 0.6492 - val_loss: 0.8273 - val_acc: 0.6325
Epoch 137/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8138 - acc: 0.6494 - val_loss: 0.8272 - val_acc: 0.6300
Epoch 138/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8134 - acc: 0.6497 - val_loss: 0.8269 - val_acc: 0.6313
Epoch 139/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8130 - acc: 0.6497 - val_loss: 0.8267 - val_acc: 0.6336
Epoch 140/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8127 - acc: 0.6495 - val_loss: 0.8266 - val_acc: 0.6320
Epoch 141/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8124 - acc: 0.6497 - val_loss: 0.8262 - val_acc: 0.6328
Epoch 142/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8121 - acc: 0.6492 - val_loss: 0.8254 - val_acc: 0.6320
Epoch 143/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8118 - acc: 0.6490 - val_loss: 0.8253 - val_acc: 0.6333
Epoch 144/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8114 - acc: 0.6502 - val_loss: 0.8252 - val_acc: 0.6325
Epoch 145/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8109 - acc: 0.6501 - val_loss: 0.8244 - val_acc: 0.6325
Epoch 146/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8108 - acc: 0.6491 - val_loss: 0.8243 - val_acc: 0.6336
Epoch 147/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8104 - acc: 0.6506 - val_loss: 0.8250 - val_acc: 0.6325
Epoch 148/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8102 - acc: 0.6500 - val_loss: 0.8240 - val_acc: 0.6336
Epoch 149/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8098 - acc: 0.6503 - val_loss: 0.8240 - val_acc: 0.6333
Epoch 150/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8095 - acc: 0.6511 - val_loss: 0.8236 - val_acc: 0.6330
15663/15663 [==============================] - 0s 21us/step
==== Training loss, score are: 0.8092060357547237 0.6507693290147244 =======
3916/3916 [==============================] - 0s 24us/step
==== CV loss, score are: 0.823633152508273 0.633043922369765 =======
Running fold 3
Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_16 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
global_average_pooling1d_16  (None, 100)               0         
_________________________________________________________________
dense_16 (Dense)             (None, 3)                 303       
=================================================================
Total params: 24,734,903
Trainable params: 303
Non-trainable params: 24,734,600
_________________________________________________________________
15663/15663 [==============================] - 1s 38us/step
Before training loss, score are: 1.1061448133106715 0.30179403690891327
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 0s 32us/step - loss: 1.0897 - acc: 0.3903 - val_loss: 1.0821 - val_acc: 0.3999
Epoch 2/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0754 - acc: 0.4069 - val_loss: 1.0705 - val_acc: 0.4099
Epoch 3/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0641 - acc: 0.4226 - val_loss: 1.0600 - val_acc: 0.4208
Epoch 4/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0537 - acc: 0.4382 - val_loss: 1.0504 - val_acc: 0.4456
Epoch 5/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0444 - acc: 0.4521 - val_loss: 1.0415 - val_acc: 0.4630
Epoch 6/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0358 - acc: 0.4687 - val_loss: 1.0333 - val_acc: 0.4831
Epoch 7/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0278 - acc: 0.4839 - val_loss: 1.0258 - val_acc: 0.4852
Epoch 8/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0203 - acc: 0.4948 - val_loss: 1.0189 - val_acc: 0.4900
Epoch 9/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0133 - acc: 0.5025 - val_loss: 1.0118 - val_acc: 0.5003
Epoch 10/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0067 - acc: 0.5143 - val_loss: 1.0055 - val_acc: 0.5105
Epoch 11/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0004 - acc: 0.5196 - val_loss: 0.9995 - val_acc: 0.5197
Epoch 12/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9944 - acc: 0.5263 - val_loss: 0.9940 - val_acc: 0.5245
Epoch 13/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9889 - acc: 0.5344 - val_loss: 0.9884 - val_acc: 0.5281
Epoch 14/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9836 - acc: 0.5406 - val_loss: 0.9833 - val_acc: 0.5383
Epoch 15/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9784 - acc: 0.5491 - val_loss: 0.9785 - val_acc: 0.5322
Epoch 16/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9736 - acc: 0.5526 - val_loss: 0.9739 - val_acc: 0.5345
Epoch 17/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9690 - acc: 0.5547 - val_loss: 0.9693 - val_acc: 0.5575
Epoch 18/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9646 - acc: 0.5593 - val_loss: 0.9649 - val_acc: 0.5501
Epoch 19/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9602 - acc: 0.5643 - val_loss: 0.9608 - val_acc: 0.5531
Epoch 20/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9563 - acc: 0.5667 - val_loss: 0.9568 - val_acc: 0.5649
Epoch 21/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9524 - acc: 0.5720 - val_loss: 0.9531 - val_acc: 0.5605
Epoch 22/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9485 - acc: 0.5732 - val_loss: 0.9497 - val_acc: 0.5595
Epoch 23/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9448 - acc: 0.5762 - val_loss: 0.9460 - val_acc: 0.5738
Epoch 24/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9415 - acc: 0.5801 - val_loss: 0.9425 - val_acc: 0.5725
Epoch 25/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9382 - acc: 0.5822 - val_loss: 0.9393 - val_acc: 0.5730
Epoch 26/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9348 - acc: 0.5860 - val_loss: 0.9361 - val_acc: 0.5733
Epoch 27/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9316 - acc: 0.5911 - val_loss: 0.9336 - val_acc: 0.5712
Epoch 28/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9287 - acc: 0.5878 - val_loss: 0.9303 - val_acc: 0.5766
Epoch 29/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9259 - acc: 0.5938 - val_loss: 0.9274 - val_acc: 0.5774
Epoch 30/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9229 - acc: 0.5938 - val_loss: 0.9248 - val_acc: 0.5822
Epoch 31/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9204 - acc: 0.5959 - val_loss: 0.9221 - val_acc: 0.5817
Epoch 32/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9175 - acc: 0.5992 - val_loss: 0.9196 - val_acc: 0.5820
Epoch 33/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9150 - acc: 0.5985 - val_loss: 0.9171 - val_acc: 0.5845
Epoch 34/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9125 - acc: 0.6009 - val_loss: 0.9149 - val_acc: 0.5827
Epoch 35/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9101 - acc: 0.6020 - val_loss: 0.9126 - val_acc: 0.5878
Epoch 36/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9077 - acc: 0.6060 - val_loss: 0.9103 - val_acc: 0.5881
Epoch 37/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9055 - acc: 0.6053 - val_loss: 0.9081 - val_acc: 0.5899
Epoch 38/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9033 - acc: 0.6056 - val_loss: 0.9059 - val_acc: 0.5930
Epoch 39/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9012 - acc: 0.6094 - val_loss: 0.9038 - val_acc: 0.5932
Epoch 40/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8990 - acc: 0.6109 - val_loss: 0.9019 - val_acc: 0.5914
Epoch 41/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8970 - acc: 0.6109 - val_loss: 0.8999 - val_acc: 0.5958
Epoch 42/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8950 - acc: 0.6130 - val_loss: 0.8982 - val_acc: 0.5960
Epoch 43/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8932 - acc: 0.6130 - val_loss: 0.8963 - val_acc: 0.5963
Epoch 44/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8913 - acc: 0.6138 - val_loss: 0.8945 - val_acc: 0.5960
Epoch 45/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8893 - acc: 0.6153 - val_loss: 0.8928 - val_acc: 0.6004
Epoch 46/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8876 - acc: 0.6160 - val_loss: 0.8911 - val_acc: 0.5988
Epoch 47/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8858 - acc: 0.6161 - val_loss: 0.8894 - val_acc: 0.6009
Epoch 48/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8842 - acc: 0.6187 - val_loss: 0.8879 - val_acc: 0.6006
Epoch 49/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8825 - acc: 0.6179 - val_loss: 0.8863 - val_acc: 0.6039
Epoch 50/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8810 - acc: 0.6202 - val_loss: 0.8849 - val_acc: 0.6021
Epoch 51/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8793 - acc: 0.6214 - val_loss: 0.8833 - val_acc: 0.6067
Epoch 52/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8778 - acc: 0.6206 - val_loss: 0.8818 - val_acc: 0.6039
Epoch 53/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8763 - acc: 0.6218 - val_loss: 0.8805 - val_acc: 0.6047
Epoch 54/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8749 - acc: 0.6217 - val_loss: 0.8792 - val_acc: 0.6042
Epoch 55/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8734 - acc: 0.6218 - val_loss: 0.8778 - val_acc: 0.6055
Epoch 56/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8722 - acc: 0.6230 - val_loss: 0.8765 - val_acc: 0.6065
Epoch 57/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8707 - acc: 0.6247 - val_loss: 0.8752 - val_acc: 0.6093
Epoch 58/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8693 - acc: 0.6257 - val_loss: 0.8740 - val_acc: 0.6078
Epoch 59/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8681 - acc: 0.6241 - val_loss: 0.8727 - val_acc: 0.6083
Epoch 60/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8667 - acc: 0.6254 - val_loss: 0.8717 - val_acc: 0.6093
Epoch 61/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8655 - acc: 0.6258 - val_loss: 0.8704 - val_acc: 0.6106
Epoch 62/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8643 - acc: 0.6263 - val_loss: 0.8693 - val_acc: 0.6111
Epoch 63/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8631 - acc: 0.6272 - val_loss: 0.8686 - val_acc: 0.6096
Epoch 64/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8618 - acc: 0.6271 - val_loss: 0.8671 - val_acc: 0.6134
Epoch 65/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8608 - acc: 0.6296 - val_loss: 0.8662 - val_acc: 0.6103
Epoch 66/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8597 - acc: 0.6290 - val_loss: 0.8652 - val_acc: 0.6111
Epoch 67/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8585 - acc: 0.6286 - val_loss: 0.8640 - val_acc: 0.6136
Epoch 68/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8575 - acc: 0.6295 - val_loss: 0.8630 - val_acc: 0.6154
Epoch 69/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8564 - acc: 0.6309 - val_loss: 0.8620 - val_acc: 0.6154
Epoch 70/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8555 - acc: 0.6310 - val_loss: 0.8610 - val_acc: 0.6141
Epoch 71/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8544 - acc: 0.6328 - val_loss: 0.8601 - val_acc: 0.6167
Epoch 72/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8534 - acc: 0.6308 - val_loss: 0.8593 - val_acc: 0.6177
Epoch 73/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8524 - acc: 0.6327 - val_loss: 0.8585 - val_acc: 0.6164
Epoch 74/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8515 - acc: 0.6329 - val_loss: 0.8574 - val_acc: 0.6175
Epoch 75/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8506 - acc: 0.6326 - val_loss: 0.8566 - val_acc: 0.6187
Epoch 76/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8496 - acc: 0.6340 - val_loss: 0.8557 - val_acc: 0.6177
Epoch 77/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8487 - acc: 0.6335 - val_loss: 0.8553 - val_acc: 0.6159
Epoch 78/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8478 - acc: 0.6338 - val_loss: 0.8541 - val_acc: 0.6180
Epoch 79/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8469 - acc: 0.6346 - val_loss: 0.8533 - val_acc: 0.6200
Epoch 80/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8460 - acc: 0.6354 - val_loss: 0.8526 - val_acc: 0.6208
Epoch 81/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8453 - acc: 0.6367 - val_loss: 0.8518 - val_acc: 0.6187
Epoch 82/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8444 - acc: 0.6368 - val_loss: 0.8510 - val_acc: 0.6210
Epoch 83/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8437 - acc: 0.6372 - val_loss: 0.8505 - val_acc: 0.6190
Epoch 84/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8428 - acc: 0.6370 - val_loss: 0.8500 - val_acc: 0.6177
Epoch 85/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8421 - acc: 0.6366 - val_loss: 0.8490 - val_acc: 0.6195
Epoch 86/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8412 - acc: 0.6369 - val_loss: 0.8483 - val_acc: 0.6259
Epoch 87/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8405 - acc: 0.6377 - val_loss: 0.8479 - val_acc: 0.6200
Epoch 88/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8398 - acc: 0.6384 - val_loss: 0.8470 - val_acc: 0.6228
Epoch 89/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8390 - acc: 0.6392 - val_loss: 0.8464 - val_acc: 0.6261
Epoch 90/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8383 - acc: 0.6386 - val_loss: 0.8456 - val_acc: 0.6241
Epoch 91/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8376 - acc: 0.6383 - val_loss: 0.8451 - val_acc: 0.6246
Epoch 92/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8369 - acc: 0.6391 - val_loss: 0.8444 - val_acc: 0.6256
Epoch 93/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8362 - acc: 0.6409 - val_loss: 0.8439 - val_acc: 0.6205
Epoch 94/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8356 - acc: 0.6400 - val_loss: 0.8432 - val_acc: 0.6236
Epoch 95/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8349 - acc: 0.6394 - val_loss: 0.8426 - val_acc: 0.6236
Epoch 96/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8344 - acc: 0.6404 - val_loss: 0.8420 - val_acc: 0.6236
Epoch 97/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8336 - acc: 0.6417 - val_loss: 0.8419 - val_acc: 0.6233
Epoch 98/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8330 - acc: 0.6420 - val_loss: 0.8410 - val_acc: 0.6236
Epoch 99/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8323 - acc: 0.6406 - val_loss: 0.8403 - val_acc: 0.6251
Epoch 100/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8318 - acc: 0.6418 - val_loss: 0.8398 - val_acc: 0.6259
Epoch 101/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8312 - acc: 0.6418 - val_loss: 0.8392 - val_acc: 0.6261
Epoch 102/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8306 - acc: 0.6419 - val_loss: 0.8389 - val_acc: 0.6233
Epoch 103/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8299 - acc: 0.6427 - val_loss: 0.8385 - val_acc: 0.6249
Epoch 104/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8295 - acc: 0.6419 - val_loss: 0.8377 - val_acc: 0.6236
Epoch 105/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8288 - acc: 0.6427 - val_loss: 0.8372 - val_acc: 0.6246
Epoch 106/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8282 - acc: 0.6416 - val_loss: 0.8369 - val_acc: 0.6267
Epoch 107/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8277 - acc: 0.6422 - val_loss: 0.8362 - val_acc: 0.6241
Epoch 108/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8272 - acc: 0.6439 - val_loss: 0.8358 - val_acc: 0.6239
Epoch 109/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8267 - acc: 0.6442 - val_loss: 0.8353 - val_acc: 0.6264
Epoch 110/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8261 - acc: 0.6440 - val_loss: 0.8349 - val_acc: 0.6259
Epoch 111/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8256 - acc: 0.6441 - val_loss: 0.8346 - val_acc: 0.6277
Epoch 112/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8252 - acc: 0.6429 - val_loss: 0.8340 - val_acc: 0.6287
Epoch 113/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8245 - acc: 0.6447 - val_loss: 0.8337 - val_acc: 0.6274
Epoch 114/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8241 - acc: 0.6436 - val_loss: 0.8330 - val_acc: 0.6246
Epoch 115/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8236 - acc: 0.6448 - val_loss: 0.8325 - val_acc: 0.6261
Epoch 116/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8232 - acc: 0.6452 - val_loss: 0.8323 - val_acc: 0.6277
Epoch 117/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8227 - acc: 0.6451 - val_loss: 0.8317 - val_acc: 0.6272
Epoch 118/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8222 - acc: 0.6455 - val_loss: 0.8314 - val_acc: 0.6290
Epoch 119/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8217 - acc: 0.6452 - val_loss: 0.8312 - val_acc: 0.6284
Epoch 120/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8213 - acc: 0.6449 - val_loss: 0.8306 - val_acc: 0.6302
Epoch 121/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8208 - acc: 0.6461 - val_loss: 0.8302 - val_acc: 0.6300
Epoch 122/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8203 - acc: 0.6457 - val_loss: 0.8297 - val_acc: 0.6300
Epoch 123/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8200 - acc: 0.6469 - val_loss: 0.8296 - val_acc: 0.6297
Epoch 124/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8195 - acc: 0.6466 - val_loss: 0.8291 - val_acc: 0.6307
Epoch 125/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8190 - acc: 0.6474 - val_loss: 0.8286 - val_acc: 0.6318
Epoch 126/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8186 - acc: 0.6467 - val_loss: 0.8282 - val_acc: 0.6315
Epoch 127/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8182 - acc: 0.6464 - val_loss: 0.8281 - val_acc: 0.6310
Epoch 128/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8180 - acc: 0.6474 - val_loss: 0.8280 - val_acc: 0.6297
Epoch 129/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8175 - acc: 0.6470 - val_loss: 0.8272 - val_acc: 0.6297
Epoch 130/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8170 - acc: 0.6478 - val_loss: 0.8269 - val_acc: 0.6300
Epoch 131/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8167 - acc: 0.6468 - val_loss: 0.8266 - val_acc: 0.6323
Epoch 132/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8162 - acc: 0.6478 - val_loss: 0.8263 - val_acc: 0.6282
Epoch 133/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8157 - acc: 0.6478 - val_loss: 0.8265 - val_acc: 0.6297
Epoch 134/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8156 - acc: 0.6481 - val_loss: 0.8255 - val_acc: 0.6302
Epoch 135/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8152 - acc: 0.6478 - val_loss: 0.8252 - val_acc: 0.6297
Epoch 136/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8147 - acc: 0.6490 - val_loss: 0.8250 - val_acc: 0.6310
Epoch 137/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8144 - acc: 0.6478 - val_loss: 0.8245 - val_acc: 0.6305
Epoch 138/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8140 - acc: 0.6496 - val_loss: 0.8242 - val_acc: 0.6302
Epoch 139/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8136 - acc: 0.6492 - val_loss: 0.8239 - val_acc: 0.6297
Epoch 140/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8132 - acc: 0.6485 - val_loss: 0.8236 - val_acc: 0.6310
Epoch 141/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8130 - acc: 0.6488 - val_loss: 0.8233 - val_acc: 0.6300
Epoch 142/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8127 - acc: 0.6488 - val_loss: 0.8230 - val_acc: 0.6292
Epoch 143/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8124 - acc: 0.6483 - val_loss: 0.8228 - val_acc: 0.6292
Epoch 144/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8119 - acc: 0.6499 - val_loss: 0.8225 - val_acc: 0.6300
Epoch 145/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8117 - acc: 0.6501 - val_loss: 0.8225 - val_acc: 0.6300
Epoch 146/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8113 - acc: 0.6491 - val_loss: 0.8218 - val_acc: 0.6292
Epoch 147/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8109 - acc: 0.6497 - val_loss: 0.8220 - val_acc: 0.6302
Epoch 148/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8108 - acc: 0.6501 - val_loss: 0.8213 - val_acc: 0.6292
Epoch 149/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8103 - acc: 0.6505 - val_loss: 0.8211 - val_acc: 0.6297
Epoch 150/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8100 - acc: 0.6508 - val_loss: 0.8207 - val_acc: 0.6297
15663/15663 [==============================] - 0s 22us/step
==== Training loss, score are: 0.8096670357460858 0.6505777948296771 =======
3916/3916 [==============================] - 0s 24us/step
==== CV loss, score are: 0.8207469563075057 0.6297242083758938 =======
Running fold 4
Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_17 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
global_average_pooling1d_17  (None, 100)               0         
_________________________________________________________________
dense_17 (Dense)             (None, 3)                 303       
=================================================================
Total params: 24,734,903
Trainable params: 303
Non-trainable params: 24,734,600
_________________________________________________________________
15663/15663 [==============================] - 1s 38us/step
Before training loss, score are: 1.0959287749726234 0.401200280918713
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 33us/step - loss: 1.0847 - acc: 0.4019 - val_loss: 1.0758 - val_acc: 0.4132
Epoch 2/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0712 - acc: 0.4098 - val_loss: 1.0636 - val_acc: 0.4349
Epoch 3/150
15663/15663 [==============================] - 0s 14us/step - loss: 1.0599 - acc: 0.4301 - val_loss: 1.0528 - val_acc: 0.4372
Epoch 4/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0497 - acc: 0.4451 - val_loss: 1.0432 - val_acc: 0.4729
Epoch 5/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0405 - acc: 0.4633 - val_loss: 1.0337 - val_acc: 0.4740
Epoch 6/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0318 - acc: 0.4790 - val_loss: 1.0256 - val_acc: 0.4931
Epoch 7/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0238 - acc: 0.4938 - val_loss: 1.0178 - val_acc: 0.4895
Epoch 8/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0165 - acc: 0.5021 - val_loss: 1.0105 - val_acc: 0.5097
Epoch 9/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0096 - acc: 0.5120 - val_loss: 1.0038 - val_acc: 0.5235
Epoch 10/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0029 - acc: 0.5229 - val_loss: 0.9975 - val_acc: 0.5189
Epoch 11/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9967 - acc: 0.5267 - val_loss: 0.9915 - val_acc: 0.5401
Epoch 12/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9908 - acc: 0.5330 - val_loss: 0.9860 - val_acc: 0.5521
Epoch 13/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9854 - acc: 0.5422 - val_loss: 0.9808 - val_acc: 0.5582
Epoch 14/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9800 - acc: 0.5511 - val_loss: 0.9754 - val_acc: 0.5513
Epoch 15/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9750 - acc: 0.5515 - val_loss: 0.9710 - val_acc: 0.5674
Epoch 16/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9703 - acc: 0.5600 - val_loss: 0.9659 - val_acc: 0.5651
Epoch 17/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9656 - acc: 0.5629 - val_loss: 0.9616 - val_acc: 0.5666
Epoch 18/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9613 - acc: 0.5672 - val_loss: 0.9574 - val_acc: 0.5733
Epoch 19/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9571 - acc: 0.5701 - val_loss: 0.9537 - val_acc: 0.5815
Epoch 20/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9531 - acc: 0.5717 - val_loss: 0.9500 - val_acc: 0.5848
Epoch 21/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9494 - acc: 0.5780 - val_loss: 0.9459 - val_acc: 0.5881
Epoch 22/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9457 - acc: 0.5783 - val_loss: 0.9423 - val_acc: 0.5901
Epoch 23/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9420 - acc: 0.5819 - val_loss: 0.9389 - val_acc: 0.5940
Epoch 24/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9387 - acc: 0.5855 - val_loss: 0.9354 - val_acc: 0.5850
Epoch 25/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9355 - acc: 0.5858 - val_loss: 0.9325 - val_acc: 0.6011
Epoch 26/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9323 - acc: 0.5877 - val_loss: 0.9295 - val_acc: 0.6027
Epoch 27/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9292 - acc: 0.5903 - val_loss: 0.9266 - val_acc: 0.6019
Epoch 28/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9262 - acc: 0.5916 - val_loss: 0.9235 - val_acc: 0.6019
Epoch 29/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9235 - acc: 0.5921 - val_loss: 0.9206 - val_acc: 0.6034
Epoch 30/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9207 - acc: 0.5938 - val_loss: 0.9181 - val_acc: 0.6052
Epoch 31/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9179 - acc: 0.5958 - val_loss: 0.9155 - val_acc: 0.6024
Epoch 32/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9155 - acc: 0.5984 - val_loss: 0.9129 - val_acc: 0.6073
Epoch 33/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9130 - acc: 0.5998 - val_loss: 0.9104 - val_acc: 0.6085
Epoch 34/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9107 - acc: 0.5988 - val_loss: 0.9083 - val_acc: 0.6113
Epoch 35/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.9082 - acc: 0.6021 - val_loss: 0.9058 - val_acc: 0.6090
Epoch 36/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9060 - acc: 0.6035 - val_loss: 0.9036 - val_acc: 0.6098
Epoch 37/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9039 - acc: 0.6039 - val_loss: 0.9015 - val_acc: 0.6118
Epoch 38/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9016 - acc: 0.6054 - val_loss: 0.8993 - val_acc: 0.6149
Epoch 39/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8997 - acc: 0.6067 - val_loss: 0.8975 - val_acc: 0.6147
Epoch 40/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8975 - acc: 0.6085 - val_loss: 0.8954 - val_acc: 0.6141
Epoch 41/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8956 - acc: 0.6079 - val_loss: 0.8942 - val_acc: 0.6175
Epoch 42/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8938 - acc: 0.6114 - val_loss: 0.8915 - val_acc: 0.6182
Epoch 43/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8920 - acc: 0.6105 - val_loss: 0.8900 - val_acc: 0.6164
Epoch 44/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8902 - acc: 0.6126 - val_loss: 0.8887 - val_acc: 0.6208
Epoch 45/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8886 - acc: 0.6131 - val_loss: 0.8863 - val_acc: 0.6193
Epoch 46/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8868 - acc: 0.6137 - val_loss: 0.8847 - val_acc: 0.6210
Epoch 47/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8851 - acc: 0.6150 - val_loss: 0.8830 - val_acc: 0.6213
Epoch 48/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8836 - acc: 0.6158 - val_loss: 0.8813 - val_acc: 0.6203
Epoch 49/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8820 - acc: 0.6171 - val_loss: 0.8801 - val_acc: 0.6223
Epoch 50/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8805 - acc: 0.6178 - val_loss: 0.8783 - val_acc: 0.6221
Epoch 51/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8790 - acc: 0.6180 - val_loss: 0.8768 - val_acc: 0.6241
Epoch 52/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8776 - acc: 0.6176 - val_loss: 0.8754 - val_acc: 0.6254
Epoch 53/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8761 - acc: 0.6183 - val_loss: 0.8738 - val_acc: 0.6231
Epoch 54/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8746 - acc: 0.6192 - val_loss: 0.8729 - val_acc: 0.6256
Epoch 55/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8734 - acc: 0.6203 - val_loss: 0.8715 - val_acc: 0.6236
Epoch 56/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8721 - acc: 0.6202 - val_loss: 0.8699 - val_acc: 0.6256
Epoch 57/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8708 - acc: 0.6213 - val_loss: 0.8686 - val_acc: 0.6256
Epoch 58/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8695 - acc: 0.6208 - val_loss: 0.8673 - val_acc: 0.6305
Epoch 59/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8682 - acc: 0.6218 - val_loss: 0.8665 - val_acc: 0.6323
Epoch 60/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8670 - acc: 0.6220 - val_loss: 0.8650 - val_acc: 0.6274
Epoch 61/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8660 - acc: 0.6229 - val_loss: 0.8636 - val_acc: 0.6274
Epoch 62/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8647 - acc: 0.6240 - val_loss: 0.8624 - val_acc: 0.6282
Epoch 63/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8636 - acc: 0.6231 - val_loss: 0.8615 - val_acc: 0.6318
Epoch 64/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8625 - acc: 0.6243 - val_loss: 0.8601 - val_acc: 0.6305
Epoch 65/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8614 - acc: 0.6253 - val_loss: 0.8591 - val_acc: 0.6290
Epoch 66/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8602 - acc: 0.6245 - val_loss: 0.8584 - val_acc: 0.6323
Epoch 67/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8594 - acc: 0.6264 - val_loss: 0.8572 - val_acc: 0.6315
Epoch 68/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8582 - acc: 0.6270 - val_loss: 0.8561 - val_acc: 0.6369
Epoch 69/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8573 - acc: 0.6266 - val_loss: 0.8549 - val_acc: 0.6333
Epoch 70/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8563 - acc: 0.6286 - val_loss: 0.8538 - val_acc: 0.6330
Epoch 71/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8553 - acc: 0.6271 - val_loss: 0.8528 - val_acc: 0.6336
Epoch 72/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8544 - acc: 0.6294 - val_loss: 0.8519 - val_acc: 0.6346
Epoch 73/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8535 - acc: 0.6286 - val_loss: 0.8510 - val_acc: 0.6330
Epoch 74/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8526 - acc: 0.6287 - val_loss: 0.8502 - val_acc: 0.6361
Epoch 75/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8517 - acc: 0.6308 - val_loss: 0.8492 - val_acc: 0.6364
Epoch 76/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8507 - acc: 0.6298 - val_loss: 0.8492 - val_acc: 0.6384
Epoch 77/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8500 - acc: 0.6307 - val_loss: 0.8472 - val_acc: 0.6353
Epoch 78/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8492 - acc: 0.6307 - val_loss: 0.8464 - val_acc: 0.6369
Epoch 79/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8482 - acc: 0.6301 - val_loss: 0.8459 - val_acc: 0.6389
Epoch 80/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8475 - acc: 0.6316 - val_loss: 0.8449 - val_acc: 0.6382
Epoch 81/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8467 - acc: 0.6318 - val_loss: 0.8440 - val_acc: 0.6389
Epoch 82/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8459 - acc: 0.6312 - val_loss: 0.8434 - val_acc: 0.6394
Epoch 83/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8452 - acc: 0.6324 - val_loss: 0.8424 - val_acc: 0.6394
Epoch 84/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8444 - acc: 0.6321 - val_loss: 0.8421 - val_acc: 0.6394
Epoch 85/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8438 - acc: 0.6331 - val_loss: 0.8408 - val_acc: 0.6389
Epoch 86/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8429 - acc: 0.6326 - val_loss: 0.8405 - val_acc: 0.6397
Epoch 87/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8422 - acc: 0.6337 - val_loss: 0.8392 - val_acc: 0.6382
Epoch 88/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8415 - acc: 0.6326 - val_loss: 0.8386 - val_acc: 0.6389
Epoch 89/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8409 - acc: 0.6351 - val_loss: 0.8380 - val_acc: 0.6384
Epoch 90/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8401 - acc: 0.6338 - val_loss: 0.8371 - val_acc: 0.6389
Epoch 91/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8396 - acc: 0.6340 - val_loss: 0.8366 - val_acc: 0.6420
Epoch 92/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8389 - acc: 0.6336 - val_loss: 0.8358 - val_acc: 0.6407
Epoch 93/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8382 - acc: 0.6351 - val_loss: 0.8351 - val_acc: 0.6438
Epoch 94/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8376 - acc: 0.6352 - val_loss: 0.8351 - val_acc: 0.6404
Epoch 95/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8370 - acc: 0.6356 - val_loss: 0.8337 - val_acc: 0.6402
Epoch 96/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8364 - acc: 0.6351 - val_loss: 0.8331 - val_acc: 0.6402
Epoch 97/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8357 - acc: 0.6347 - val_loss: 0.8324 - val_acc: 0.6404
Epoch 98/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8352 - acc: 0.6358 - val_loss: 0.8319 - val_acc: 0.6425
Epoch 99/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8346 - acc: 0.6362 - val_loss: 0.8315 - val_acc: 0.6415
Epoch 100/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8340 - acc: 0.6364 - val_loss: 0.8308 - val_acc: 0.6435
Epoch 101/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8334 - acc: 0.6368 - val_loss: 0.8301 - val_acc: 0.6430
Epoch 102/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8330 - acc: 0.6380 - val_loss: 0.8294 - val_acc: 0.6427
Epoch 103/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8324 - acc: 0.6372 - val_loss: 0.8292 - val_acc: 0.6417
Epoch 104/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8318 - acc: 0.6379 - val_loss: 0.8283 - val_acc: 0.6422
Epoch 105/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8313 - acc: 0.6376 - val_loss: 0.8276 - val_acc: 0.6440
Epoch 106/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8308 - acc: 0.6377 - val_loss: 0.8274 - val_acc: 0.6435
Epoch 107/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8302 - acc: 0.6381 - val_loss: 0.8268 - val_acc: 0.6433
Epoch 108/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8297 - acc: 0.6372 - val_loss: 0.8264 - val_acc: 0.6443
Epoch 109/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8292 - acc: 0.6377 - val_loss: 0.8256 - val_acc: 0.6440
Epoch 110/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8287 - acc: 0.6378 - val_loss: 0.8253 - val_acc: 0.6450
Epoch 111/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8283 - acc: 0.6378 - val_loss: 0.8248 - val_acc: 0.6463
Epoch 112/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8277 - acc: 0.6389 - val_loss: 0.8241 - val_acc: 0.6456
Epoch 113/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8274 - acc: 0.6376 - val_loss: 0.8236 - val_acc: 0.6450
Epoch 114/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8269 - acc: 0.6386 - val_loss: 0.8230 - val_acc: 0.6440
Epoch 115/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8264 - acc: 0.6398 - val_loss: 0.8228 - val_acc: 0.6456
Epoch 116/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8259 - acc: 0.6394 - val_loss: 0.8220 - val_acc: 0.6458
Epoch 117/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8255 - acc: 0.6404 - val_loss: 0.8220 - val_acc: 0.6433
Epoch 118/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8251 - acc: 0.6394 - val_loss: 0.8219 - val_acc: 0.6489
Epoch 119/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8245 - acc: 0.6408 - val_loss: 0.8206 - val_acc: 0.6461
Epoch 120/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8241 - acc: 0.6403 - val_loss: 0.8209 - val_acc: 0.6491
Epoch 121/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8238 - acc: 0.6408 - val_loss: 0.8199 - val_acc: 0.6468
Epoch 122/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8234 - acc: 0.6406 - val_loss: 0.8195 - val_acc: 0.6471
Epoch 123/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8230 - acc: 0.6412 - val_loss: 0.8189 - val_acc: 0.6479
Epoch 124/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8226 - acc: 0.6404 - val_loss: 0.8188 - val_acc: 0.6491
Epoch 125/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8222 - acc: 0.6414 - val_loss: 0.8183 - val_acc: 0.6481
Epoch 126/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8218 - acc: 0.6420 - val_loss: 0.8177 - val_acc: 0.6473
Epoch 127/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8214 - acc: 0.6416 - val_loss: 0.8175 - val_acc: 0.6458
Epoch 128/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8210 - acc: 0.6423 - val_loss: 0.8168 - val_acc: 0.6471
Epoch 129/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8207 - acc: 0.6416 - val_loss: 0.8166 - val_acc: 0.6499
Epoch 130/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8202 - acc: 0.6425 - val_loss: 0.8161 - val_acc: 0.6448
Epoch 131/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8199 - acc: 0.6422 - val_loss: 0.8157 - val_acc: 0.6453
Epoch 132/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8196 - acc: 0.6429 - val_loss: 0.8153 - val_acc: 0.6481
Epoch 133/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8192 - acc: 0.6443 - val_loss: 0.8151 - val_acc: 0.6502
Epoch 134/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8188 - acc: 0.6427 - val_loss: 0.8146 - val_acc: 0.6484
Epoch 135/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8184 - acc: 0.6434 - val_loss: 0.8143 - val_acc: 0.6507
Epoch 136/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8181 - acc: 0.6441 - val_loss: 0.8137 - val_acc: 0.6499
Epoch 137/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8177 - acc: 0.6429 - val_loss: 0.8133 - val_acc: 0.6496
Epoch 138/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8174 - acc: 0.6437 - val_loss: 0.8134 - val_acc: 0.6517
Epoch 139/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8171 - acc: 0.6443 - val_loss: 0.8126 - val_acc: 0.6489
Epoch 140/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8166 - acc: 0.6444 - val_loss: 0.8124 - val_acc: 0.6514
Epoch 141/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8165 - acc: 0.6444 - val_loss: 0.8121 - val_acc: 0.6481
Epoch 142/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8161 - acc: 0.6446 - val_loss: 0.8115 - val_acc: 0.6499
Epoch 143/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8157 - acc: 0.6442 - val_loss: 0.8110 - val_acc: 0.6507
Epoch 144/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8154 - acc: 0.6449 - val_loss: 0.8111 - val_acc: 0.6491
Epoch 145/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8152 - acc: 0.6438 - val_loss: 0.8107 - val_acc: 0.6479
Epoch 146/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8148 - acc: 0.6456 - val_loss: 0.8101 - val_acc: 0.6496
Epoch 147/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8145 - acc: 0.6450 - val_loss: 0.8097 - val_acc: 0.6496
Epoch 148/150
15663/15663 [==============================] - 0s 14us/step - loss: 0.8142 - acc: 0.6457 - val_loss: 0.8097 - val_acc: 0.6512
Epoch 149/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8140 - acc: 0.6448 - val_loss: 0.8094 - val_acc: 0.6519
Epoch 150/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8136 - acc: 0.6447 - val_loss: 0.8091 - val_acc: 0.6530
15663/15663 [==============================] - 0s 22us/step
==== Training loss, score are: 0.8133315369596216 0.6457894401083594 =======
3916/3916 [==============================] - 0s 21us/step
==== CV loss, score are: 0.809124727475631 0.6529622063329928 =======
Running fold 5
Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_18 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
global_average_pooling1d_18  (None, 100)               0         
_________________________________________________________________
dense_18 (Dense)             (None, 3)                 303       
=================================================================
Total params: 24,734,903
Trainable params: 303
Non-trainable params: 24,734,600
_________________________________________________________________
15664/15664 [==============================] - 1s 38us/step
Before training loss, score are: 1.0946128056655744 0.3972165474974464
Train on 15664 samples, validate on 3915 samples
Epoch 1/150
15664/15664 [==============================] - 1s 34us/step - loss: 1.0872 - acc: 0.3994 - val_loss: 1.0766 - val_acc: 0.4212
Epoch 2/150
15664/15664 [==============================] - 0s 15us/step - loss: 1.0747 - acc: 0.4014 - val_loss: 1.0646 - val_acc: 0.4330
Epoch 3/150
15664/15664 [==============================] - 0s 16us/step - loss: 1.0636 - acc: 0.4180 - val_loss: 1.0539 - val_acc: 0.4580
Epoch 4/150
15664/15664 [==============================] - 0s 15us/step - loss: 1.0532 - acc: 0.4427 - val_loss: 1.0435 - val_acc: 0.4600
Epoch 5/150
15664/15664 [==============================] - 0s 15us/step - loss: 1.0442 - acc: 0.4545 - val_loss: 1.0344 - val_acc: 0.4805
Epoch 6/150
15664/15664 [==============================] - 0s 16us/step - loss: 1.0357 - acc: 0.4724 - val_loss: 1.0264 - val_acc: 0.5006
Epoch 7/150
15664/15664 [==============================] - 0s 14us/step - loss: 1.0277 - acc: 0.4865 - val_loss: 1.0180 - val_acc: 0.5052
Epoch 8/150
15664/15664 [==============================] - 0s 15us/step - loss: 1.0201 - acc: 0.5011 - val_loss: 1.0105 - val_acc: 0.5165
Epoch 9/150
15664/15664 [==============================] - 0s 14us/step - loss: 1.0130 - acc: 0.5109 - val_loss: 1.0043 - val_acc: 0.5361
Epoch 10/150
15664/15664 [==============================] - 0s 15us/step - loss: 1.0065 - acc: 0.5202 - val_loss: 0.9977 - val_acc: 0.5336
Epoch 11/150
15664/15664 [==============================] - 0s 14us/step - loss: 1.0003 - acc: 0.5289 - val_loss: 0.9912 - val_acc: 0.5425
Epoch 12/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9945 - acc: 0.5340 - val_loss: 0.9856 - val_acc: 0.5466
Epoch 13/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9888 - acc: 0.5410 - val_loss: 0.9797 - val_acc: 0.5517
Epoch 14/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9835 - acc: 0.5479 - val_loss: 0.9748 - val_acc: 0.5589
Epoch 15/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9785 - acc: 0.5501 - val_loss: 0.9696 - val_acc: 0.5604
Epoch 16/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9736 - acc: 0.5561 - val_loss: 0.9653 - val_acc: 0.5678
Epoch 17/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.9690 - acc: 0.5577 - val_loss: 0.9615 - val_acc: 0.5788
Epoch 18/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9646 - acc: 0.5621 - val_loss: 0.9565 - val_acc: 0.5788
Epoch 19/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9604 - acc: 0.5675 - val_loss: 0.9527 - val_acc: 0.5844
Epoch 20/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.9564 - acc: 0.5696 - val_loss: 0.9487 - val_acc: 0.5852
Epoch 21/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.9525 - acc: 0.5735 - val_loss: 0.9454 - val_acc: 0.5883
Epoch 22/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9488 - acc: 0.5773 - val_loss: 0.9414 - val_acc: 0.5949
Epoch 23/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.9451 - acc: 0.5796 - val_loss: 0.9375 - val_acc: 0.5893
Epoch 24/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9416 - acc: 0.5812 - val_loss: 0.9346 - val_acc: 0.6015
Epoch 25/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9384 - acc: 0.5852 - val_loss: 0.9313 - val_acc: 0.5987
Epoch 26/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.9352 - acc: 0.5878 - val_loss: 0.9273 - val_acc: 0.5969
Epoch 27/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.9321 - acc: 0.5875 - val_loss: 0.9249 - val_acc: 0.6036
Epoch 28/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9292 - acc: 0.5905 - val_loss: 0.9224 - val_acc: 0.6061
Epoch 29/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9263 - acc: 0.5920 - val_loss: 0.9187 - val_acc: 0.6000
Epoch 30/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9235 - acc: 0.5914 - val_loss: 0.9166 - val_acc: 0.6069
Epoch 31/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.9208 - acc: 0.5944 - val_loss: 0.9142 - val_acc: 0.6112
Epoch 32/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9181 - acc: 0.5950 - val_loss: 0.9117 - val_acc: 0.6135
Epoch 33/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9157 - acc: 0.5964 - val_loss: 0.9097 - val_acc: 0.6158
Epoch 34/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9134 - acc: 0.5983 - val_loss: 0.9066 - val_acc: 0.6163
Epoch 35/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9109 - acc: 0.5995 - val_loss: 0.9040 - val_acc: 0.6166
Epoch 36/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9086 - acc: 0.6007 - val_loss: 0.9018 - val_acc: 0.6156
Epoch 37/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.9063 - acc: 0.6030 - val_loss: 0.8996 - val_acc: 0.6176
Epoch 38/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9042 - acc: 0.6033 - val_loss: 0.8976 - val_acc: 0.6186
Epoch 39/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9020 - acc: 0.6046 - val_loss: 0.8952 - val_acc: 0.6156
Epoch 40/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.9000 - acc: 0.6057 - val_loss: 0.8936 - val_acc: 0.6181
Epoch 41/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8980 - acc: 0.6079 - val_loss: 0.8917 - val_acc: 0.6186
Epoch 42/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8962 - acc: 0.6073 - val_loss: 0.8898 - val_acc: 0.6204
Epoch 43/150
15664/15664 [==============================] - 0s 17us/step - loss: 0.8942 - acc: 0.6099 - val_loss: 0.8880 - val_acc: 0.6197
Epoch 44/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8924 - acc: 0.6099 - val_loss: 0.8856 - val_acc: 0.6186
Epoch 45/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8907 - acc: 0.6110 - val_loss: 0.8847 - val_acc: 0.6245
Epoch 46/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8889 - acc: 0.6115 - val_loss: 0.8822 - val_acc: 0.6194
Epoch 47/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8872 - acc: 0.6126 - val_loss: 0.8807 - val_acc: 0.6209
Epoch 48/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8856 - acc: 0.6134 - val_loss: 0.8794 - val_acc: 0.6232
Epoch 49/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8838 - acc: 0.6148 - val_loss: 0.8775 - val_acc: 0.6248
Epoch 50/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8823 - acc: 0.6152 - val_loss: 0.8766 - val_acc: 0.6245
Epoch 51/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8808 - acc: 0.6163 - val_loss: 0.8749 - val_acc: 0.6271
Epoch 52/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8792 - acc: 0.6179 - val_loss: 0.8731 - val_acc: 0.6289
Epoch 53/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8777 - acc: 0.6164 - val_loss: 0.8721 - val_acc: 0.6284
Epoch 54/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8764 - acc: 0.6171 - val_loss: 0.8702 - val_acc: 0.6284
Epoch 55/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8750 - acc: 0.6181 - val_loss: 0.8689 - val_acc: 0.6309
Epoch 56/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8737 - acc: 0.6187 - val_loss: 0.8675 - val_acc: 0.6314
Epoch 57/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8722 - acc: 0.6200 - val_loss: 0.8662 - val_acc: 0.6312
Epoch 58/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8711 - acc: 0.6205 - val_loss: 0.8646 - val_acc: 0.6312
Epoch 59/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8697 - acc: 0.6202 - val_loss: 0.8640 - val_acc: 0.6330
Epoch 60/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8684 - acc: 0.6217 - val_loss: 0.8625 - val_acc: 0.6314
Epoch 61/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8674 - acc: 0.6216 - val_loss: 0.8616 - val_acc: 0.6350
Epoch 62/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8660 - acc: 0.6216 - val_loss: 0.8597 - val_acc: 0.6319
Epoch 63/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8649 - acc: 0.6223 - val_loss: 0.8590 - val_acc: 0.6350
Epoch 64/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8637 - acc: 0.6231 - val_loss: 0.8582 - val_acc: 0.6360
Epoch 65/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8626 - acc: 0.6250 - val_loss: 0.8566 - val_acc: 0.6350
Epoch 66/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8614 - acc: 0.6249 - val_loss: 0.8553 - val_acc: 0.6342
Epoch 67/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8605 - acc: 0.6247 - val_loss: 0.8544 - val_acc: 0.6340
Epoch 68/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8594 - acc: 0.6251 - val_loss: 0.8537 - val_acc: 0.6375
Epoch 69/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8585 - acc: 0.6267 - val_loss: 0.8530 - val_acc: 0.6378
Epoch 70/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8574 - acc: 0.6263 - val_loss: 0.8517 - val_acc: 0.6388
Epoch 71/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8563 - acc: 0.6278 - val_loss: 0.8514 - val_acc: 0.6388
Epoch 72/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8555 - acc: 0.6272 - val_loss: 0.8504 - val_acc: 0.6421
Epoch 73/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8545 - acc: 0.6296 - val_loss: 0.8486 - val_acc: 0.6381
Epoch 74/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8535 - acc: 0.6292 - val_loss: 0.8480 - val_acc: 0.6381
Epoch 75/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8526 - acc: 0.6288 - val_loss: 0.8471 - val_acc: 0.6414
Epoch 76/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8517 - acc: 0.6287 - val_loss: 0.8462 - val_acc: 0.6381
Epoch 77/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8507 - acc: 0.6303 - val_loss: 0.8452 - val_acc: 0.6386
Epoch 78/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8499 - acc: 0.6297 - val_loss: 0.8447 - val_acc: 0.6378
Epoch 79/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8490 - acc: 0.6307 - val_loss: 0.8444 - val_acc: 0.6442
Epoch 80/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8482 - acc: 0.6310 - val_loss: 0.8431 - val_acc: 0.6416
Epoch 81/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8474 - acc: 0.6311 - val_loss: 0.8422 - val_acc: 0.6442
Epoch 82/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8466 - acc: 0.6322 - val_loss: 0.8418 - val_acc: 0.6455
Epoch 83/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8459 - acc: 0.6324 - val_loss: 0.8412 - val_acc: 0.6470
Epoch 84/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8451 - acc: 0.6327 - val_loss: 0.8401 - val_acc: 0.6442
Epoch 85/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8443 - acc: 0.6325 - val_loss: 0.8394 - val_acc: 0.6447
Epoch 86/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8436 - acc: 0.6332 - val_loss: 0.8385 - val_acc: 0.6437
Epoch 87/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8427 - acc: 0.6325 - val_loss: 0.8378 - val_acc: 0.6462
Epoch 88/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8421 - acc: 0.6333 - val_loss: 0.8365 - val_acc: 0.6447
Epoch 89/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8414 - acc: 0.6328 - val_loss: 0.8363 - val_acc: 0.6439
Epoch 90/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8408 - acc: 0.6341 - val_loss: 0.8358 - val_acc: 0.6450
Epoch 91/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8400 - acc: 0.6339 - val_loss: 0.8348 - val_acc: 0.6442
Epoch 92/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8393 - acc: 0.6336 - val_loss: 0.8344 - val_acc: 0.6473
Epoch 93/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8386 - acc: 0.6342 - val_loss: 0.8333 - val_acc: 0.6452
Epoch 94/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8380 - acc: 0.6350 - val_loss: 0.8326 - val_acc: 0.6442
Epoch 95/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8374 - acc: 0.6350 - val_loss: 0.8321 - val_acc: 0.6447
Epoch 96/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8367 - acc: 0.6346 - val_loss: 0.8319 - val_acc: 0.6467
Epoch 97/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8361 - acc: 0.6358 - val_loss: 0.8312 - val_acc: 0.6465
Epoch 98/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8354 - acc: 0.6355 - val_loss: 0.8307 - val_acc: 0.6478
Epoch 99/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8348 - acc: 0.6359 - val_loss: 0.8298 - val_acc: 0.6467
Epoch 100/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8342 - acc: 0.6374 - val_loss: 0.8298 - val_acc: 0.6506
Epoch 101/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8337 - acc: 0.6374 - val_loss: 0.8291 - val_acc: 0.6470
Epoch 102/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8331 - acc: 0.6375 - val_loss: 0.8281 - val_acc: 0.6442
Epoch 103/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8326 - acc: 0.6368 - val_loss: 0.8282 - val_acc: 0.6503
Epoch 104/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8320 - acc: 0.6373 - val_loss: 0.8269 - val_acc: 0.6475
Epoch 105/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8314 - acc: 0.6367 - val_loss: 0.8265 - val_acc: 0.6488
Epoch 106/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8309 - acc: 0.6376 - val_loss: 0.8262 - val_acc: 0.6498
Epoch 107/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8303 - acc: 0.6373 - val_loss: 0.8255 - val_acc: 0.6480
Epoch 108/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8299 - acc: 0.6371 - val_loss: 0.8252 - val_acc: 0.6501
Epoch 109/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8293 - acc: 0.6383 - val_loss: 0.8245 - val_acc: 0.6498
Epoch 110/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8288 - acc: 0.6393 - val_loss: 0.8243 - val_acc: 0.6508
Epoch 111/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8283 - acc: 0.6385 - val_loss: 0.8236 - val_acc: 0.6519
Epoch 112/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8277 - acc: 0.6389 - val_loss: 0.8234 - val_acc: 0.6513
Epoch 113/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8274 - acc: 0.6378 - val_loss: 0.8228 - val_acc: 0.6524
Epoch 114/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8268 - acc: 0.6391 - val_loss: 0.8221 - val_acc: 0.6542
Epoch 115/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8264 - acc: 0.6387 - val_loss: 0.8219 - val_acc: 0.6531
Epoch 116/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8260 - acc: 0.6394 - val_loss: 0.8210 - val_acc: 0.6521
Epoch 117/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8254 - acc: 0.6404 - val_loss: 0.8209 - val_acc: 0.6559
Epoch 118/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8250 - acc: 0.6411 - val_loss: 0.8204 - val_acc: 0.6524
Epoch 119/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8245 - acc: 0.6393 - val_loss: 0.8207 - val_acc: 0.6549
Epoch 120/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8240 - acc: 0.6410 - val_loss: 0.8198 - val_acc: 0.6544
Epoch 121/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8235 - acc: 0.6410 - val_loss: 0.8192 - val_acc: 0.6559
Epoch 122/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8231 - acc: 0.6415 - val_loss: 0.8185 - val_acc: 0.6549
Epoch 123/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8228 - acc: 0.6418 - val_loss: 0.8183 - val_acc: 0.6557
Epoch 124/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8223 - acc: 0.6410 - val_loss: 0.8185 - val_acc: 0.6552
Epoch 125/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8218 - acc: 0.6424 - val_loss: 0.8174 - val_acc: 0.6554
Epoch 126/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8215 - acc: 0.6417 - val_loss: 0.8175 - val_acc: 0.6557
Epoch 127/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8211 - acc: 0.6424 - val_loss: 0.8168 - val_acc: 0.6559
Epoch 128/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8206 - acc: 0.6420 - val_loss: 0.8160 - val_acc: 0.6559
Epoch 129/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8203 - acc: 0.6414 - val_loss: 0.8157 - val_acc: 0.6552
Epoch 130/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8198 - acc: 0.6427 - val_loss: 0.8158 - val_acc: 0.6562
Epoch 131/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8194 - acc: 0.6430 - val_loss: 0.8152 - val_acc: 0.6564
Epoch 132/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8191 - acc: 0.6424 - val_loss: 0.8147 - val_acc: 0.6554
Epoch 133/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8187 - acc: 0.6438 - val_loss: 0.8144 - val_acc: 0.6559
Epoch 134/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8182 - acc: 0.6450 - val_loss: 0.8143 - val_acc: 0.6552
Epoch 135/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8179 - acc: 0.6434 - val_loss: 0.8133 - val_acc: 0.6547
Epoch 136/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8177 - acc: 0.6441 - val_loss: 0.8133 - val_acc: 0.6554
Epoch 137/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8172 - acc: 0.6444 - val_loss: 0.8133 - val_acc: 0.6559
Epoch 138/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8169 - acc: 0.6443 - val_loss: 0.8125 - val_acc: 0.6559
Epoch 139/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8166 - acc: 0.6447 - val_loss: 0.8124 - val_acc: 0.6562
Epoch 140/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8162 - acc: 0.6451 - val_loss: 0.8119 - val_acc: 0.6559
Epoch 141/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8158 - acc: 0.6465 - val_loss: 0.8122 - val_acc: 0.6562
Epoch 142/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8155 - acc: 0.6447 - val_loss: 0.8112 - val_acc: 0.6559
Epoch 143/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8152 - acc: 0.6452 - val_loss: 0.8108 - val_acc: 0.6554
Epoch 144/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8148 - acc: 0.6449 - val_loss: 0.8116 - val_acc: 0.6567
Epoch 145/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8145 - acc: 0.6457 - val_loss: 0.8104 - val_acc: 0.6577
Epoch 146/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8142 - acc: 0.6452 - val_loss: 0.8102 - val_acc: 0.6564
Epoch 147/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8139 - acc: 0.6457 - val_loss: 0.8098 - val_acc: 0.6567
Epoch 148/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8135 - acc: 0.6455 - val_loss: 0.8100 - val_acc: 0.6577
Epoch 149/150
15664/15664 [==============================] - 0s 15us/step - loss: 0.8132 - acc: 0.6455 - val_loss: 0.8096 - val_acc: 0.6570
Epoch 150/150
15664/15664 [==============================] - 0s 14us/step - loss: 0.8129 - acc: 0.6464 - val_loss: 0.8088 - val_acc: 0.6590
15664/15664 [==============================] - 0s 22us/step
==== Training loss, score are: 0.8126655830426163 0.6449182839632278 =======
3915/3915 [==============================] - 0s 20us/step
==== CV loss, score are: 0.8087702428700824 0.6590038314023998 =======


===== Model: fast_text_glove  ========:
 Cross-val log losses are: [0.81551025462464311, 0.82363314849221192, 0.82074695217256188, 0.80912472312447459, 0.80877023530219094]
====== Mean cross-val log loss is: 0.8155570627432166 =========


</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="CNN-model-definition-and-running:">CNN model definition and running:<a class="anchor-link" href="#CNN-model-definition-and-running:">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[51]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># CNN </span>
<span class="k">def</span> <span class="nf">run_CNN</span><span class="p">(</span><span class="n">sentences_encoded_train</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span>
                 <span class="n">sentences_encoded_val</span><span class="p">,</span> <span class="n">y_val_one_hot</span><span class="p">,</span>
                 <span class="n">sentences_encoded_test</span><span class="p">,</span> 
                 <span class="n">model_params</span><span class="p">):</span>  

    <span class="c1"># All model params</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">]</span> 
    <span class="n">wordvecdim</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;wordvecdim&#39;</span><span class="p">]</span>
    <span class="n">sentence_maxlength_cap</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;sentence_maxlength_cap&#39;</span><span class="p">]</span>
    <span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;embedding_matrix&#39;</span><span class="p">]</span>
    <span class="n">word_vector_type</span> <span class="o">=</span> <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;word_vector_type&#39;</span><span class="p">]</span>
    <span class="n">tokenizer</span> <span class="o">=</span>  <span class="n">model_params</span><span class="p">[</span><span class="s1">&#39;tokenizer&#39;</span><span class="p">]</span>    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Creating CNN model vocab_size: </span><span class="si">{}</span><span class="s2">, wordvecdim </span><span class="si">{}</span><span class="s2">, sentence_maxlength_cap </span><span class="si">{}</span><span class="s2">, word_vector_type </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span> 
          <span class="n">vocab_size</span><span class="p">,</span> <span class="n">wordvecdim</span><span class="p">,</span> <span class="n">sentence_maxlength_cap</span><span class="p">,</span> <span class="n">word_vector_type</span><span class="p">))</span>
    
    <span class="c1"># Train CNN model (1 hidden layer)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">word_vector_type</span> <span class="o">==</span> <span class="s2">&quot;glove&quot;</span><span class="p">:</span>
        <span class="n">embedded_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">wordvecdim</span><span class="p">,</span> 
                                   <span class="n">input_length</span> <span class="o">=</span> <span class="n">sentence_maxlength_cap</span><span class="p">,</span> 
                                   <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">embedding_matrix</span><span class="p">],</span> <span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1">#embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = True)</span>
    <span class="k">elif</span> <span class="n">word_vector_type</span> <span class="o">==</span> <span class="s2">&quot;gensim&quot;</span><span class="p">:</span>
        <span class="n">embedded_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">wordvecdim</span><span class="p">,</span> 
                                   <span class="n">input_length</span> <span class="o">=</span> <span class="n">sentence_maxlength_cap</span><span class="p">,</span> 
                                   <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">embedding_matrix</span><span class="p">],</span> <span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">embedded_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">wordvecdim</span><span class="p">,</span> 
                                   <span class="n">input_length</span> <span class="o">=</span> <span class="n">sentence_maxlength_cap</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">embedded_layer</span><span class="p">)</span>
    <span class="c1">#model.add(Flatten())</span>
    <span class="c1">#model.add(Dense(20,activation=&#39;relu&#39;))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.8</span><span class="p">))</span>
    <span class="c1">#model.add(Conv1D(filters = 16, kernel_size = 4, activation=&#39;relu&#39;))</span>
    <span class="c1">#model.add(MaxPooling1D(pool_size=2))</span>
    <span class="c1">#model.add(Dropout(0.8))</span>
    <span class="c1">#model.add(Flatten())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">GlobalAveragePooling1D</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

    <span class="c1"># Compile the model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                  <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

    <span class="c1"># Pre training loss</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">sentences_encoded_train</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before training loss, score are: </span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">score</span><span class="p">))</span>

    <span class="c1"># Fit the model to train data</span>
    <span class="c1">#num_epochs = 200</span>
    <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">150</span>
    <span class="n">validation_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">sentences_encoded_val</span><span class="p">,</span> <span class="n">y_val_one_hot</span><span class="p">)</span>
    <span class="n">checkpointer</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">sentences_encoded_train</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span>
              <span class="n">epochs</span><span class="o">=</span> <span class="n">num_epochs</span><span class="p">,</span> 
              <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> 
              <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
              <span class="n">validation_data</span> <span class="o">=</span> <span class="n">validation_data</span><span class="p">,</span>
              <span class="c1">#validation_split = 0.05, shuffle = True,              </span>
              <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpointer</span><span class="p">])</span>

    <span class="c1"># Training loss and predictions:</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">sentences_encoded_train</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;====== Training loss, score are: </span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2"> =======&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">score</span><span class="p">))</span>
    <span class="n">pred_train</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sentences_encoded_train</span><span class="p">)</span>

    <span class="c1"># Val loss and predictions:</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">sentences_encoded_val</span><span class="p">,</span> <span class="n">y_val_one_hot</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;===== CV loss, score are: </span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2"> =======&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">score</span><span class="p">))</span>
    <span class="n">pred_val</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sentences_encoded_val</span><span class="p">)</span>
    
    <span class="c1"># Test loss and predictions:</span>
    <span class="n">pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sentences_encoded_test</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pred_val</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span>
</pre></div>

</div>
</div>
</div>

</div>## Create DL submission:
       
create_submission_dl('fasttext', model, sentences_encoded_ftest )
#create_submission_dl('test', model, sentences_encoded_test )
#pred_test = model.predict(sentences_encoded)
#print(pred_test[0:2,:])

<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[52]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Run CNN and add predictions as features:</span>

<span class="c1"># No pre-trained vectors:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Timestamp: {:%Y-%b-</span><span class="si">%d</span><span class="s1"> %H:%M:%S}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()))</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;cnn_none&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_NN_preprocess</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_CNN</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;temp&#39;</span><span class="p">:</span> <span class="mi">0</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;char_level&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;word_vector_type&#39;</span><span class="p">:</span> <span class="kc">None</span> <span class="p">}</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;NN&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
<span class="c1"># Pre-trained word2vec using vectors:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Timestamp: {:%Y-%b-</span><span class="si">%d</span><span class="s1"> %H:%M:%S}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()))</span>
<span class="n">wordvecsize</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">create_gensim_wordvectors</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">if</span> <span class="n">create_gensim_wordvectors</span><span class="p">:</span>
    <span class="n">create_gensim_wordvec</span><span class="p">(</span><span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">wordvecsize</span><span class="p">)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;cnn_gensim&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_NN_preprocess</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_CNN</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;temp&#39;</span><span class="p">:</span> <span class="mi">0</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;char_level&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;word_vector_type&#39;</span><span class="p">:</span> <span class="s2">&quot;gensim&quot;</span> <span class="p">}</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;NN&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
    
<span class="c1"># Glove vectors:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Timestamp: {:%Y-%b-</span><span class="si">%d</span><span class="s1"> %H:%M:%S}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()))</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;cnn_glove&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_NN_preprocess</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_CNN</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;temp&#39;</span><span class="p">:</span> <span class="mi">0</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;char_level&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;word_vector_type&#39;</span><span class="p">:</span> <span class="s2">&quot;glove&quot;</span> <span class="p">}</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;NN&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Timestamp: 2018-Jan-14 03:03:10
Running kfold training with model cnn_none
Shapes: x_train_raw.shape (19579, 27), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 26)
Running preprocess: &lt;function run_NN_preprocess at 0x7f620ee8eea0&gt;
No of unique words found is 247346
No of words occuring more than 2 times is 75104
No of unique words found is after capping is 247346
max sentence length is 1439
mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000
After padding: max sentence size is 161
After padding: mean, median length of sentence is 161.000000 161.000000
Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)
Running fold 1
Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_19 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 158, 32)           2592      
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 79, 32)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_19  (None, 32)                0         
_________________________________________________________________
dense_19 (Dense)             (None, 3)                 99        
=================================================================
Total params: 4,949,611
Trainable params: 4,949,611
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 82us/step
Before training loss, score are: 1.09661056803902 0.4066909276905594
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 2s 102us/step - loss: 1.0811 - acc: 0.4089 - val_loss: 1.0644 - val_acc: 0.4170
Epoch 2/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.9596 - acc: 0.5576 - val_loss: 0.8339 - val_acc: 0.7012
Epoch 3/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.6538 - acc: 0.7885 - val_loss: 0.5921 - val_acc: 0.7942
Epoch 4/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.4098 - acc: 0.8846 - val_loss: 0.4632 - val_acc: 0.8394
Epoch 5/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.2732 - acc: 0.9282 - val_loss: 0.3998 - val_acc: 0.8504
Epoch 6/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.1879 - acc: 0.9551 - val_loss: 0.3681 - val_acc: 0.8613
Epoch 7/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.1389 - acc: 0.9690 - val_loss: 0.3559 - val_acc: 0.8618
Epoch 8/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1035 - acc: 0.9775 - val_loss: 0.3501 - val_acc: 0.8606
Epoch 9/150
15663/15663 [==============================] - 1s 56us/step - loss: 0.0823 - acc: 0.9834 - val_loss: 0.3505 - val_acc: 0.8613
Epoch 10/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0655 - acc: 0.9870 - val_loss: 0.3532 - val_acc: 0.8624
Epoch 11/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0527 - acc: 0.9909 - val_loss: 0.3622 - val_acc: 0.8606
15663/15663 [==============================] - 0s 27us/step
====== Training loss, score are: 0.028422342061788113 0.9966162293302688 =======
3916/3916 [==============================] - 0s 45us/step
===== CV loss, score are: 0.36218819420963555 0.8605720123182887 =======
Running fold 2
Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_20 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 158, 32)           2592      
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 79, 32)            0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_20  (None, 32)                0         
_________________________________________________________________
dense_20 (Dense)             (None, 3)                 99        
=================================================================
Total params: 4,949,611
Trainable params: 4,949,611
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 46us/step
Before training loss, score are: 1.0992496341213842 0.3213943689081668
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 79us/step - loss: 1.0834 - acc: 0.4037 - val_loss: 1.0745 - val_acc: 0.3966
Epoch 2/150
15663/15663 [==============================] - 1s 57us/step - loss: 0.9889 - acc: 0.5229 - val_loss: 0.8753 - val_acc: 0.6223
Epoch 3/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.6710 - acc: 0.7768 - val_loss: 0.6023 - val_acc: 0.8087
Epoch 4/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.4187 - acc: 0.8757 - val_loss: 0.4689 - val_acc: 0.8417
Epoch 5/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.2781 - acc: 0.9232 - val_loss: 0.4146 - val_acc: 0.8430
Epoch 6/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1978 - acc: 0.9477 - val_loss: 0.3792 - val_acc: 0.8624
Epoch 7/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.1432 - acc: 0.9650 - val_loss: 0.3646 - val_acc: 0.8573
Epoch 8/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1088 - acc: 0.9750 - val_loss: 0.3504 - val_acc: 0.8659
Epoch 9/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.0830 - acc: 0.9818 - val_loss: 0.3515 - val_acc: 0.8631
Epoch 10/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.0660 - acc: 0.9868 - val_loss: 0.3531 - val_acc: 0.8682
Epoch 11/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0539 - acc: 0.9898 - val_loss: 0.3639 - val_acc: 0.8670
15663/15663 [==============================] - 0s 26us/step
====== Training loss, score are: 0.03306201452471771 0.9972546766264445 =======
3916/3916 [==============================] - 0s 25us/step
===== CV loss, score are: 0.363909604661433 0.8669560776911182 =======
Running fold 3
Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_21 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 158, 32)           2592      
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, 79, 32)            0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_21  (None, 32)                0         
_________________________________________________________________
dense_21 (Dense)             (None, 3)                 99        
=================================================================
Total params: 4,949,611
Trainable params: 4,949,611
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 46us/step
Before training loss, score are: 1.0986588536220316 0.32247972931166546
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 78us/step - loss: 1.0828 - acc: 0.4048 - val_loss: 1.0634 - val_acc: 0.4188
Epoch 2/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.9692 - acc: 0.5284 - val_loss: 0.8532 - val_acc: 0.6356
Epoch 3/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.6736 - acc: 0.7681 - val_loss: 0.5988 - val_acc: 0.7875
Epoch 4/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.4240 - acc: 0.8728 - val_loss: 0.4666 - val_acc: 0.8394
Epoch 5/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.2837 - acc: 0.9203 - val_loss: 0.4002 - val_acc: 0.8565
Epoch 6/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.1974 - acc: 0.9496 - val_loss: 0.3668 - val_acc: 0.8647
Epoch 7/150
15663/15663 [==============================] - 1s 57us/step - loss: 0.1449 - acc: 0.9640 - val_loss: 0.3495 - val_acc: 0.8657
Epoch 8/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1095 - acc: 0.9751 - val_loss: 0.3405 - val_acc: 0.8721
Epoch 9/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0837 - acc: 0.9842 - val_loss: 0.3416 - val_acc: 0.8685
Epoch 10/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0665 - acc: 0.9861 - val_loss: 0.3388 - val_acc: 0.8690
Epoch 11/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0537 - acc: 0.9900 - val_loss: 0.3382 - val_acc: 0.8693
Epoch 12/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.0444 - acc: 0.9923 - val_loss: 0.3412 - val_acc: 0.8680
Epoch 13/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.0366 - acc: 0.9937 - val_loss: 0.3495 - val_acc: 0.8664
Epoch 14/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.0311 - acc: 0.9950 - val_loss: 0.3558 - val_acc: 0.8664
15663/15663 [==============================] - 0s 26us/step
====== Training loss, score are: 0.014943611511237236 0.9990423290557364 =======
3916/3916 [==============================] - 0s 27us/step
===== CV loss, score are: 0.3558085434770438 0.8664453524004085 =======
Running fold 4
Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_22 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
conv1d_4 (Conv1D)            (None, 158, 32)           2592      
_________________________________________________________________
max_pooling1d_4 (MaxPooling1 (None, 79, 32)            0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_22  (None, 32)                0         
_________________________________________________________________
dense_22 (Dense)             (None, 3)                 99        
=================================================================
Total params: 4,949,611
Trainable params: 4,949,611
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 48us/step
Before training loss, score are: 1.0996042492288758 0.3130307093216056
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 80us/step - loss: 1.0829 - acc: 0.4020 - val_loss: 1.0603 - val_acc: 0.4275
Epoch 2/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.9574 - acc: 0.5685 - val_loss: 0.8192 - val_acc: 0.7135
Epoch 3/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.6284 - acc: 0.8043 - val_loss: 0.5672 - val_acc: 0.8039
Epoch 4/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.3913 - acc: 0.8892 - val_loss: 0.4511 - val_acc: 0.8340
Epoch 5/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.2589 - acc: 0.9314 - val_loss: 0.3920 - val_acc: 0.8529
Epoch 6/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1801 - acc: 0.9557 - val_loss: 0.3632 - val_acc: 0.8626
Epoch 7/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.1327 - acc: 0.9679 - val_loss: 0.3499 - val_acc: 0.8626
Epoch 8/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.1011 - acc: 0.9778 - val_loss: 0.3435 - val_acc: 0.8629
Epoch 9/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.0770 - acc: 0.9853 - val_loss: 0.3452 - val_acc: 0.8603
Epoch 10/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.0636 - acc: 0.9864 - val_loss: 0.3452 - val_acc: 0.8603
Epoch 11/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.0516 - acc: 0.9904 - val_loss: 0.3486 - val_acc: 0.8624
15663/15663 [==============================] - 0s 27us/step
====== Training loss, score are: 0.026671318204130795 0.9977654344633851 =======
3916/3916 [==============================] - 0s 27us/step
===== CV loss, score are: 0.34859397109365803 0.862359550622681 =======
Running fold 5
Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_23 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
conv1d_5 (Conv1D)            (None, 158, 32)           2592      
_________________________________________________________________
max_pooling1d_5 (MaxPooling1 (None, 79, 32)            0         
_________________________________________________________________
dropout_5 (Dropout)          (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_23  (None, 32)                0         
_________________________________________________________________
dense_23 (Dense)             (None, 3)                 99        
=================================================================
Total params: 4,949,611
Trainable params: 4,949,611
Non-trainable params: 0
_________________________________________________________________
15664/15664 [==============================] - 1s 50us/step
Before training loss, score are: 1.100977397427739 0.28919816138917265
Train on 15664 samples, validate on 3915 samples
Epoch 1/150
15664/15664 [==============================] - 1s 92us/step - loss: 1.0846 - acc: 0.3942 - val_loss: 1.0570 - val_acc: 0.4281
Epoch 2/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.9579 - acc: 0.5699 - val_loss: 0.8181 - val_acc: 0.7374
Epoch 3/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.6286 - acc: 0.8092 - val_loss: 0.5688 - val_acc: 0.8347
Epoch 4/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.3823 - acc: 0.8969 - val_loss: 0.4450 - val_acc: 0.8506
Epoch 5/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.2524 - acc: 0.9371 - val_loss: 0.3938 - val_acc: 0.8618
Epoch 6/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.1764 - acc: 0.9581 - val_loss: 0.3642 - val_acc: 0.8639
Epoch 7/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.1287 - acc: 0.9735 - val_loss: 0.3476 - val_acc: 0.8656
Epoch 8/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.0967 - acc: 0.9791 - val_loss: 0.3453 - val_acc: 0.8677
Epoch 9/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.0752 - acc: 0.9854 - val_loss: 0.3468 - val_acc: 0.8646
Epoch 10/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.0597 - acc: 0.9891 - val_loss: 0.3456 - val_acc: 0.8649
Epoch 11/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.0490 - acc: 0.9922 - val_loss: 0.3506 - val_acc: 0.8646
15664/15664 [==============================] - 0s 30us/step
====== Training loss, score are: 0.0258067818553557 0.9981486210418795 =======
3915/3915 [==============================] - 0s 30us/step
===== CV loss, score are: 0.3506191249779815 0.8646232439183641 =======


===== Model: cnn_none  ========:
 Cross-val log losses are: [0.36218818797598962, 0.3639095987832564, 0.35580853542306584, 0.34859396705752815, 0.35061912189149319]
====== Mean cross-val log loss is: 0.3562238822262666 =========


Timestamp: 2018-Jan-14 03:04:21
Running kfold training with model cnn_gensim
Shapes: x_train_raw.shape (19579, 30), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 29)
Running preprocess: &lt;function run_NN_preprocess at 0x7f620ee8eea0&gt;
No of unique words found is 247346
No of words occuring more than 2 times is 75104
No of unique words found is after capping is 247346
max sentence length is 1439
mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000
After padding: max sentence size is 161
After padding: mean, median length of sentence is 161.000000 161.000000
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>9786it [00:00, 97837.00it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Creating word embedding matrix for gensim vectors
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>76597it [00:00, 99453.01it/s]
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Found 76597 word vectors.
of--mirth 247345
None
(247346, 20)
Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)
Running fold 1
Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_24 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
conv1d_6 (Conv1D)            (None, 158, 32)           2592      
_________________________________________________________________
max_pooling1d_6 (MaxPooling1 (None, 79, 32)            0         
_________________________________________________________________
dropout_6 (Dropout)          (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_24  (None, 32)                0         
_________________________________________________________________
dense_24 (Dense)             (None, 3)                 99        
=================================================================
Total params: 4,949,611
Trainable params: 4,949,611
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 51us/step
Before training loss, score are: 1.8506783989359727 0.3065823916330851
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 91us/step - loss: 1.1146 - acc: 0.4193 - val_loss: 0.9560 - val_acc: 0.5715
Epoch 2/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.9522 - acc: 0.5461 - val_loss: 0.8747 - val_acc: 0.6284
Epoch 3/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.8946 - acc: 0.5879 - val_loss: 0.8356 - val_acc: 0.6384
Epoch 4/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.8648 - acc: 0.6095 - val_loss: 0.8059 - val_acc: 0.6588
Epoch 5/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.8295 - acc: 0.6339 - val_loss: 0.7847 - val_acc: 0.6647
Epoch 6/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.7934 - acc: 0.6545 - val_loss: 0.7557 - val_acc: 0.6874
Epoch 7/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.7691 - acc: 0.6676 - val_loss: 0.7354 - val_acc: 0.6910
Epoch 8/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.7405 - acc: 0.6862 - val_loss: 0.7105 - val_acc: 0.7066
Epoch 9/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.7069 - acc: 0.7057 - val_loss: 0.6896 - val_acc: 0.7165
Epoch 10/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.6775 - acc: 0.7198 - val_loss: 0.6646 - val_acc: 0.7314
Epoch 11/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.6430 - acc: 0.7388 - val_loss: 0.6491 - val_acc: 0.7319
Epoch 12/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.6052 - acc: 0.7592 - val_loss: 0.6311 - val_acc: 0.7400
Epoch 13/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.5728 - acc: 0.7773 - val_loss: 0.5987 - val_acc: 0.7577
Epoch 14/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.5447 - acc: 0.7867 - val_loss: 0.5895 - val_acc: 0.7594
Epoch 15/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.5044 - acc: 0.8089 - val_loss: 0.5618 - val_acc: 0.7758
Epoch 16/150
15663/15663 [==============================] - 1s 56us/step - loss: 0.4702 - acc: 0.8233 - val_loss: 0.5457 - val_acc: 0.7809
Epoch 17/150
15663/15663 [==============================] - 1s 56us/step - loss: 0.4414 - acc: 0.8368 - val_loss: 0.5322 - val_acc: 0.7852
Epoch 18/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.4110 - acc: 0.8473 - val_loss: 0.5125 - val_acc: 0.7962
Epoch 19/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.3831 - acc: 0.8616 - val_loss: 0.4976 - val_acc: 0.8054
Epoch 20/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.3548 - acc: 0.8750 - val_loss: 0.4842 - val_acc: 0.8121
Epoch 21/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.3280 - acc: 0.8848 - val_loss: 0.4809 - val_acc: 0.8149
Epoch 22/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.3029 - acc: 0.8931 - val_loss: 0.4648 - val_acc: 0.8205
Epoch 23/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.2825 - acc: 0.9047 - val_loss: 0.4568 - val_acc: 0.8243
Epoch 24/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.2603 - acc: 0.9114 - val_loss: 0.4475 - val_acc: 0.8253
Epoch 25/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.2377 - acc: 0.9212 - val_loss: 0.4410 - val_acc: 0.8279
Epoch 26/150
15663/15663 [==============================] - 1s 57us/step - loss: 0.2252 - acc: 0.9247 - val_loss: 0.4361 - val_acc: 0.8289
Epoch 27/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.2011 - acc: 0.9370 - val_loss: 0.4284 - val_acc: 0.8330
Epoch 28/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1875 - acc: 0.9380 - val_loss: 0.4225 - val_acc: 0.8355
Epoch 29/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1710 - acc: 0.9460 - val_loss: 0.4195 - val_acc: 0.8361
Epoch 30/150
15663/15663 [==============================] - 1s 56us/step - loss: 0.1567 - acc: 0.9503 - val_loss: 0.4201 - val_acc: 0.8378
Epoch 31/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1488 - acc: 0.9550 - val_loss: 0.4160 - val_acc: 0.8394
Epoch 32/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1327 - acc: 0.9604 - val_loss: 0.4132 - val_acc: 0.8404
Epoch 33/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1252 - acc: 0.9630 - val_loss: 0.4193 - val_acc: 0.8384
Epoch 34/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1127 - acc: 0.9680 - val_loss: 0.4128 - val_acc: 0.8442
Epoch 35/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1054 - acc: 0.9704 - val_loss: 0.4141 - val_acc: 0.8440
Epoch 36/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0994 - acc: 0.9713 - val_loss: 0.4127 - val_acc: 0.8450
Epoch 37/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0908 - acc: 0.9744 - val_loss: 0.4148 - val_acc: 0.8465
Epoch 38/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0812 - acc: 0.9765 - val_loss: 0.4196 - val_acc: 0.8493
Epoch 39/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0786 - acc: 0.9776 - val_loss: 0.4162 - val_acc: 0.8478
15663/15663 [==============================] - 0s 27us/step
====== Training loss, score are: 0.04184272022462284 0.9925940113643619 =======
3916/3916 [==============================] - 0s 27us/step
===== CV loss, score are: 0.41617487559705757 0.8478038815726299 =======
Running fold 2
Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_25 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
conv1d_7 (Conv1D)            (None, 158, 32)           2592      
_________________________________________________________________
max_pooling1d_7 (MaxPooling1 (None, 79, 32)            0         
_________________________________________________________________
dropout_7 (Dropout)          (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_25  (None, 32)                0         
_________________________________________________________________
dense_25 (Dense)             (None, 3)                 99        
=================================================================
Total params: 4,949,611
Trainable params: 4,949,611
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 56us/step
Before training loss, score are: 1.2340137674107845 0.31047692013309736
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 84us/step - loss: 1.0597 - acc: 0.4475 - val_loss: 0.9445 - val_acc: 0.5873
Epoch 2/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.9343 - acc: 0.5609 - val_loss: 0.8813 - val_acc: 0.5996
Epoch 3/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.8806 - acc: 0.6004 - val_loss: 0.8379 - val_acc: 0.6310
Epoch 4/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.8433 - acc: 0.6203 - val_loss: 0.8119 - val_acc: 0.6433
Epoch 5/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.8130 - acc: 0.6430 - val_loss: 0.7843 - val_acc: 0.6581
Epoch 6/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.7804 - acc: 0.6656 - val_loss: 0.7633 - val_acc: 0.6752
Epoch 7/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.7446 - acc: 0.6833 - val_loss: 0.7448 - val_acc: 0.6851
Epoch 8/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.7174 - acc: 0.6983 - val_loss: 0.7173 - val_acc: 0.6966
Epoch 9/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.6809 - acc: 0.7179 - val_loss: 0.7027 - val_acc: 0.7094
Epoch 10/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.6570 - acc: 0.7299 - val_loss: 0.6719 - val_acc: 0.7199
Epoch 11/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.6172 - acc: 0.7513 - val_loss: 0.6527 - val_acc: 0.7298
Epoch 12/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.5819 - acc: 0.7700 - val_loss: 0.6290 - val_acc: 0.7423
Epoch 13/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.5448 - acc: 0.7889 - val_loss: 0.6068 - val_acc: 0.7546
Epoch 14/150
15663/15663 [==============================] - 1s 56us/step - loss: 0.5130 - acc: 0.8023 - val_loss: 0.5871 - val_acc: 0.7666
Epoch 15/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.4713 - acc: 0.8212 - val_loss: 0.5720 - val_acc: 0.7697
Epoch 16/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.4409 - acc: 0.8343 - val_loss: 0.5618 - val_acc: 0.7817
Epoch 17/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.4115 - acc: 0.8485 - val_loss: 0.5346 - val_acc: 0.7924
Epoch 18/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.3792 - acc: 0.8623 - val_loss: 0.5223 - val_acc: 0.8023
Epoch 19/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.3543 - acc: 0.8742 - val_loss: 0.5107 - val_acc: 0.8054
Epoch 20/150
15663/15663 [==============================] - 1s 56us/step - loss: 0.3281 - acc: 0.8832 - val_loss: 0.4957 - val_acc: 0.8133
Epoch 21/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.2965 - acc: 0.8969 - val_loss: 0.4828 - val_acc: 0.8154
Epoch 22/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.2783 - acc: 0.9026 - val_loss: 0.4726 - val_acc: 0.8202
Epoch 23/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.2551 - acc: 0.9114 - val_loss: 0.4640 - val_acc: 0.8243
Epoch 24/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.2382 - acc: 0.9200 - val_loss: 0.4602 - val_acc: 0.8253
Epoch 25/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.2149 - acc: 0.9293 - val_loss: 0.4504 - val_acc: 0.8312
Epoch 26/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1959 - acc: 0.9369 - val_loss: 0.4467 - val_acc: 0.8327
Epoch 27/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1816 - acc: 0.9411 - val_loss: 0.4470 - val_acc: 0.8338
Epoch 28/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1649 - acc: 0.9485 - val_loss: 0.4389 - val_acc: 0.8330
Epoch 29/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1543 - acc: 0.9508 - val_loss: 0.4391 - val_acc: 0.8340
Epoch 30/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1426 - acc: 0.9549 - val_loss: 0.4332 - val_acc: 0.8394
Epoch 31/150
15663/15663 [==============================] - 1s 56us/step - loss: 0.1314 - acc: 0.9600 - val_loss: 0.4333 - val_acc: 0.8381
Epoch 32/150
15663/15663 [==============================] - 1s 56us/step - loss: 0.1178 - acc: 0.9638 - val_loss: 0.4329 - val_acc: 0.8391
Epoch 33/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1137 - acc: 0.9652 - val_loss: 0.4316 - val_acc: 0.8409
Epoch 34/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.1037 - acc: 0.9697 - val_loss: 0.4334 - val_acc: 0.8404
Epoch 35/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0988 - acc: 0.9712 - val_loss: 0.4320 - val_acc: 0.8437
Epoch 36/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0900 - acc: 0.9743 - val_loss: 0.4315 - val_acc: 0.8460
Epoch 37/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0862 - acc: 0.9753 - val_loss: 0.4315 - val_acc: 0.8463
Epoch 38/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0771 - acc: 0.9793 - val_loss: 0.4374 - val_acc: 0.8465
Epoch 39/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.0738 - acc: 0.9796 - val_loss: 0.4373 - val_acc: 0.8447
15663/15663 [==============================] - 0s 26us/step
====== Training loss, score are: 0.04190364783788058 0.9927855455532146 =======
3916/3916 [==============================] - 0s 27us/step
===== CV loss, score are: 0.43734352934470094 0.8447395301327886 =======
Running fold 3
Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_26 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
conv1d_8 (Conv1D)            (None, 158, 32)           2592      
_________________________________________________________________
max_pooling1d_8 (MaxPooling1 (None, 79, 32)            0         
_________________________________________________________________
dropout_8 (Dropout)          (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_26  (None, 32)                0         
_________________________________________________________________
dense_26 (Dense)             (None, 3)                 99        
=================================================================
Total params: 4,949,611
Trainable params: 4,949,611
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 54us/step
Before training loss, score are: 1.3637523977059192 0.4042648279612863
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 85us/step - loss: 1.0693 - acc: 0.4401 - val_loss: 0.9585 - val_acc: 0.5679
Epoch 2/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.9407 - acc: 0.5590 - val_loss: 0.8834 - val_acc: 0.6182
Epoch 3/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.8857 - acc: 0.5945 - val_loss: 0.8449 - val_acc: 0.6323
Epoch 4/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.8476 - acc: 0.6179 - val_loss: 0.8176 - val_acc: 0.6466
Epoch 5/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.8164 - acc: 0.6413 - val_loss: 0.7877 - val_acc: 0.6647
Epoch 6/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.7820 - acc: 0.6640 - val_loss: 0.7645 - val_acc: 0.6780
Epoch 7/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.7509 - acc: 0.6781 - val_loss: 0.7394 - val_acc: 0.6928
Epoch 8/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.7209 - acc: 0.6985 - val_loss: 0.7174 - val_acc: 0.7086
Epoch 9/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.6901 - acc: 0.7141 - val_loss: 0.6941 - val_acc: 0.7158
Epoch 10/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.6511 - acc: 0.7361 - val_loss: 0.6733 - val_acc: 0.7265
Epoch 11/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.6188 - acc: 0.7530 - val_loss: 0.6493 - val_acc: 0.7400
Epoch 12/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.5821 - acc: 0.7682 - val_loss: 0.6277 - val_acc: 0.7459
Epoch 13/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.5517 - acc: 0.7846 - val_loss: 0.6093 - val_acc: 0.7572
Epoch 14/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.5166 - acc: 0.7998 - val_loss: 0.5870 - val_acc: 0.7666
Epoch 15/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.4777 - acc: 0.8174 - val_loss: 0.5687 - val_acc: 0.7778
Epoch 16/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.4445 - acc: 0.8356 - val_loss: 0.5523 - val_acc: 0.7850
Epoch 17/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.4137 - acc: 0.8477 - val_loss: 0.5370 - val_acc: 0.7901
Epoch 18/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.3878 - acc: 0.8597 - val_loss: 0.5162 - val_acc: 0.7990
Epoch 19/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.3527 - acc: 0.8761 - val_loss: 0.5020 - val_acc: 0.8075
Epoch 20/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.3271 - acc: 0.8862 - val_loss: 0.4906 - val_acc: 0.8075
Epoch 21/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.2991 - acc: 0.8980 - val_loss: 0.4756 - val_acc: 0.8154
Epoch 22/150
15663/15663 [==============================] - 1s 56us/step - loss: 0.2725 - acc: 0.9063 - val_loss: 0.4673 - val_acc: 0.8200
Epoch 23/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.2530 - acc: 0.9146 - val_loss: 0.4577 - val_acc: 0.8238
Epoch 24/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.2325 - acc: 0.9252 - val_loss: 0.4522 - val_acc: 0.8292
Epoch 25/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.2143 - acc: 0.9273 - val_loss: 0.4420 - val_acc: 0.8322
Epoch 26/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1912 - acc: 0.9403 - val_loss: 0.4351 - val_acc: 0.8355
Epoch 27/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1832 - acc: 0.9406 - val_loss: 0.4408 - val_acc: 0.8340
Epoch 28/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1622 - acc: 0.9515 - val_loss: 0.4320 - val_acc: 0.8396
Epoch 29/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1559 - acc: 0.9509 - val_loss: 0.4235 - val_acc: 0.8417
Epoch 30/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1398 - acc: 0.9582 - val_loss: 0.4219 - val_acc: 0.8450
Epoch 31/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1309 - acc: 0.9607 - val_loss: 0.4191 - val_acc: 0.8468
Epoch 32/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1197 - acc: 0.9639 - val_loss: 0.4166 - val_acc: 0.8463
Epoch 33/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1130 - acc: 0.9672 - val_loss: 0.4150 - val_acc: 0.8504
Epoch 34/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1012 - acc: 0.9731 - val_loss: 0.4164 - val_acc: 0.8455
Epoch 35/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0959 - acc: 0.9719 - val_loss: 0.4139 - val_acc: 0.8509
Epoch 36/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.0871 - acc: 0.9759 - val_loss: 0.4132 - val_acc: 0.8511
Epoch 37/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0819 - acc: 0.9771 - val_loss: 0.4156 - val_acc: 0.8521
Epoch 38/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.0744 - acc: 0.9808 - val_loss: 0.4188 - val_acc: 0.8501
Epoch 39/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.0693 - acc: 0.9805 - val_loss: 0.4174 - val_acc: 0.8524
15663/15663 [==============================] - 0s 26us/step
====== Training loss, score are: 0.037740302610646974 0.9936793717678606 =======
3916/3916 [==============================] - 0s 26us/step
===== CV loss, score are: 0.41739493480620027 0.8524004085193007 =======
Running fold 4
Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_27 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
conv1d_9 (Conv1D)            (None, 158, 32)           2592      
_________________________________________________________________
max_pooling1d_9 (MaxPooling1 (None, 79, 32)            0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_27  (None, 32)                0         
_________________________________________________________________
dense_27 (Dense)             (None, 3)                 99        
=================================================================
Total params: 4,949,611
Trainable params: 4,949,611
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 55us/step
Before training loss, score are: 1.3859350388133647 0.3248419842999047
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 87us/step - loss: 1.0710 - acc: 0.4370 - val_loss: 0.9456 - val_acc: 0.5794
Epoch 2/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.9395 - acc: 0.5524 - val_loss: 0.8759 - val_acc: 0.6144
Epoch 3/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.8860 - acc: 0.5933 - val_loss: 0.8375 - val_acc: 0.6328
Epoch 4/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.8489 - acc: 0.6234 - val_loss: 0.8016 - val_acc: 0.6547
Epoch 5/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.8186 - acc: 0.6423 - val_loss: 0.7773 - val_acc: 0.6673
Epoch 6/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.7815 - acc: 0.6636 - val_loss: 0.7530 - val_acc: 0.6795
Epoch 7/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.7553 - acc: 0.6771 - val_loss: 0.7295 - val_acc: 0.6987
Epoch 8/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.7207 - acc: 0.6991 - val_loss: 0.7033 - val_acc: 0.7066
Epoch 9/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.6843 - acc: 0.7188 - val_loss: 0.6765 - val_acc: 0.7219
Epoch 10/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.6516 - acc: 0.7326 - val_loss: 0.6557 - val_acc: 0.7314
Epoch 11/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.6113 - acc: 0.7585 - val_loss: 0.6282 - val_acc: 0.7454
Epoch 12/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.5773 - acc: 0.7724 - val_loss: 0.6079 - val_acc: 0.7572
Epoch 13/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.5397 - acc: 0.7896 - val_loss: 0.5848 - val_acc: 0.7648
Epoch 14/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.5088 - acc: 0.8071 - val_loss: 0.5646 - val_acc: 0.7758
Epoch 15/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.4691 - acc: 0.8248 - val_loss: 0.5436 - val_acc: 0.7855
Epoch 16/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.4380 - acc: 0.8394 - val_loss: 0.5269 - val_acc: 0.7932
Epoch 17/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.4060 - acc: 0.8495 - val_loss: 0.5130 - val_acc: 0.7993
Epoch 18/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.3725 - acc: 0.8679 - val_loss: 0.4950 - val_acc: 0.8062
Epoch 19/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.3494 - acc: 0.8759 - val_loss: 0.4809 - val_acc: 0.8156
Epoch 20/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.3189 - acc: 0.8849 - val_loss: 0.4705 - val_acc: 0.8192
Epoch 21/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.2934 - acc: 0.9008 - val_loss: 0.4579 - val_acc: 0.8258
Epoch 22/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.2710 - acc: 0.9069 - val_loss: 0.4495 - val_acc: 0.8223
Epoch 23/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.2488 - acc: 0.9182 - val_loss: 0.4399 - val_acc: 0.8330
Epoch 24/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.2295 - acc: 0.9242 - val_loss: 0.4335 - val_acc: 0.8338
Epoch 25/150
15663/15663 [==============================] - 1s 54us/step - loss: 0.2101 - acc: 0.9319 - val_loss: 0.4280 - val_acc: 0.8332
Epoch 26/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1901 - acc: 0.9391 - val_loss: 0.4211 - val_acc: 0.8361
Epoch 27/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1788 - acc: 0.9421 - val_loss: 0.4191 - val_acc: 0.8407
Epoch 28/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1625 - acc: 0.9506 - val_loss: 0.4138 - val_acc: 0.8435
Epoch 29/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1540 - acc: 0.9512 - val_loss: 0.4110 - val_acc: 0.8435
Epoch 30/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1386 - acc: 0.9560 - val_loss: 0.4084 - val_acc: 0.8455
Epoch 31/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1261 - acc: 0.9618 - val_loss: 0.4094 - val_acc: 0.8463
Epoch 32/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1208 - acc: 0.9639 - val_loss: 0.4063 - val_acc: 0.8478
Epoch 33/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.1068 - acc: 0.9693 - val_loss: 0.4062 - val_acc: 0.8465
Epoch 34/150
15663/15663 [==============================] - 1s 56us/step - loss: 0.1023 - acc: 0.9698 - val_loss: 0.4114 - val_acc: 0.8442
Epoch 35/150
15663/15663 [==============================] - 1s 56us/step - loss: 0.0959 - acc: 0.9724 - val_loss: 0.4090 - val_acc: 0.8478
Epoch 36/150
15663/15663 [==============================] - 1s 55us/step - loss: 0.0867 - acc: 0.9754 - val_loss: 0.4094 - val_acc: 0.8491
15663/15663 [==============================] - 0s 27us/step
====== Training loss, score are: 0.05056679517277832 0.989465619613101 =======
3916/3916 [==============================] - 0s 28us/step
===== CV loss, score are: 0.4093880111368733 0.8490806946471957 =======
Running fold 5
Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_28 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
conv1d_10 (Conv1D)           (None, 158, 32)           2592      
_________________________________________________________________
max_pooling1d_10 (MaxPooling (None, 79, 32)            0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_28  (None, 32)                0         
_________________________________________________________________
dense_28 (Dense)             (None, 3)                 99        
=================================================================
Total params: 4,949,611
Trainable params: 4,949,611
Non-trainable params: 0
_________________________________________________________________
15664/15664 [==============================] - 1s 57us/step
Before training loss, score are: 1.7870014084006476 0.28919816138917265
Train on 15664 samples, validate on 3915 samples
Epoch 1/150
15664/15664 [==============================] - 1s 88us/step - loss: 1.0962 - acc: 0.4301 - val_loss: 0.9670 - val_acc: 0.5739
Epoch 2/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.9525 - acc: 0.5466 - val_loss: 0.8780 - val_acc: 0.6255
Epoch 3/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.8963 - acc: 0.5884 - val_loss: 0.8360 - val_acc: 0.6450
Epoch 4/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.8595 - acc: 0.6129 - val_loss: 0.8071 - val_acc: 0.6572
Epoch 5/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.8282 - acc: 0.6345 - val_loss: 0.7786 - val_acc: 0.6754
Epoch 6/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.7948 - acc: 0.6513 - val_loss: 0.7542 - val_acc: 0.6879
Epoch 7/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.7661 - acc: 0.6657 - val_loss: 0.7306 - val_acc: 0.6989
Epoch 8/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.7303 - acc: 0.6923 - val_loss: 0.7074 - val_acc: 0.7147
Epoch 9/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.6990 - acc: 0.7074 - val_loss: 0.6822 - val_acc: 0.7318
Epoch 10/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.6646 - acc: 0.7270 - val_loss: 0.6594 - val_acc: 0.7428
Epoch 11/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.6329 - acc: 0.7446 - val_loss: 0.6356 - val_acc: 0.7510
Epoch 12/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.5953 - acc: 0.7637 - val_loss: 0.6144 - val_acc: 0.7655
Epoch 13/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.5653 - acc: 0.7794 - val_loss: 0.5902 - val_acc: 0.7722
Epoch 14/150
15664/15664 [==============================] - 1s 56us/step - loss: 0.5312 - acc: 0.7925 - val_loss: 0.5735 - val_acc: 0.7808
Epoch 15/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.4951 - acc: 0.8086 - val_loss: 0.5496 - val_acc: 0.7890
Epoch 16/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.4640 - acc: 0.8269 - val_loss: 0.5323 - val_acc: 0.7957
Epoch 17/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.4304 - acc: 0.8433 - val_loss: 0.5195 - val_acc: 0.8003
Epoch 18/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.3997 - acc: 0.8531 - val_loss: 0.5027 - val_acc: 0.8036
Epoch 19/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.3671 - acc: 0.8696 - val_loss: 0.4855 - val_acc: 0.8146
Epoch 20/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.3451 - acc: 0.8781 - val_loss: 0.4756 - val_acc: 0.8156
Epoch 21/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.3175 - acc: 0.8900 - val_loss: 0.4664 - val_acc: 0.8232
Epoch 22/150
15664/15664 [==============================] - 1s 54us/step - loss: 0.2876 - acc: 0.9014 - val_loss: 0.4533 - val_acc: 0.8258
Epoch 23/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.2702 - acc: 0.9090 - val_loss: 0.4500 - val_acc: 0.8281
Epoch 24/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.2497 - acc: 0.9145 - val_loss: 0.4383 - val_acc: 0.8345
Epoch 25/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.2297 - acc: 0.9227 - val_loss: 0.4338 - val_acc: 0.8355
Epoch 26/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.2123 - acc: 0.9306 - val_loss: 0.4327 - val_acc: 0.8398
Epoch 27/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.1920 - acc: 0.9385 - val_loss: 0.4232 - val_acc: 0.8444
Epoch 28/150
15664/15664 [==============================] - 1s 56us/step - loss: 0.1789 - acc: 0.9447 - val_loss: 0.4201 - val_acc: 0.8457
Epoch 29/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.1637 - acc: 0.9484 - val_loss: 0.4171 - val_acc: 0.8480
Epoch 30/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.1540 - acc: 0.9510 - val_loss: 0.4192 - val_acc: 0.8452
Epoch 31/150
15664/15664 [==============================] - 1s 56us/step - loss: 0.1408 - acc: 0.9567 - val_loss: 0.4155 - val_acc: 0.8470
Epoch 32/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.1311 - acc: 0.9606 - val_loss: 0.4109 - val_acc: 0.8516
Epoch 33/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.1235 - acc: 0.9623 - val_loss: 0.4145 - val_acc: 0.8473
Epoch 34/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.1119 - acc: 0.9674 - val_loss: 0.4101 - val_acc: 0.8536
Epoch 35/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.1029 - acc: 0.9706 - val_loss: 0.4100 - val_acc: 0.8534
Epoch 36/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.0987 - acc: 0.9714 - val_loss: 0.4118 - val_acc: 0.8529
Epoch 37/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.0899 - acc: 0.9744 - val_loss: 0.4141 - val_acc: 0.8549
Epoch 38/150
15664/15664 [==============================] - 1s 55us/step - loss: 0.0830 - acc: 0.9770 - val_loss: 0.4142 - val_acc: 0.8552
15664/15664 [==============================] - 0s 28us/step
====== Training loss, score are: 0.046418898252806456 0.9911261491317671 =======
3915/3915 [==============================] - 0s 28us/step
===== CV loss, score are: 0.4142267006224599 0.8551724138692268 =======


===== Model: cnn_gensim  ========:
 Cross-val log losses are: [0.41617487087517757, 0.43917219240889271, 0.42064044705858999, 0.41159092715723844, 0.41786453428926779]
====== Mean cross-val log loss is: 0.4210885943578333 =========


Timestamp: 2018-Jan-14 03:07:31
Running kfold training with model cnn_glove
Shapes: x_train_raw.shape (19579, 33), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 32)
Running preprocess: &lt;function run_NN_preprocess at 0x7f620ee8eea0&gt;
No of unique words found is 247346
No of words occuring more than 2 times is 75104
No of unique words found is after capping is 247346
max sentence length is 1439
mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000
After padding: max sentence size is 161
After padding: mean, median length of sentence is 161.000000 161.000000
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>5053it [00:00, 50520.41it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Creating word embedding matrix for glove vectors
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>400000it [00:08, 49567.78it/s]
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Found 400000 word vectors.
of--mirth 247345
None
(247346, 100)
Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)
Running fold 1
Creating CNN model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_29 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
conv1d_11 (Conv1D)           (None, 158, 32)           12832     
_________________________________________________________________
max_pooling1d_11 (MaxPooling (None, 79, 32)            0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_29  (None, 32)                0         
_________________________________________________________________
dense_29 (Dense)             (None, 3)                 99        
=================================================================
Total params: 24,747,531
Trainable params: 12,931
Non-trainable params: 24,734,600
_________________________________________________________________
15663/15663 [==============================] - 1s 68us/step
Before training loss, score are: 1.1105597337026107 0.2910042776102034
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 84us/step - loss: 1.0482 - acc: 0.4597 - val_loss: 0.9916 - val_acc: 0.5444
Epoch 2/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.9595 - acc: 0.5519 - val_loss: 0.9194 - val_acc: 0.5822
Epoch 3/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.9139 - acc: 0.5826 - val_loss: 0.8771 - val_acc: 0.6267
Epoch 4/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8864 - acc: 0.5991 - val_loss: 0.8543 - val_acc: 0.6417
Epoch 5/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8616 - acc: 0.6165 - val_loss: 0.8289 - val_acc: 0.6532
Epoch 6/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8449 - acc: 0.6237 - val_loss: 0.8141 - val_acc: 0.6629
Epoch 7/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8329 - acc: 0.6311 - val_loss: 0.8029 - val_acc: 0.6670
Epoch 8/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8271 - acc: 0.6356 - val_loss: 0.7959 - val_acc: 0.6657
Epoch 9/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8135 - acc: 0.6437 - val_loss: 0.7851 - val_acc: 0.6698
Epoch 10/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8068 - acc: 0.6450 - val_loss: 0.7763 - val_acc: 0.6775
Epoch 11/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8029 - acc: 0.6480 - val_loss: 0.7728 - val_acc: 0.6757
Epoch 12/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7925 - acc: 0.6508 - val_loss: 0.7700 - val_acc: 0.6747
Epoch 13/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7906 - acc: 0.6576 - val_loss: 0.7651 - val_acc: 0.6772
Epoch 14/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7841 - acc: 0.6581 - val_loss: 0.7667 - val_acc: 0.6713
Epoch 15/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7810 - acc: 0.6598 - val_loss: 0.7530 - val_acc: 0.6803
Epoch 16/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7756 - acc: 0.6614 - val_loss: 0.7508 - val_acc: 0.6826
Epoch 17/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7724 - acc: 0.6624 - val_loss: 0.7479 - val_acc: 0.6874
Epoch 18/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7724 - acc: 0.6621 - val_loss: 0.7457 - val_acc: 0.6844
Epoch 19/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7681 - acc: 0.6616 - val_loss: 0.7417 - val_acc: 0.6851
Epoch 20/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7655 - acc: 0.6665 - val_loss: 0.7385 - val_acc: 0.6859
Epoch 21/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7593 - acc: 0.6738 - val_loss: 0.7356 - val_acc: 0.6862
Epoch 22/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7554 - acc: 0.6715 - val_loss: 0.7342 - val_acc: 0.6872
Epoch 23/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7550 - acc: 0.6733 - val_loss: 0.7308 - val_acc: 0.6900
Epoch 24/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7500 - acc: 0.6759 - val_loss: 0.7252 - val_acc: 0.6879
Epoch 25/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7449 - acc: 0.6766 - val_loss: 0.7237 - val_acc: 0.6928
Epoch 26/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7449 - acc: 0.6790 - val_loss: 0.7219 - val_acc: 0.6933
Epoch 27/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7460 - acc: 0.6768 - val_loss: 0.7249 - val_acc: 0.6902
Epoch 28/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7441 - acc: 0.6782 - val_loss: 0.7179 - val_acc: 0.6948
Epoch 29/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7403 - acc: 0.6788 - val_loss: 0.7149 - val_acc: 0.6982
Epoch 30/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7416 - acc: 0.6817 - val_loss: 0.7172 - val_acc: 0.6951
Epoch 31/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7385 - acc: 0.6805 - val_loss: 0.7170 - val_acc: 0.6931
Epoch 32/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7380 - acc: 0.6815 - val_loss: 0.7132 - val_acc: 0.6977
Epoch 33/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7328 - acc: 0.6827 - val_loss: 0.7079 - val_acc: 0.7002
Epoch 34/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7276 - acc: 0.6873 - val_loss: 0.7121 - val_acc: 0.6954
Epoch 35/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7321 - acc: 0.6826 - val_loss: 0.7084 - val_acc: 0.7010
Epoch 36/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7295 - acc: 0.6836 - val_loss: 0.7077 - val_acc: 0.7020
Epoch 37/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7304 - acc: 0.6812 - val_loss: 0.7093 - val_acc: 0.7053
Epoch 38/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7243 - acc: 0.6842 - val_loss: 0.7098 - val_acc: 0.7025
Epoch 39/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7263 - acc: 0.6849 - val_loss: 0.7028 - val_acc: 0.7081
Epoch 40/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7196 - acc: 0.6919 - val_loss: 0.7001 - val_acc: 0.7040
Epoch 41/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7221 - acc: 0.6898 - val_loss: 0.7020 - val_acc: 0.7035
Epoch 42/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7221 - acc: 0.6905 - val_loss: 0.7008 - val_acc: 0.7045
Epoch 43/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7178 - acc: 0.6898 - val_loss: 0.6955 - val_acc: 0.7053
Epoch 44/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7173 - acc: 0.6895 - val_loss: 0.6979 - val_acc: 0.7028
Epoch 45/150
15663/15663 [==============================] - 0s 24us/step - loss: 0.7179 - acc: 0.6883 - val_loss: 0.6975 - val_acc: 0.7076
Epoch 46/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7110 - acc: 0.6958 - val_loss: 0.6928 - val_acc: 0.7063
Epoch 47/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7133 - acc: 0.6937 - val_loss: 0.6904 - val_acc: 0.7074
Epoch 48/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7128 - acc: 0.6948 - val_loss: 0.6951 - val_acc: 0.7084
Epoch 49/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7133 - acc: 0.6915 - val_loss: 0.6897 - val_acc: 0.7127
Epoch 50/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7124 - acc: 0.6917 - val_loss: 0.6925 - val_acc: 0.7089
Epoch 51/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7115 - acc: 0.6961 - val_loss: 0.6917 - val_acc: 0.7094
Epoch 52/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7076 - acc: 0.6918 - val_loss: 0.7023 - val_acc: 0.7045
15663/15663 [==============================] - 0s 27us/step
====== Training loss, score are: 0.6544604023160632 0.7287237438701666 =======
3916/3916 [==============================] - 0s 41us/step
===== CV loss, score are: 0.7023483652990612 0.7045454545454546 =======
Running fold 2
Creating CNN model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_30 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
conv1d_12 (Conv1D)           (None, 158, 32)           12832     
_________________________________________________________________
max_pooling1d_12 (MaxPooling (None, 79, 32)            0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_30  (None, 32)                0         
_________________________________________________________________
dense_30 (Dense)             (None, 3)                 99        
=================================================================
Total params: 24,747,531
Trainable params: 12,931
Non-trainable params: 24,734,600
_________________________________________________________________
15663/15663 [==============================] - 1s 60us/step
Before training loss, score are: 1.1185543177221147 0.2866628359857437
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 61us/step - loss: 1.0494 - acc: 0.4610 - val_loss: 0.9967 - val_acc: 0.5033
Epoch 2/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.9605 - acc: 0.5561 - val_loss: 0.9207 - val_acc: 0.5914
Epoch 3/150
15663/15663 [==============================] - 0s 27us/step - loss: 0.9091 - acc: 0.5893 - val_loss: 0.8775 - val_acc: 0.6193
Epoch 4/150
15663/15663 [==============================] - 0s 28us/step - loss: 0.8788 - acc: 0.6067 - val_loss: 0.8553 - val_acc: 0.6310
Epoch 5/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8596 - acc: 0.6164 - val_loss: 0.8407 - val_acc: 0.6366
Epoch 6/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8393 - acc: 0.6278 - val_loss: 0.8184 - val_acc: 0.6461
Epoch 7/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8298 - acc: 0.6297 - val_loss: 0.8204 - val_acc: 0.6491
Epoch 8/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.8183 - acc: 0.6363 - val_loss: 0.7971 - val_acc: 0.6596
Epoch 9/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8099 - acc: 0.6430 - val_loss: 0.7919 - val_acc: 0.6555
Epoch 10/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8022 - acc: 0.6443 - val_loss: 0.7827 - val_acc: 0.6596
Epoch 11/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7956 - acc: 0.6521 - val_loss: 0.7844 - val_acc: 0.6583
Epoch 12/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7886 - acc: 0.6510 - val_loss: 0.7722 - val_acc: 0.6680
Epoch 13/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7883 - acc: 0.6577 - val_loss: 0.7653 - val_acc: 0.6749
Epoch 14/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7818 - acc: 0.6604 - val_loss: 0.7667 - val_acc: 0.6719
Epoch 15/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7758 - acc: 0.6602 - val_loss: 0.7591 - val_acc: 0.6752
Epoch 16/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7765 - acc: 0.6636 - val_loss: 0.7539 - val_acc: 0.6808
Epoch 17/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7708 - acc: 0.6638 - val_loss: 0.7537 - val_acc: 0.6742
Epoch 18/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7690 - acc: 0.6612 - val_loss: 0.7519 - val_acc: 0.6826
Epoch 19/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7666 - acc: 0.6640 - val_loss: 0.7429 - val_acc: 0.6882
Epoch 20/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7646 - acc: 0.6666 - val_loss: 0.7452 - val_acc: 0.6800
Epoch 21/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7550 - acc: 0.6695 - val_loss: 0.7386 - val_acc: 0.6849
Epoch 22/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7609 - acc: 0.6718 - val_loss: 0.7352 - val_acc: 0.6877
Epoch 23/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7499 - acc: 0.6733 - val_loss: 0.7328 - val_acc: 0.6864
Epoch 24/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7516 - acc: 0.6757 - val_loss: 0.7297 - val_acc: 0.6956
Epoch 25/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7480 - acc: 0.6759 - val_loss: 0.7268 - val_acc: 0.6918
Epoch 26/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7486 - acc: 0.6754 - val_loss: 0.7259 - val_acc: 0.6925
Epoch 27/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7436 - acc: 0.6821 - val_loss: 0.7223 - val_acc: 0.6951
Epoch 28/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7414 - acc: 0.6776 - val_loss: 0.7213 - val_acc: 0.6966
Epoch 29/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7430 - acc: 0.6769 - val_loss: 0.7254 - val_acc: 0.6946
Epoch 30/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7411 - acc: 0.6752 - val_loss: 0.7242 - val_acc: 0.6959
Epoch 31/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7318 - acc: 0.6863 - val_loss: 0.7229 - val_acc: 0.6900
15663/15663 [==============================] - 0s 27us/step
====== Training loss, score are: 0.6886357561173372 0.7111025985033281 =======
3916/3916 [==============================] - 0s 29us/step
===== CV loss, score are: 0.7228803083650669 0.6899897855562866 =======
Running fold 3
Creating CNN model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_31 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
conv1d_13 (Conv1D)           (None, 158, 32)           12832     
_________________________________________________________________
max_pooling1d_13 (MaxPooling (None, 79, 32)            0         
_________________________________________________________________
dropout_13 (Dropout)         (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_31  (None, 32)                0         
_________________________________________________________________
dense_31 (Dense)             (None, 3)                 99        
=================================================================
Total params: 24,747,531
Trainable params: 12,931
Non-trainable params: 24,734,600
_________________________________________________________________
15663/15663 [==============================] - 1s 63us/step
Before training loss, score are: 1.1242099555829685 0.28768435166343026
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 64us/step - loss: 1.0430 - acc: 0.4710 - val_loss: 0.9837 - val_acc: 0.5197
Epoch 2/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.9544 - acc: 0.5609 - val_loss: 0.9116 - val_acc: 0.6062
Epoch 3/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.9069 - acc: 0.5918 - val_loss: 0.8720 - val_acc: 0.6147
Epoch 4/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8751 - acc: 0.6112 - val_loss: 0.8502 - val_acc: 0.6279
Epoch 5/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8557 - acc: 0.6189 - val_loss: 0.8297 - val_acc: 0.6435
Epoch 6/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8382 - acc: 0.6316 - val_loss: 0.8157 - val_acc: 0.6448
Epoch 7/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8256 - acc: 0.6391 - val_loss: 0.8032 - val_acc: 0.6504
Epoch 8/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8153 - acc: 0.6411 - val_loss: 0.7987 - val_acc: 0.6525
Epoch 9/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8100 - acc: 0.6450 - val_loss: 0.7895 - val_acc: 0.6565
Epoch 10/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8027 - acc: 0.6466 - val_loss: 0.7892 - val_acc: 0.6581
Epoch 11/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7984 - acc: 0.6531 - val_loss: 0.7760 - val_acc: 0.6678
Epoch 12/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7876 - acc: 0.6574 - val_loss: 0.7750 - val_acc: 0.6568
Epoch 13/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7846 - acc: 0.6543 - val_loss: 0.7685 - val_acc: 0.6719
Epoch 14/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7810 - acc: 0.6589 - val_loss: 0.7616 - val_acc: 0.6729
Epoch 15/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7762 - acc: 0.6621 - val_loss: 0.7581 - val_acc: 0.6754
Epoch 16/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7746 - acc: 0.6633 - val_loss: 0.7544 - val_acc: 0.6754
Epoch 17/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7725 - acc: 0.6598 - val_loss: 0.7506 - val_acc: 0.6770
Epoch 18/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7641 - acc: 0.6686 - val_loss: 0.7546 - val_acc: 0.6701
Epoch 19/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7597 - acc: 0.6706 - val_loss: 0.7498 - val_acc: 0.6752
Epoch 20/150
15663/15663 [==============================] - 0s 24us/step - loss: 0.7558 - acc: 0.6724 - val_loss: 0.7432 - val_acc: 0.6785
Epoch 21/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7585 - acc: 0.6698 - val_loss: 0.7477 - val_acc: 0.6736
Epoch 22/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7548 - acc: 0.6751 - val_loss: 0.7418 - val_acc: 0.6803
Epoch 23/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7486 - acc: 0.6801 - val_loss: 0.7340 - val_acc: 0.6874
Epoch 24/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7452 - acc: 0.6771 - val_loss: 0.7323 - val_acc: 0.6849
Epoch 25/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7462 - acc: 0.6785 - val_loss: 0.7328 - val_acc: 0.6862
Epoch 26/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7427 - acc: 0.6808 - val_loss: 0.7280 - val_acc: 0.6885
Epoch 27/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7341 - acc: 0.6835 - val_loss: 0.7290 - val_acc: 0.6864
Epoch 28/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7366 - acc: 0.6852 - val_loss: 0.7288 - val_acc: 0.6943
Epoch 29/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7355 - acc: 0.6835 - val_loss: 0.7214 - val_acc: 0.6923
Epoch 30/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7414 - acc: 0.6798 - val_loss: 0.7219 - val_acc: 0.6948
Epoch 31/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7360 - acc: 0.6824 - val_loss: 0.7178 - val_acc: 0.6931
Epoch 32/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7354 - acc: 0.6810 - val_loss: 0.7257 - val_acc: 0.6834
Epoch 33/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7318 - acc: 0.6854 - val_loss: 0.7157 - val_acc: 0.6987
Epoch 34/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7248 - acc: 0.6854 - val_loss: 0.7145 - val_acc: 0.6928
Epoch 35/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7283 - acc: 0.6884 - val_loss: 0.7158 - val_acc: 0.6997
Epoch 36/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7308 - acc: 0.6893 - val_loss: 0.7107 - val_acc: 0.6974
Epoch 37/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7233 - acc: 0.6870 - val_loss: 0.7087 - val_acc: 0.6994
Epoch 38/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7228 - acc: 0.6918 - val_loss: 0.7078 - val_acc: 0.7017
Epoch 39/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7182 - acc: 0.6928 - val_loss: 0.7061 - val_acc: 0.7012
Epoch 40/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7258 - acc: 0.6892 - val_loss: 0.7041 - val_acc: 0.7028
Epoch 41/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7220 - acc: 0.6919 - val_loss: 0.7050 - val_acc: 0.6992
Epoch 42/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7219 - acc: 0.6930 - val_loss: 0.7017 - val_acc: 0.7025
Epoch 43/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7170 - acc: 0.6927 - val_loss: 0.7056 - val_acc: 0.6987
Epoch 44/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7149 - acc: 0.6911 - val_loss: 0.7029 - val_acc: 0.7015
Epoch 45/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7082 - acc: 0.6963 - val_loss: 0.6983 - val_acc: 0.7081
Epoch 46/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7125 - acc: 0.6922 - val_loss: 0.6982 - val_acc: 0.6999
Epoch 47/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7167 - acc: 0.6953 - val_loss: 0.7016 - val_acc: 0.6989
Epoch 48/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7102 - acc: 0.6955 - val_loss: 0.6958 - val_acc: 0.7030
Epoch 49/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7035 - acc: 0.7004 - val_loss: 0.7012 - val_acc: 0.6997
Epoch 50/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7077 - acc: 0.6971 - val_loss: 0.6963 - val_acc: 0.7005
Epoch 51/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7055 - acc: 0.6936 - val_loss: 0.6937 - val_acc: 0.7058
Epoch 52/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6967 - acc: 0.7041 - val_loss: 0.6930 - val_acc: 0.7109
Epoch 53/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6987 - acc: 0.7051 - val_loss: 0.6906 - val_acc: 0.7017
Epoch 54/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7018 - acc: 0.6986 - val_loss: 0.6900 - val_acc: 0.7071
Epoch 55/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6988 - acc: 0.7008 - val_loss: 0.6896 - val_acc: 0.7061
Epoch 56/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6991 - acc: 0.7053 - val_loss: 0.6936 - val_acc: 0.7015
Epoch 57/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6996 - acc: 0.7049 - val_loss: 0.6873 - val_acc: 0.7045
Epoch 58/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6972 - acc: 0.7009 - val_loss: 0.6860 - val_acc: 0.7061
Epoch 59/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6985 - acc: 0.7053 - val_loss: 0.6841 - val_acc: 0.7132
Epoch 60/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6976 - acc: 0.7032 - val_loss: 0.6907 - val_acc: 0.6994
Epoch 61/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6942 - acc: 0.7076 - val_loss: 0.6825 - val_acc: 0.7068
Epoch 62/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6932 - acc: 0.7055 - val_loss: 0.6836 - val_acc: 0.7099
Epoch 63/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6968 - acc: 0.6968 - val_loss: 0.6820 - val_acc: 0.7071
Epoch 64/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6971 - acc: 0.7031 - val_loss: 0.6808 - val_acc: 0.7063
Epoch 65/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6911 - acc: 0.7050 - val_loss: 0.6805 - val_acc: 0.7091
Epoch 66/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6939 - acc: 0.7020 - val_loss: 0.6781 - val_acc: 0.7104
Epoch 67/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6842 - acc: 0.7089 - val_loss: 0.6801 - val_acc: 0.7071
Epoch 68/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6859 - acc: 0.7053 - val_loss: 0.6795 - val_acc: 0.7153
Epoch 69/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6897 - acc: 0.7087 - val_loss: 0.6782 - val_acc: 0.7127
15663/15663 [==============================] - 0s 28us/step
====== Training loss, score are: 0.6160765802396757 0.7513247781547864 =======
3916/3916 [==============================] - 0s 28us/step
===== CV loss, score are: 0.6781785657992767 0.7127170581922346 =======
Running fold 4
Creating CNN model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_32 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
conv1d_14 (Conv1D)           (None, 158, 32)           12832     
_________________________________________________________________
max_pooling1d_14 (MaxPooling (None, 79, 32)            0         
_________________________________________________________________
dropout_14 (Dropout)         (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_32  (None, 32)                0         
_________________________________________________________________
dense_32 (Dense)             (None, 3)                 99        
=================================================================
Total params: 24,747,531
Trainable params: 12,931
Non-trainable params: 24,734,600
_________________________________________________________________
15663/15663 [==============================] - 1s 62us/step
Before training loss, score are: 1.1296464027759026 0.28934431462968163
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 63us/step - loss: 1.0543 - acc: 0.4539 - val_loss: 0.9894 - val_acc: 0.5735
Epoch 2/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.9602 - acc: 0.5593 - val_loss: 0.9125 - val_acc: 0.6065
Epoch 3/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.9113 - acc: 0.5860 - val_loss: 0.8711 - val_acc: 0.6256
Epoch 4/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8774 - acc: 0.6049 - val_loss: 0.8465 - val_acc: 0.6336
Epoch 5/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8580 - acc: 0.6167 - val_loss: 0.8175 - val_acc: 0.6563
Epoch 6/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8480 - acc: 0.6245 - val_loss: 0.8053 - val_acc: 0.6673
Epoch 7/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8320 - acc: 0.6358 - val_loss: 0.7944 - val_acc: 0.6637
Epoch 8/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8241 - acc: 0.6352 - val_loss: 0.7907 - val_acc: 0.6652
Epoch 9/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8132 - acc: 0.6414 - val_loss: 0.7760 - val_acc: 0.6701
Epoch 10/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8044 - acc: 0.6464 - val_loss: 0.7739 - val_acc: 0.6706
Epoch 11/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.8010 - acc: 0.6521 - val_loss: 0.7673 - val_acc: 0.6772
Epoch 12/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7980 - acc: 0.6474 - val_loss: 0.7566 - val_acc: 0.6818
Epoch 13/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7927 - acc: 0.6512 - val_loss: 0.7504 - val_acc: 0.6859
Epoch 14/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7863 - acc: 0.6577 - val_loss: 0.7446 - val_acc: 0.6859
Epoch 15/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7818 - acc: 0.6618 - val_loss: 0.7461 - val_acc: 0.6892
Epoch 16/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7765 - acc: 0.6636 - val_loss: 0.7361 - val_acc: 0.6941
Epoch 17/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7763 - acc: 0.6628 - val_loss: 0.7415 - val_acc: 0.6879
Epoch 18/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7719 - acc: 0.6630 - val_loss: 0.7316 - val_acc: 0.6961
Epoch 19/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7658 - acc: 0.6692 - val_loss: 0.7276 - val_acc: 0.6951
Epoch 20/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7695 - acc: 0.6636 - val_loss: 0.7224 - val_acc: 0.6961
Epoch 21/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7622 - acc: 0.6706 - val_loss: 0.7223 - val_acc: 0.7015
Epoch 22/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7658 - acc: 0.6689 - val_loss: 0.7188 - val_acc: 0.7025
Epoch 23/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7545 - acc: 0.6739 - val_loss: 0.7149 - val_acc: 0.6992
Epoch 24/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7536 - acc: 0.6740 - val_loss: 0.7132 - val_acc: 0.6992
Epoch 25/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7577 - acc: 0.6734 - val_loss: 0.7112 - val_acc: 0.7040
Epoch 26/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7500 - acc: 0.6718 - val_loss: 0.7048 - val_acc: 0.7056
Epoch 27/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7496 - acc: 0.6746 - val_loss: 0.7028 - val_acc: 0.7086
Epoch 28/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7455 - acc: 0.6772 - val_loss: 0.7031 - val_acc: 0.7109
Epoch 29/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7407 - acc: 0.6825 - val_loss: 0.6988 - val_acc: 0.7094
Epoch 30/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7445 - acc: 0.6763 - val_loss: 0.6974 - val_acc: 0.7097
Epoch 31/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7432 - acc: 0.6769 - val_loss: 0.7053 - val_acc: 0.7071
Epoch 32/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7393 - acc: 0.6822 - val_loss: 0.6991 - val_acc: 0.7089
Epoch 33/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7370 - acc: 0.6805 - val_loss: 0.6919 - val_acc: 0.7135
Epoch 34/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7343 - acc: 0.6834 - val_loss: 0.6924 - val_acc: 0.7122
Epoch 35/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7343 - acc: 0.6819 - val_loss: 0.6905 - val_acc: 0.7160
Epoch 36/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7307 - acc: 0.6862 - val_loss: 0.6973 - val_acc: 0.7104
Epoch 37/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7271 - acc: 0.6882 - val_loss: 0.6869 - val_acc: 0.7153
Epoch 38/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7234 - acc: 0.6895 - val_loss: 0.6836 - val_acc: 0.7186
Epoch 39/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7300 - acc: 0.6895 - val_loss: 0.6896 - val_acc: 0.7158
Epoch 40/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7258 - acc: 0.6900 - val_loss: 0.6831 - val_acc: 0.7183
Epoch 41/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7238 - acc: 0.6900 - val_loss: 0.6819 - val_acc: 0.7176
Epoch 42/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7181 - acc: 0.6907 - val_loss: 0.6758 - val_acc: 0.7188
Epoch 43/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7196 - acc: 0.6911 - val_loss: 0.6770 - val_acc: 0.7201
Epoch 44/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7191 - acc: 0.6893 - val_loss: 0.6767 - val_acc: 0.7229
Epoch 45/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7205 - acc: 0.6921 - val_loss: 0.6754 - val_acc: 0.7245
Epoch 46/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7167 - acc: 0.6888 - val_loss: 0.6772 - val_acc: 0.7181
Epoch 47/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7122 - acc: 0.6958 - val_loss: 0.6735 - val_acc: 0.7273
Epoch 48/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7138 - acc: 0.6928 - val_loss: 0.6715 - val_acc: 0.7224
Epoch 49/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7125 - acc: 0.6928 - val_loss: 0.6687 - val_acc: 0.7237
Epoch 50/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7123 - acc: 0.6949 - val_loss: 0.6665 - val_acc: 0.7252
Epoch 51/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7119 - acc: 0.6919 - val_loss: 0.6686 - val_acc: 0.7285
Epoch 52/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.7130 - acc: 0.6955 - val_loss: 0.6665 - val_acc: 0.7319
Epoch 53/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7078 - acc: 0.6992 - val_loss: 0.6643 - val_acc: 0.7296
Epoch 54/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7115 - acc: 0.6935 - val_loss: 0.6634 - val_acc: 0.7270
Epoch 55/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7071 - acc: 0.6995 - val_loss: 0.6608 - val_acc: 0.7308
Epoch 56/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7034 - acc: 0.6950 - val_loss: 0.6619 - val_acc: 0.7314
Epoch 57/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7055 - acc: 0.6983 - val_loss: 0.6579 - val_acc: 0.7314
Epoch 58/150
15663/15663 [==============================] - 0s 26us/step - loss: 0.6996 - acc: 0.7015 - val_loss: 0.6583 - val_acc: 0.7326
Epoch 59/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7040 - acc: 0.6965 - val_loss: 0.6564 - val_acc: 0.7303
Epoch 60/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7036 - acc: 0.6980 - val_loss: 0.6574 - val_acc: 0.7357
Epoch 61/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.6998 - acc: 0.7012 - val_loss: 0.6573 - val_acc: 0.7296
Epoch 62/150
15663/15663 [==============================] - 0s 25us/step - loss: 0.7021 - acc: 0.6967 - val_loss: 0.6613 - val_acc: 0.7303
15663/15663 [==============================] - 0s 28us/step
====== Training loss, score are: 0.6350062289495528 0.7438549447933361 =======
3916/3916 [==============================] - 0s 29us/step
===== CV loss, score are: 0.6612772687706445 0.7303370785908022 =======
Running fold 5
Creating CNN model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_33 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
conv1d_15 (Conv1D)           (None, 158, 32)           12832     
_________________________________________________________________
max_pooling1d_15 (MaxPooling (None, 79, 32)            0         
_________________________________________________________________
dropout_15 (Dropout)         (None, 79, 32)            0         
_________________________________________________________________
global_average_pooling1d_33  (None, 32)                0         
_________________________________________________________________
dense_33 (Dense)             (None, 3)                 99        
=================================================================
Total params: 24,747,531
Trainable params: 12,931
Non-trainable params: 24,734,600
_________________________________________________________________
15664/15664 [==============================] - 1s 68us/step
Before training loss, score are: 1.1106298339987921 0.31115934627170583
Train on 15664 samples, validate on 3915 samples
Epoch 1/150
15664/15664 [==============================] - 1s 83us/step - loss: 1.0462 - acc: 0.4755 - val_loss: 0.9780 - val_acc: 0.5709
Epoch 2/150
15664/15664 [==============================] - 0s 27us/step - loss: 0.9554 - acc: 0.5568 - val_loss: 0.9026 - val_acc: 0.6146
Epoch 3/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.9077 - acc: 0.5877 - val_loss: 0.8619 - val_acc: 0.6276
Epoch 4/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.8772 - acc: 0.6068 - val_loss: 0.8354 - val_acc: 0.6524
Epoch 5/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.8568 - acc: 0.6212 - val_loss: 0.8180 - val_acc: 0.6572
Epoch 6/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.8420 - acc: 0.6256 - val_loss: 0.8022 - val_acc: 0.6582
Epoch 7/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.8311 - acc: 0.6328 - val_loss: 0.7981 - val_acc: 0.6720
Epoch 8/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.8195 - acc: 0.6360 - val_loss: 0.7817 - val_acc: 0.6705
Epoch 9/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.8130 - acc: 0.6401 - val_loss: 0.7781 - val_acc: 0.6802
Epoch 10/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.8045 - acc: 0.6503 - val_loss: 0.7865 - val_acc: 0.6779
Epoch 11/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.8011 - acc: 0.6502 - val_loss: 0.7761 - val_acc: 0.6799
Epoch 12/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7931 - acc: 0.6500 - val_loss: 0.7676 - val_acc: 0.6861
Epoch 13/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7860 - acc: 0.6563 - val_loss: 0.7566 - val_acc: 0.6899
Epoch 14/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7869 - acc: 0.6526 - val_loss: 0.7548 - val_acc: 0.6932
Epoch 15/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7827 - acc: 0.6559 - val_loss: 0.7483 - val_acc: 0.6927
Epoch 16/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7762 - acc: 0.6604 - val_loss: 0.7538 - val_acc: 0.6940
Epoch 17/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7710 - acc: 0.6626 - val_loss: 0.7381 - val_acc: 0.6948
Epoch 18/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7715 - acc: 0.6604 - val_loss: 0.7372 - val_acc: 0.6966
Epoch 19/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7668 - acc: 0.6652 - val_loss: 0.7332 - val_acc: 0.6991
Epoch 20/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7609 - acc: 0.6662 - val_loss: 0.7324 - val_acc: 0.7006
Epoch 21/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7592 - acc: 0.6689 - val_loss: 0.7319 - val_acc: 0.7017
Epoch 22/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7619 - acc: 0.6672 - val_loss: 0.7289 - val_acc: 0.7047
Epoch 23/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7521 - acc: 0.6734 - val_loss: 0.7257 - val_acc: 0.7029
Epoch 24/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7511 - acc: 0.6729 - val_loss: 0.7210 - val_acc: 0.7042
Epoch 25/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7455 - acc: 0.6755 - val_loss: 0.7193 - val_acc: 0.7060
Epoch 26/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7471 - acc: 0.6766 - val_loss: 0.7170 - val_acc: 0.7022
Epoch 27/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7469 - acc: 0.6769 - val_loss: 0.7175 - val_acc: 0.7088
Epoch 28/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7377 - acc: 0.6786 - val_loss: 0.7142 - val_acc: 0.7068
Epoch 29/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7420 - acc: 0.6811 - val_loss: 0.7114 - val_acc: 0.7022
Epoch 30/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7363 - acc: 0.6828 - val_loss: 0.7099 - val_acc: 0.7093
Epoch 31/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7335 - acc: 0.6788 - val_loss: 0.7165 - val_acc: 0.7065
Epoch 32/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7350 - acc: 0.6842 - val_loss: 0.7105 - val_acc: 0.7114
Epoch 33/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7313 - acc: 0.6814 - val_loss: 0.7040 - val_acc: 0.7093
Epoch 34/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7263 - acc: 0.6853 - val_loss: 0.7086 - val_acc: 0.7040
Epoch 35/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7268 - acc: 0.6881 - val_loss: 0.6978 - val_acc: 0.7119
Epoch 36/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7282 - acc: 0.6876 - val_loss: 0.7009 - val_acc: 0.7093
Epoch 37/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7259 - acc: 0.6916 - val_loss: 0.6982 - val_acc: 0.7114
Epoch 38/150
15664/15664 [==============================] - 0s 25us/step - loss: 0.7204 - acc: 0.6915 - val_loss: 0.7007 - val_acc: 0.7109
15664/15664 [==============================] - 0s 27us/step
====== Training loss, score are: 0.6713878553174246 0.7183988764044944 =======
3915/3915 [==============================] - 0s 46us/step
===== CV loss, score are: 0.7007125627339876 0.7108556832542516 =======


===== Model: cnn_glove  ========:
 Cross-val log losses are: [0.70234836755586638, 0.72288030508889789, 0.67817856567603885, 0.66127726744962534, 0.70071256569482632]
====== Mean cross-val log loss is: 0.6930794142930509 =========


</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Run-FastText-at-char-level:">Run FastText at char level:<a class="anchor-link" href="#Run-FastText-at-char-level:">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[53]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Run Fast text (char level) and add predictions as features:</span>

<span class="c1"># No pre-trained vectors:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Timestamp: {:%Y-%b-</span><span class="si">%d</span><span class="s1"> %H:%M:%S}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()))</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;fast_text_char_none&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_NN_preprocess</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_Fasttext</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;temp&#39;</span><span class="p">:</span> <span class="mi">0</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;char_level&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;word_vector_type&#39;</span><span class="p">:</span> <span class="kc">None</span> <span class="p">}</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;NN&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>

<span class="c1"># Pre-trained word2vec using vectors:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Timestamp: {:%Y-%b-</span><span class="si">%d</span><span class="s1"> %H:%M:%S}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()))</span>
<span class="n">wordvecsize</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">create_gensim_wordvectors</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">if</span> <span class="n">create_gensim_wordvectors</span><span class="p">:</span>
    <span class="n">create_gensim_wordvec</span><span class="p">(</span><span class="n">train_raw</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">wordvecsize</span><span class="p">)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;fast_text_char_gensim&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_NN_preprocess</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_Fasttext</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;temp&#39;</span><span class="p">:</span> <span class="mi">0</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;char_level&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;word_vector_type&#39;</span><span class="p">:</span> <span class="s2">&quot;gensim&quot;</span> <span class="p">}</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;NN&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
    
<span class="c1"># Glove vectors:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Timestamp: {:%Y-%b-</span><span class="si">%d</span><span class="s1"> %H:%M:%S}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()))</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;fast_text_char_glove&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_NN_preprocess</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_Fasttext</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;temp&#39;</span><span class="p">:</span> <span class="mi">0</span> <span class="p">}</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;char_level&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;word_vector_type&#39;</span><span class="p">:</span> <span class="s2">&quot;glove&quot;</span> <span class="p">}</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;NN&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_one_hot</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span> <span class="o">=</span> <span class="n">add_pred_features</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                        <span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> 
                                        <span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Timestamp: 2018-Jan-14 03:09:47
Running kfold training with model fast_text_char_none
Shapes: x_train_raw.shape (19579, 36), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 35)
Running preprocess: &lt;function run_NN_preprocess at 0x7f620ee8eea0&gt;
No of unique words found is 247346
No of words occuring more than 2 times is 75104
No of unique words found is after capping is 247346
max sentence length is 1439
mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000
After padding: max sentence size is 161
After padding: mean, median length of sentence is 161.000000 161.000000
Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)
Running fold 1
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_34 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_34  (None, 20)                0         
_________________________________________________________________
dense_34 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 60us/step
Before training loss, score are: 1.0967000038818802 0.40158334932115386
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 86us/step - loss: 1.0839 - acc: 0.4067 - val_loss: 1.0800 - val_acc: 0.3910
Epoch 2/150
15663/15663 [==============================] - 1s 48us/step - loss: 1.0612 - acc: 0.4115 - val_loss: 1.0540 - val_acc: 0.4047
Epoch 3/150
15663/15663 [==============================] - 1s 48us/step - loss: 1.0187 - acc: 0.4510 - val_loss: 1.0073 - val_acc: 0.4676
Epoch 4/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.9545 - acc: 0.5750 - val_loss: 0.9459 - val_acc: 0.5664
Epoch 5/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.8780 - acc: 0.6929 - val_loss: 0.8801 - val_acc: 0.6519
Epoch 6/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7994 - acc: 0.7654 - val_loss: 0.8168 - val_acc: 0.7109
Epoch 7/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7260 - acc: 0.8102 - val_loss: 0.7614 - val_acc: 0.7365
Epoch 8/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6604 - acc: 0.8322 - val_loss: 0.7118 - val_acc: 0.7689
Epoch 9/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6025 - acc: 0.8532 - val_loss: 0.6704 - val_acc: 0.7812
Epoch 10/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5520 - acc: 0.8678 - val_loss: 0.6346 - val_acc: 0.7921
Epoch 11/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5075 - acc: 0.8821 - val_loss: 0.6052 - val_acc: 0.7949
Epoch 12/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4682 - acc: 0.8940 - val_loss: 0.5763 - val_acc: 0.8105
Epoch 13/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4330 - acc: 0.9053 - val_loss: 0.5557 - val_acc: 0.8054
Epoch 14/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4012 - acc: 0.9129 - val_loss: 0.5337 - val_acc: 0.8161
Epoch 15/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3727 - acc: 0.9228 - val_loss: 0.5132 - val_acc: 0.8279
Epoch 16/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3467 - acc: 0.9303 - val_loss: 0.4976 - val_acc: 0.8289
Epoch 17/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3232 - acc: 0.9364 - val_loss: 0.4826 - val_acc: 0.8350
Epoch 18/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3016 - acc: 0.9420 - val_loss: 0.4746 - val_acc: 0.8279
Epoch 19/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2818 - acc: 0.9474 - val_loss: 0.4577 - val_acc: 0.8378
Epoch 20/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2632 - acc: 0.9525 - val_loss: 0.4457 - val_acc: 0.8432
Epoch 21/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2465 - acc: 0.9579 - val_loss: 0.4378 - val_acc: 0.8422
Epoch 22/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2309 - acc: 0.9600 - val_loss: 0.4271 - val_acc: 0.8463
Epoch 23/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2163 - acc: 0.9639 - val_loss: 0.4213 - val_acc: 0.8455
Epoch 24/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.2030 - acc: 0.9667 - val_loss: 0.4134 - val_acc: 0.8437
Epoch 25/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1905 - acc: 0.9701 - val_loss: 0.4064 - val_acc: 0.8498
Epoch 26/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1793 - acc: 0.9724 - val_loss: 0.3981 - val_acc: 0.8539
Epoch 27/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1684 - acc: 0.9740 - val_loss: 0.3949 - val_acc: 0.8486
Epoch 28/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1582 - acc: 0.9764 - val_loss: 0.3879 - val_acc: 0.8547
Epoch 29/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1488 - acc: 0.9779 - val_loss: 0.3823 - val_acc: 0.8555
Epoch 30/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1400 - acc: 0.9798 - val_loss: 0.3821 - val_acc: 0.8514
Epoch 31/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1320 - acc: 0.9817 - val_loss: 0.3748 - val_acc: 0.8552
Epoch 32/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1244 - acc: 0.9834 - val_loss: 0.3705 - val_acc: 0.8573
Epoch 33/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1175 - acc: 0.9845 - val_loss: 0.3691 - val_acc: 0.8560
Epoch 34/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1105 - acc: 0.9861 - val_loss: 0.3656 - val_acc: 0.8583
Epoch 35/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1042 - acc: 0.9872 - val_loss: 0.3630 - val_acc: 0.8593
Epoch 36/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0982 - acc: 0.9884 - val_loss: 0.3614 - val_acc: 0.8590
Epoch 37/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0929 - acc: 0.9894 - val_loss: 0.3571 - val_acc: 0.8608
Epoch 38/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0876 - acc: 0.9906 - val_loss: 0.3560 - val_acc: 0.8608
Epoch 39/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0831 - acc: 0.9903 - val_loss: 0.3545 - val_acc: 0.8618
Epoch 40/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0781 - acc: 0.9911 - val_loss: 0.3530 - val_acc: 0.8621
Epoch 41/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0737 - acc: 0.9918 - val_loss: 0.3508 - val_acc: 0.8624
Epoch 42/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0699 - acc: 0.9922 - val_loss: 0.3509 - val_acc: 0.8606
Epoch 43/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0659 - acc: 0.9930 - val_loss: 0.3508 - val_acc: 0.8603
Epoch 44/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0625 - acc: 0.9933 - val_loss: 0.3487 - val_acc: 0.8618
Epoch 45/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0590 - acc: 0.9941 - val_loss: 0.3482 - val_acc: 0.8616
Epoch 46/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0559 - acc: 0.9944 - val_loss: 0.3483 - val_acc: 0.8608
Epoch 47/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0527 - acc: 0.9951 - val_loss: 0.3486 - val_acc: 0.8624
Epoch 48/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0500 - acc: 0.9956 - val_loss: 0.3485 - val_acc: 0.8624
15663/15663 [==============================] - 0s 23us/step
==== Training loss, score are: 0.04770270062365888 0.9959139373044755 =======
3916/3916 [==============================] - 0s 24us/step
==== CV loss, score are: 0.34853063423312114 0.8623595505617978 =======
Running fold 2
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_35 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_35  (None, 20)                0         
_________________________________________________________________
dense_35 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 60us/step
Before training loss, score are: 1.097707118140489 0.3122645725623893
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 87us/step - loss: 1.0826 - acc: 0.4056 - val_loss: 1.0777 - val_acc: 0.3943
Epoch 2/150
15663/15663 [==============================] - 1s 47us/step - loss: 1.0583 - acc: 0.4122 - val_loss: 1.0503 - val_acc: 0.4145
Epoch 3/150
15663/15663 [==============================] - 1s 47us/step - loss: 1.0152 - acc: 0.4627 - val_loss: 1.0056 - val_acc: 0.4609
Epoch 4/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.9505 - acc: 0.5664 - val_loss: 0.9449 - val_acc: 0.5996
Epoch 5/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.8739 - acc: 0.7055 - val_loss: 0.8814 - val_acc: 0.6535
Epoch 6/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.7958 - acc: 0.7663 - val_loss: 0.8198 - val_acc: 0.7204
Epoch 7/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7220 - acc: 0.8082 - val_loss: 0.7642 - val_acc: 0.7510
Epoch 8/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6563 - acc: 0.8394 - val_loss: 0.7169 - val_acc: 0.7589
Epoch 9/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5981 - acc: 0.8555 - val_loss: 0.6752 - val_acc: 0.7806
Epoch 10/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5473 - acc: 0.8731 - val_loss: 0.6400 - val_acc: 0.7932
Epoch 11/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5028 - acc: 0.8840 - val_loss: 0.6077 - val_acc: 0.8077
Epoch 12/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4630 - acc: 0.8975 - val_loss: 0.5820 - val_acc: 0.8062
Epoch 13/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4277 - acc: 0.9070 - val_loss: 0.5573 - val_acc: 0.8246
Epoch 14/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3961 - acc: 0.9176 - val_loss: 0.5367 - val_acc: 0.8200
Epoch 15/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3676 - acc: 0.9263 - val_loss: 0.5194 - val_acc: 0.8202
Epoch 16/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3421 - acc: 0.9318 - val_loss: 0.5005 - val_acc: 0.8371
Epoch 17/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3184 - acc: 0.9380 - val_loss: 0.4873 - val_acc: 0.8304
Epoch 18/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2970 - acc: 0.9448 - val_loss: 0.4715 - val_acc: 0.8435
Epoch 19/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2773 - acc: 0.9501 - val_loss: 0.4599 - val_acc: 0.8427
Epoch 20/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2590 - acc: 0.9547 - val_loss: 0.4495 - val_acc: 0.8458
Epoch 21/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2424 - acc: 0.9589 - val_loss: 0.4387 - val_acc: 0.8465
Epoch 22/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2271 - acc: 0.9620 - val_loss: 0.4291 - val_acc: 0.8504
Epoch 23/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2128 - acc: 0.9660 - val_loss: 0.4205 - val_acc: 0.8519
Epoch 24/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1993 - acc: 0.9690 - val_loss: 0.4130 - val_acc: 0.8524
Epoch 25/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1871 - acc: 0.9712 - val_loss: 0.4055 - val_acc: 0.8537
Epoch 26/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1756 - acc: 0.9739 - val_loss: 0.4003 - val_acc: 0.8565
Epoch 27/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1650 - acc: 0.9760 - val_loss: 0.3943 - val_acc: 0.8550
Epoch 28/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1550 - acc: 0.9781 - val_loss: 0.3898 - val_acc: 0.8557
Epoch 29/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1458 - acc: 0.9791 - val_loss: 0.3862 - val_acc: 0.8565
Epoch 30/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1372 - acc: 0.9805 - val_loss: 0.3803 - val_acc: 0.8590
Epoch 31/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1291 - acc: 0.9820 - val_loss: 0.3745 - val_acc: 0.8613
Epoch 32/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1217 - acc: 0.9837 - val_loss: 0.3724 - val_acc: 0.8603
Epoch 33/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1144 - acc: 0.9854 - val_loss: 0.3711 - val_acc: 0.8585
Epoch 34/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1079 - acc: 0.9864 - val_loss: 0.3657 - val_acc: 0.8626
Epoch 35/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1016 - acc: 0.9873 - val_loss: 0.3627 - val_acc: 0.8618
Epoch 36/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0958 - acc: 0.9887 - val_loss: 0.3588 - val_acc: 0.8616
Epoch 37/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0904 - acc: 0.9896 - val_loss: 0.3577 - val_acc: 0.8636
Epoch 38/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0852 - acc: 0.9900 - val_loss: 0.3549 - val_acc: 0.8629
Epoch 39/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0803 - acc: 0.9914 - val_loss: 0.3537 - val_acc: 0.8621
Epoch 40/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0758 - acc: 0.9918 - val_loss: 0.3519 - val_acc: 0.8624
Epoch 41/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0715 - acc: 0.9923 - val_loss: 0.3524 - val_acc: 0.8616
Epoch 42/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0677 - acc: 0.9927 - val_loss: 0.3490 - val_acc: 0.8616
Epoch 43/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0639 - acc: 0.9934 - val_loss: 0.3500 - val_acc: 0.8621
Epoch 44/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0602 - acc: 0.9939 - val_loss: 0.3472 - val_acc: 0.8608
Epoch 45/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0570 - acc: 0.9943 - val_loss: 0.3479 - val_acc: 0.8606
Epoch 46/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0539 - acc: 0.9947 - val_loss: 0.3482 - val_acc: 0.8626
Epoch 47/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0509 - acc: 0.9951 - val_loss: 0.3454 - val_acc: 0.8631
Epoch 48/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0482 - acc: 0.9955 - val_loss: 0.3458 - val_acc: 0.8629
Epoch 49/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0454 - acc: 0.9957 - val_loss: 0.3471 - val_acc: 0.8616
Epoch 50/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0430 - acc: 0.9959 - val_loss: 0.3467 - val_acc: 0.8631
15663/15663 [==============================] - 0s 23us/step
==== Training loss, score are: 0.04114073684328111 0.9962331609525633 =======
3916/3916 [==============================] - 0s 25us/step
==== CV loss, score are: 0.3467473699135727 0.8631256384065373 =======
Running fold 3
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_36 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_36  (None, 20)                0         
_________________________________________________________________
dense_36 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 61us/step
Before training loss, score are: 1.0956880871320922 0.39539041054444407
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 86us/step - loss: 1.0827 - acc: 0.4046 - val_loss: 1.0754 - val_acc: 0.4002
Epoch 2/150
15663/15663 [==============================] - 1s 47us/step - loss: 1.0587 - acc: 0.4167 - val_loss: 1.0486 - val_acc: 0.4367
Epoch 3/150
15663/15663 [==============================] - 1s 47us/step - loss: 1.0149 - acc: 0.4722 - val_loss: 1.0026 - val_acc: 0.4875
Epoch 4/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.9501 - acc: 0.5916 - val_loss: 0.9436 - val_acc: 0.6083
Epoch 5/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.8741 - acc: 0.6942 - val_loss: 0.8812 - val_acc: 0.7043
Epoch 6/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.7971 - acc: 0.7681 - val_loss: 0.8212 - val_acc: 0.7268
Epoch 7/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.7250 - acc: 0.8076 - val_loss: 0.7677 - val_acc: 0.7354
Epoch 8/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.6602 - acc: 0.8327 - val_loss: 0.7200 - val_acc: 0.7674
Epoch 9/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.6028 - acc: 0.8561 - val_loss: 0.6785 - val_acc: 0.7850
Epoch 10/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5520 - acc: 0.8697 - val_loss: 0.6438 - val_acc: 0.7745
Epoch 11/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5072 - acc: 0.8854 - val_loss: 0.6115 - val_acc: 0.8003
Epoch 12/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4672 - acc: 0.8980 - val_loss: 0.5840 - val_acc: 0.8062
Epoch 13/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4317 - acc: 0.9066 - val_loss: 0.5597 - val_acc: 0.8151
Epoch 14/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3993 - acc: 0.9173 - val_loss: 0.5383 - val_acc: 0.8238
Epoch 15/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3704 - acc: 0.9249 - val_loss: 0.5200 - val_acc: 0.8228
Epoch 16/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3447 - acc: 0.9325 - val_loss: 0.5028 - val_acc: 0.8299
Epoch 17/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3204 - acc: 0.9396 - val_loss: 0.4877 - val_acc: 0.8302
Epoch 18/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2987 - acc: 0.9444 - val_loss: 0.4729 - val_acc: 0.8384
Epoch 19/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2785 - acc: 0.9505 - val_loss: 0.4610 - val_acc: 0.8473
Epoch 20/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2605 - acc: 0.9558 - val_loss: 0.4495 - val_acc: 0.8460
Epoch 21/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2434 - acc: 0.9595 - val_loss: 0.4396 - val_acc: 0.8468
Epoch 22/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2277 - acc: 0.9634 - val_loss: 0.4289 - val_acc: 0.8509
Epoch 23/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2132 - acc: 0.9662 - val_loss: 0.4204 - val_acc: 0.8521
Epoch 24/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1999 - acc: 0.9694 - val_loss: 0.4123 - val_acc: 0.8539
Epoch 25/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1872 - acc: 0.9712 - val_loss: 0.4056 - val_acc: 0.8529
Epoch 26/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1759 - acc: 0.9731 - val_loss: 0.3986 - val_acc: 0.8588
Epoch 27/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1652 - acc: 0.9757 - val_loss: 0.3920 - val_acc: 0.8580
Epoch 28/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1551 - acc: 0.9777 - val_loss: 0.3879 - val_acc: 0.8550
Epoch 29/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1459 - acc: 0.9801 - val_loss: 0.3822 - val_acc: 0.8613
Epoch 30/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1370 - acc: 0.9819 - val_loss: 0.3765 - val_acc: 0.8641
Epoch 31/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1289 - acc: 0.9835 - val_loss: 0.3725 - val_acc: 0.8629
Epoch 32/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1215 - acc: 0.9844 - val_loss: 0.3683 - val_acc: 0.8659
Epoch 33/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1143 - acc: 0.9860 - val_loss: 0.3645 - val_acc: 0.8662
Epoch 34/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1074 - acc: 0.9872 - val_loss: 0.3613 - val_acc: 0.8680
Epoch 35/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1013 - acc: 0.9882 - val_loss: 0.3590 - val_acc: 0.8664
Epoch 36/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0955 - acc: 0.9895 - val_loss: 0.3575 - val_acc: 0.8652
Epoch 37/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0900 - acc: 0.9903 - val_loss: 0.3537 - val_acc: 0.8687
Epoch 38/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0848 - acc: 0.9911 - val_loss: 0.3512 - val_acc: 0.8713
Epoch 39/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0799 - acc: 0.9918 - val_loss: 0.3484 - val_acc: 0.8710
Epoch 40/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0755 - acc: 0.9923 - val_loss: 0.3484 - val_acc: 0.8680
Epoch 41/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0714 - acc: 0.9930 - val_loss: 0.3470 - val_acc: 0.8677
Epoch 42/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0672 - acc: 0.9933 - val_loss: 0.3448 - val_acc: 0.8690
Epoch 43/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0635 - acc: 0.9937 - val_loss: 0.3425 - val_acc: 0.8718
Epoch 44/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0601 - acc: 0.9941 - val_loss: 0.3414 - val_acc: 0.8708
Epoch 45/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0566 - acc: 0.9946 - val_loss: 0.3414 - val_acc: 0.8703
Epoch 46/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0534 - acc: 0.9951 - val_loss: 0.3423 - val_acc: 0.8667
Epoch 47/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0505 - acc: 0.9951 - val_loss: 0.3394 - val_acc: 0.8723
Epoch 48/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0478 - acc: 0.9957 - val_loss: 0.3395 - val_acc: 0.8698
Epoch 49/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0451 - acc: 0.9957 - val_loss: 0.3387 - val_acc: 0.8705
Epoch 50/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0427 - acc: 0.9962 - val_loss: 0.3382 - val_acc: 0.8728
Epoch 51/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0403 - acc: 0.9966 - val_loss: 0.3391 - val_acc: 0.8700
Epoch 52/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0381 - acc: 0.9969 - val_loss: 0.3384 - val_acc: 0.8723
Epoch 53/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0361 - acc: 0.9969 - val_loss: 0.3405 - val_acc: 0.8687
15663/15663 [==============================] - 0s 23us/step
==== Training loss, score are: 0.03473636081730856 0.9964885398710337 =======
3916/3916 [==============================] - 0s 24us/step
==== CV loss, score are: 0.3405472373816283 0.8687436159955103 =======
Running fold 4
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_37 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_37  (None, 20)                0         
_________________________________________________________________
dense_37 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 63us/step
Before training loss, score are: 1.1030528882465729 0.30747621784107154
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 90us/step - loss: 1.0864 - acc: 0.3888 - val_loss: 1.0711 - val_acc: 0.4127
Epoch 2/150
15663/15663 [==============================] - 1s 48us/step - loss: 1.0574 - acc: 0.4165 - val_loss: 1.0411 - val_acc: 0.4402
Epoch 3/150
15663/15663 [==============================] - 1s 48us/step - loss: 1.0111 - acc: 0.4767 - val_loss: 0.9942 - val_acc: 0.5375
Epoch 4/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.9452 - acc: 0.5839 - val_loss: 0.9341 - val_acc: 0.6287
Epoch 5/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.8686 - acc: 0.6981 - val_loss: 0.8707 - val_acc: 0.6696
Epoch 6/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7916 - acc: 0.7659 - val_loss: 0.8091 - val_acc: 0.7199
Epoch 7/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.7189 - acc: 0.8099 - val_loss: 0.7548 - val_acc: 0.7740
Epoch 8/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.6532 - acc: 0.8433 - val_loss: 0.7066 - val_acc: 0.7806
Epoch 9/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5955 - acc: 0.8622 - val_loss: 0.6647 - val_acc: 0.7817
Epoch 10/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5447 - acc: 0.8770 - val_loss: 0.6299 - val_acc: 0.7911
Epoch 11/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5002 - acc: 0.8887 - val_loss: 0.5991 - val_acc: 0.8057
Epoch 12/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4604 - acc: 0.9032 - val_loss: 0.5732 - val_acc: 0.8159
Epoch 13/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4250 - acc: 0.9121 - val_loss: 0.5492 - val_acc: 0.8159
Epoch 14/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3935 - acc: 0.9194 - val_loss: 0.5284 - val_acc: 0.8233
Epoch 15/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3653 - acc: 0.9262 - val_loss: 0.5090 - val_acc: 0.8233
Epoch 16/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3394 - acc: 0.9330 - val_loss: 0.4925 - val_acc: 0.8297
Epoch 17/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3159 - acc: 0.9408 - val_loss: 0.4779 - val_acc: 0.8343
Epoch 18/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2946 - acc: 0.9454 - val_loss: 0.4662 - val_acc: 0.8355
Epoch 19/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2751 - acc: 0.9503 - val_loss: 0.4525 - val_acc: 0.8404
Epoch 20/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2570 - acc: 0.9549 - val_loss: 0.4418 - val_acc: 0.8450
Epoch 21/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2404 - acc: 0.9589 - val_loss: 0.4318 - val_acc: 0.8447
Epoch 22/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2249 - acc: 0.9632 - val_loss: 0.4224 - val_acc: 0.8486
Epoch 23/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2109 - acc: 0.9658 - val_loss: 0.4145 - val_acc: 0.8516
Epoch 24/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1976 - acc: 0.9692 - val_loss: 0.4068 - val_acc: 0.8524
Epoch 25/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1857 - acc: 0.9713 - val_loss: 0.4005 - val_acc: 0.8524
Epoch 26/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1741 - acc: 0.9739 - val_loss: 0.3942 - val_acc: 0.8555
Epoch 27/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1636 - acc: 0.9759 - val_loss: 0.3900 - val_acc: 0.8567
Epoch 28/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1541 - acc: 0.9775 - val_loss: 0.3820 - val_acc: 0.8608
Epoch 29/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1446 - acc: 0.9798 - val_loss: 0.3777 - val_acc: 0.8570
Epoch 30/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1363 - acc: 0.9812 - val_loss: 0.3754 - val_acc: 0.8570
Epoch 31/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1282 - acc: 0.9830 - val_loss: 0.3687 - val_acc: 0.8649
Epoch 32/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1206 - acc: 0.9844 - val_loss: 0.3650 - val_acc: 0.8670
Epoch 33/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1136 - acc: 0.9853 - val_loss: 0.3617 - val_acc: 0.8659
Epoch 34/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1071 - acc: 0.9863 - val_loss: 0.3585 - val_acc: 0.8631
Epoch 35/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1010 - acc: 0.9879 - val_loss: 0.3557 - val_acc: 0.8672
Epoch 36/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0951 - acc: 0.9883 - val_loss: 0.3529 - val_acc: 0.8647
Epoch 37/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0897 - acc: 0.9892 - val_loss: 0.3504 - val_acc: 0.8662
Epoch 38/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0849 - acc: 0.9904 - val_loss: 0.3496 - val_acc: 0.8626
Epoch 39/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0798 - acc: 0.9911 - val_loss: 0.3474 - val_acc: 0.8667
Epoch 40/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0755 - acc: 0.9918 - val_loss: 0.3452 - val_acc: 0.8641
Epoch 41/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0712 - acc: 0.9923 - val_loss: 0.3433 - val_acc: 0.8657
Epoch 42/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0672 - acc: 0.9932 - val_loss: 0.3417 - val_acc: 0.8677
Epoch 43/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0635 - acc: 0.9935 - val_loss: 0.3401 - val_acc: 0.8680
Epoch 44/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0600 - acc: 0.9939 - val_loss: 0.3390 - val_acc: 0.8682
Epoch 45/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0566 - acc: 0.9950 - val_loss: 0.3379 - val_acc: 0.8687
Epoch 46/150
15663/15663 [==============================] - 1s 51us/step - loss: 0.0535 - acc: 0.9948 - val_loss: 0.3371 - val_acc: 0.8690
Epoch 47/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0506 - acc: 0.9955 - val_loss: 0.3368 - val_acc: 0.8682
Epoch 48/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0479 - acc: 0.9955 - val_loss: 0.3363 - val_acc: 0.8685
Epoch 49/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0454 - acc: 0.9958 - val_loss: 0.3361 - val_acc: 0.8685
Epoch 50/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0427 - acc: 0.9962 - val_loss: 0.3365 - val_acc: 0.8667
Epoch 51/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0406 - acc: 0.9960 - val_loss: 0.3353 - val_acc: 0.8700
Epoch 52/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0383 - acc: 0.9967 - val_loss: 0.3366 - val_acc: 0.8664
Epoch 53/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0363 - acc: 0.9970 - val_loss: 0.3355 - val_acc: 0.8705
Epoch 54/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0342 - acc: 0.9972 - val_loss: 0.3373 - val_acc: 0.8670
15663/15663 [==============================] - 0s 24us/step
==== Training loss, score are: 0.033365172293047404 0.9978931239226202 =======
3916/3916 [==============================] - 0s 24us/step
==== CV loss, score are: 0.3373016334377587 0.8669560776911182 =======
Running fold 5
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_38 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_38  (None, 20)                0         
_________________________________________________________________
dense_38 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15664/15664 [==============================] - 1s 66us/step
Before training loss, score are: 1.1011670322535108 0.2913687436159346
Train on 15664 samples, validate on 3915 samples
Epoch 1/150
15664/15664 [==============================] - 1s 93us/step - loss: 1.0854 - acc: 0.3929 - val_loss: 1.0691 - val_acc: 0.4212
Epoch 2/150
15664/15664 [==============================] - 1s 47us/step - loss: 1.0595 - acc: 0.4026 - val_loss: 1.0413 - val_acc: 0.4299
Epoch 3/150
15664/15664 [==============================] - 1s 47us/step - loss: 1.0165 - acc: 0.4510 - val_loss: 0.9977 - val_acc: 0.4991
Epoch 4/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.9541 - acc: 0.5873 - val_loss: 0.9406 - val_acc: 0.6051
Epoch 5/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.8800 - acc: 0.6984 - val_loss: 0.8788 - val_acc: 0.7055
Epoch 6/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.8036 - acc: 0.7842 - val_loss: 0.8172 - val_acc: 0.7246
Epoch 7/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.7312 - acc: 0.8151 - val_loss: 0.7636 - val_acc: 0.7630
Epoch 8/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.6662 - acc: 0.8371 - val_loss: 0.7144 - val_acc: 0.7811
Epoch 9/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.6080 - acc: 0.8579 - val_loss: 0.6728 - val_acc: 0.7908
Epoch 10/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.5572 - acc: 0.8703 - val_loss: 0.6385 - val_acc: 0.8148
Epoch 11/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.5126 - acc: 0.8857 - val_loss: 0.6057 - val_acc: 0.8143
Epoch 12/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.4730 - acc: 0.8976 - val_loss: 0.5787 - val_acc: 0.8169
Epoch 13/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.4377 - acc: 0.9060 - val_loss: 0.5561 - val_acc: 0.8289
Epoch 14/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.4059 - acc: 0.9146 - val_loss: 0.5363 - val_acc: 0.8360
Epoch 15/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.3774 - acc: 0.9227 - val_loss: 0.5169 - val_acc: 0.8363
Epoch 16/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.3517 - acc: 0.9318 - val_loss: 0.4993 - val_acc: 0.8375
Epoch 17/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.3278 - acc: 0.9399 - val_loss: 0.4840 - val_acc: 0.8406
Epoch 18/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.3062 - acc: 0.9441 - val_loss: 0.4713 - val_acc: 0.8429
Epoch 19/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.2865 - acc: 0.9489 - val_loss: 0.4594 - val_acc: 0.8483
Epoch 20/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.2682 - acc: 0.9527 - val_loss: 0.4474 - val_acc: 0.8490
Epoch 21/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.2512 - acc: 0.9564 - val_loss: 0.4373 - val_acc: 0.8526
Epoch 22/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.2358 - acc: 0.9607 - val_loss: 0.4282 - val_acc: 0.8544
Epoch 23/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.2215 - acc: 0.9628 - val_loss: 0.4200 - val_acc: 0.8513
Epoch 24/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.2079 - acc: 0.9665 - val_loss: 0.4119 - val_acc: 0.8570
Epoch 25/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1957 - acc: 0.9689 - val_loss: 0.4048 - val_acc: 0.8567
Epoch 26/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1837 - acc: 0.9718 - val_loss: 0.3987 - val_acc: 0.8559
Epoch 27/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1733 - acc: 0.9736 - val_loss: 0.3930 - val_acc: 0.8603
Epoch 28/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.1635 - acc: 0.9767 - val_loss: 0.3889 - val_acc: 0.8605
Epoch 29/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1537 - acc: 0.9784 - val_loss: 0.3824 - val_acc: 0.8608
Epoch 30/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1447 - acc: 0.9799 - val_loss: 0.3781 - val_acc: 0.8633
Epoch 31/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1364 - acc: 0.9817 - val_loss: 0.3736 - val_acc: 0.8595
Epoch 32/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1285 - acc: 0.9837 - val_loss: 0.3698 - val_acc: 0.8608
Epoch 33/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1215 - acc: 0.9848 - val_loss: 0.3660 - val_acc: 0.8613
Epoch 34/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1144 - acc: 0.9853 - val_loss: 0.3633 - val_acc: 0.8603
Epoch 35/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.1082 - acc: 0.9867 - val_loss: 0.3599 - val_acc: 0.8639
Epoch 36/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.1022 - acc: 0.9881 - val_loss: 0.3570 - val_acc: 0.8651
Epoch 37/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0965 - acc: 0.9886 - val_loss: 0.3557 - val_acc: 0.8626
Epoch 38/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0912 - acc: 0.9898 - val_loss: 0.3527 - val_acc: 0.8636
Epoch 39/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0862 - acc: 0.9901 - val_loss: 0.3504 - val_acc: 0.8662
Epoch 40/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.0816 - acc: 0.9909 - val_loss: 0.3487 - val_acc: 0.8659
Epoch 41/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0772 - acc: 0.9918 - val_loss: 0.3475 - val_acc: 0.8623
Epoch 42/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0730 - acc: 0.9921 - val_loss: 0.3466 - val_acc: 0.8641
Epoch 43/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0691 - acc: 0.9931 - val_loss: 0.3453 - val_acc: 0.8644
Epoch 44/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0654 - acc: 0.9930 - val_loss: 0.3436 - val_acc: 0.8649
Epoch 45/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0618 - acc: 0.9940 - val_loss: 0.3426 - val_acc: 0.8654
Epoch 46/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0586 - acc: 0.9941 - val_loss: 0.3419 - val_acc: 0.8651
Epoch 47/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0556 - acc: 0.9943 - val_loss: 0.3413 - val_acc: 0.8654
Epoch 48/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0527 - acc: 0.9946 - val_loss: 0.3408 - val_acc: 0.8659
Epoch 49/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0498 - acc: 0.9952 - val_loss: 0.3416 - val_acc: 0.8656
Epoch 50/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0474 - acc: 0.9953 - val_loss: 0.3404 - val_acc: 0.8626
Epoch 51/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0446 - acc: 0.9956 - val_loss: 0.3417 - val_acc: 0.8626
Epoch 52/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0425 - acc: 0.9959 - val_loss: 0.3417 - val_acc: 0.8621
Epoch 53/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0402 - acc: 0.9960 - val_loss: 0.3406 - val_acc: 0.8623
15664/15664 [==============================] - 0s 24us/step
==== Training loss, score are: 0.038446925123877616 0.9961695607763024 =======
3915/3915 [==============================] - 0s 25us/step
==== CV loss, score are: 0.3405798800984257 0.8623243934349996 =======


===== Model: fast_text_char_none  ========:
 Cross-val log losses are: [0.34853063227308723, 0.34674736640115189, 0.34054723574078843, 0.33730163001916968, 0.34057987701431947]
====== Mean cross-val log loss is: 0.3427413482897033 =========


Timestamp: 2018-Jan-14 03:13:22
Running kfold training with model fast_text_char_gensim
Shapes: x_train_raw.shape (19579, 39), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 38)
Running preprocess: &lt;function run_NN_preprocess at 0x7f620ee8eea0&gt;
No of unique words found is 247346
No of words occuring more than 2 times is 75104
No of unique words found is after capping is 247346
max sentence length is 1439
mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000
After padding: max sentence size is 161
After padding: mean, median length of sentence is 161.000000 161.000000
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>9853it [00:00, 98505.43it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Creating word embedding matrix for gensim vectors
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>76597it [00:00, 98739.39it/s]
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Found 76597 word vectors.
of--mirth 247345
None
(247346, 20)
Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)
Running fold 1
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_39 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_39  (None, 20)                0         
_________________________________________________________________
dense_39 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 67us/step
Before training loss, score are: 1.2088994647757438 0.40643554877208915
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 92us/step - loss: 1.1343 - acc: 0.3630 - val_loss: 1.1100 - val_acc: 0.3565
Epoch 2/150
15663/15663 [==============================] - 1s 47us/step - loss: 1.0794 - acc: 0.4147 - val_loss: 1.0683 - val_acc: 0.4157
Epoch 3/150
15663/15663 [==============================] - 1s 47us/step - loss: 1.0393 - acc: 0.4599 - val_loss: 1.0310 - val_acc: 0.4597
Epoch 4/150
15663/15663 [==============================] - 1s 48us/step - loss: 1.0015 - acc: 0.5125 - val_loss: 0.9953 - val_acc: 0.5117
Epoch 5/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.9638 - acc: 0.5675 - val_loss: 0.9605 - val_acc: 0.5534
Epoch 6/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.9262 - acc: 0.6137 - val_loss: 0.9262 - val_acc: 0.5975
Epoch 7/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.8883 - acc: 0.6473 - val_loss: 0.8925 - val_acc: 0.6364
Epoch 8/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.8508 - acc: 0.6783 - val_loss: 0.8606 - val_acc: 0.6537
Epoch 9/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.8138 - acc: 0.7027 - val_loss: 0.8301 - val_acc: 0.6739
Epoch 10/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.7779 - acc: 0.7241 - val_loss: 0.8022 - val_acc: 0.6834
Epoch 11/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7432 - acc: 0.7417 - val_loss: 0.7751 - val_acc: 0.7007
Epoch 12/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7097 - acc: 0.7583 - val_loss: 0.7497 - val_acc: 0.7171
Epoch 13/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6774 - acc: 0.7732 - val_loss: 0.7268 - val_acc: 0.7219
Epoch 14/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6462 - acc: 0.7871 - val_loss: 0.7035 - val_acc: 0.7349
Epoch 15/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.6165 - acc: 0.8005 - val_loss: 0.6832 - val_acc: 0.7431
Epoch 16/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5881 - acc: 0.8140 - val_loss: 0.6623 - val_acc: 0.7569
Epoch 17/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5610 - acc: 0.8259 - val_loss: 0.6442 - val_acc: 0.7633
Epoch 18/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5352 - acc: 0.8360 - val_loss: 0.6271 - val_acc: 0.7689
Epoch 19/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5102 - acc: 0.8478 - val_loss: 0.6098 - val_acc: 0.7799
Epoch 20/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4870 - acc: 0.8567 - val_loss: 0.5942 - val_acc: 0.7865
Epoch 21/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4643 - acc: 0.8643 - val_loss: 0.5804 - val_acc: 0.7893
Epoch 22/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4429 - acc: 0.8735 - val_loss: 0.5664 - val_acc: 0.7955
Epoch 23/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4222 - acc: 0.8815 - val_loss: 0.5532 - val_acc: 0.7998
Epoch 24/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4027 - acc: 0.8894 - val_loss: 0.5423 - val_acc: 0.7995
Epoch 25/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3844 - acc: 0.8956 - val_loss: 0.5321 - val_acc: 0.8008
Epoch 26/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3665 - acc: 0.9015 - val_loss: 0.5198 - val_acc: 0.8031
Epoch 27/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3495 - acc: 0.9081 - val_loss: 0.5091 - val_acc: 0.8108
Epoch 28/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3336 - acc: 0.9140 - val_loss: 0.5003 - val_acc: 0.8100
Epoch 29/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3183 - acc: 0.9202 - val_loss: 0.4907 - val_acc: 0.8110
Epoch 30/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3035 - acc: 0.9254 - val_loss: 0.4808 - val_acc: 0.8215
Epoch 31/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.2895 - acc: 0.9300 - val_loss: 0.4746 - val_acc: 0.8195
Epoch 32/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2762 - acc: 0.9343 - val_loss: 0.4662 - val_acc: 0.8238
Epoch 33/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2637 - acc: 0.9383 - val_loss: 0.4583 - val_acc: 0.8241
Epoch 34/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2516 - acc: 0.9420 - val_loss: 0.4519 - val_acc: 0.8274
Epoch 35/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2400 - acc: 0.9475 - val_loss: 0.4454 - val_acc: 0.8317
Epoch 36/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2291 - acc: 0.9510 - val_loss: 0.4392 - val_acc: 0.8320
Epoch 37/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2187 - acc: 0.9540 - val_loss: 0.4351 - val_acc: 0.8327
Epoch 38/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2087 - acc: 0.9569 - val_loss: 0.4301 - val_acc: 0.8355
Epoch 39/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1990 - acc: 0.9582 - val_loss: 0.4237 - val_acc: 0.8371
Epoch 40/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1899 - acc: 0.9618 - val_loss: 0.4200 - val_acc: 0.8386
Epoch 41/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.1815 - acc: 0.9638 - val_loss: 0.4144 - val_acc: 0.8394
Epoch 42/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1732 - acc: 0.9664 - val_loss: 0.4101 - val_acc: 0.8409
Epoch 43/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1653 - acc: 0.9687 - val_loss: 0.4068 - val_acc: 0.8422
Epoch 44/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1579 - acc: 0.9701 - val_loss: 0.4027 - val_acc: 0.8440
Epoch 45/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1507 - acc: 0.9730 - val_loss: 0.3991 - val_acc: 0.8450
Epoch 46/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1438 - acc: 0.9745 - val_loss: 0.3969 - val_acc: 0.8473
Epoch 47/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1375 - acc: 0.9759 - val_loss: 0.3937 - val_acc: 0.8483
Epoch 48/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1314 - acc: 0.9774 - val_loss: 0.3910 - val_acc: 0.8483
Epoch 49/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1253 - acc: 0.9790 - val_loss: 0.3891 - val_acc: 0.8493
Epoch 50/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1196 - acc: 0.9801 - val_loss: 0.3854 - val_acc: 0.8511
Epoch 51/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1143 - acc: 0.9814 - val_loss: 0.3832 - val_acc: 0.8506
Epoch 52/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1092 - acc: 0.9829 - val_loss: 0.3833 - val_acc: 0.8514
Epoch 53/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1042 - acc: 0.9840 - val_loss: 0.3798 - val_acc: 0.8496
Epoch 54/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0996 - acc: 0.9849 - val_loss: 0.3777 - val_acc: 0.8511
Epoch 55/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0954 - acc: 0.9855 - val_loss: 0.3775 - val_acc: 0.8521
Epoch 56/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0911 - acc: 0.9865 - val_loss: 0.3751 - val_acc: 0.8534
Epoch 57/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0870 - acc: 0.9874 - val_loss: 0.3741 - val_acc: 0.8524
Epoch 58/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0831 - acc: 0.9882 - val_loss: 0.3746 - val_acc: 0.8534
Epoch 59/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0796 - acc: 0.9887 - val_loss: 0.3711 - val_acc: 0.8537
Epoch 60/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0760 - acc: 0.9897 - val_loss: 0.3711 - val_acc: 0.8524
Epoch 61/150
15663/15663 [==============================] - 1s 51us/step - loss: 0.0727 - acc: 0.9902 - val_loss: 0.3709 - val_acc: 0.8539
Epoch 62/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0693 - acc: 0.9907 - val_loss: 0.3701 - val_acc: 0.8529
Epoch 63/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0663 - acc: 0.9914 - val_loss: 0.3686 - val_acc: 0.8534
Epoch 64/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.0634 - acc: 0.9912 - val_loss: 0.3680 - val_acc: 0.8537
Epoch 65/150
15663/15663 [==============================] - 1s 50us/step - loss: 0.0606 - acc: 0.9920 - val_loss: 0.3683 - val_acc: 0.8542
Epoch 66/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0581 - acc: 0.9923 - val_loss: 0.3673 - val_acc: 0.8537
Epoch 67/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.0556 - acc: 0.9928 - val_loss: 0.3673 - val_acc: 0.8552
Epoch 68/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.0530 - acc: 0.9936 - val_loss: 0.3677 - val_acc: 0.8555
Epoch 69/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.0507 - acc: 0.9938 - val_loss: 0.3668 - val_acc: 0.8552
Epoch 70/150
15663/15663 [==============================] - 1s 50us/step - loss: 0.0486 - acc: 0.9943 - val_loss: 0.3680 - val_acc: 0.8565
Epoch 71/150
15663/15663 [==============================] - 1s 50us/step - loss: 0.0464 - acc: 0.9944 - val_loss: 0.3698 - val_acc: 0.8565
Epoch 72/150
15663/15663 [==============================] - 1s 50us/step - loss: 0.0444 - acc: 0.9948 - val_loss: 0.3677 - val_acc: 0.8575
15663/15663 [==============================] - 0s 25us/step
==== Training loss, score are: 0.04277276798767856 0.9948924216305944 =======
3916/3916 [==============================] - 0s 26us/step
==== CV loss, score are: 0.3676888651011913 0.8575076608784474 =======
Running fold 2
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_40 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_40  (None, 20)                0         
_________________________________________________________________
dense_40 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 71us/step
Before training loss, score are: 1.2846237993145981 0.30900849135189323
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 2s 96us/step - loss: 1.0967 - acc: 0.4092 - val_loss: 1.0369 - val_acc: 0.4640
Epoch 2/150
15663/15663 [==============================] - 1s 49us/step - loss: 1.0176 - acc: 0.4918 - val_loss: 1.0116 - val_acc: 0.4949
Epoch 3/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.9889 - acc: 0.5323 - val_loss: 0.9863 - val_acc: 0.5174
Epoch 4/150
15663/15663 [==============================] - 1s 51us/step - loss: 0.9592 - acc: 0.5673 - val_loss: 0.9607 - val_acc: 0.5411
Epoch 5/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.9289 - acc: 0.5964 - val_loss: 0.9338 - val_acc: 0.5799
Epoch 6/150
15663/15663 [==============================] - 1s 50us/step - loss: 0.8972 - acc: 0.6249 - val_loss: 0.9075 - val_acc: 0.6016
Epoch 7/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.8648 - acc: 0.6527 - val_loss: 0.8810 - val_acc: 0.6157
Epoch 8/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.8318 - acc: 0.6757 - val_loss: 0.8548 - val_acc: 0.6310
Epoch 9/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7989 - acc: 0.6972 - val_loss: 0.8285 - val_acc: 0.6499
Epoch 10/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7657 - acc: 0.7158 - val_loss: 0.8024 - val_acc: 0.6711
Epoch 11/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7329 - acc: 0.7357 - val_loss: 0.7780 - val_acc: 0.6849
Epoch 12/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7010 - acc: 0.7530 - val_loss: 0.7547 - val_acc: 0.6954
Epoch 13/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6697 - acc: 0.7684 - val_loss: 0.7330 - val_acc: 0.7079
Epoch 14/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6399 - acc: 0.7831 - val_loss: 0.7109 - val_acc: 0.7224
Epoch 15/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6107 - acc: 0.7972 - val_loss: 0.6909 - val_acc: 0.7314
Epoch 16/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5831 - acc: 0.8103 - val_loss: 0.6710 - val_acc: 0.7434
Epoch 17/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5563 - acc: 0.8212 - val_loss: 0.6529 - val_acc: 0.7480
Epoch 18/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5306 - acc: 0.8313 - val_loss: 0.6362 - val_acc: 0.7600
Epoch 19/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5063 - acc: 0.8436 - val_loss: 0.6193 - val_acc: 0.7656
Epoch 20/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4832 - acc: 0.8543 - val_loss: 0.6044 - val_acc: 0.7697
Epoch 21/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4610 - acc: 0.8622 - val_loss: 0.5914 - val_acc: 0.7720
Epoch 22/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4397 - acc: 0.8714 - val_loss: 0.5757 - val_acc: 0.7814
Epoch 23/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4196 - acc: 0.8812 - val_loss: 0.5633 - val_acc: 0.7824
Epoch 24/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4003 - acc: 0.8885 - val_loss: 0.5516 - val_acc: 0.7901
Epoch 25/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3820 - acc: 0.8970 - val_loss: 0.5399 - val_acc: 0.7939
Epoch 26/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.3646 - acc: 0.9028 - val_loss: 0.5289 - val_acc: 0.7975
Epoch 27/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3479 - acc: 0.9092 - val_loss: 0.5175 - val_acc: 0.8049
Epoch 28/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3319 - acc: 0.9148 - val_loss: 0.5082 - val_acc: 0.8080
Epoch 29/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3171 - acc: 0.9190 - val_loss: 0.4985 - val_acc: 0.8118
Epoch 30/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3027 - acc: 0.9256 - val_loss: 0.4905 - val_acc: 0.8138
Epoch 31/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2889 - acc: 0.9291 - val_loss: 0.4812 - val_acc: 0.8200
Epoch 32/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2757 - acc: 0.9351 - val_loss: 0.4741 - val_acc: 0.8212
Epoch 33/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2631 - acc: 0.9393 - val_loss: 0.4672 - val_acc: 0.8230
Epoch 34/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2511 - acc: 0.9430 - val_loss: 0.4595 - val_acc: 0.8307
Epoch 35/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2402 - acc: 0.9475 - val_loss: 0.4536 - val_acc: 0.8294
Epoch 36/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2290 - acc: 0.9515 - val_loss: 0.4474 - val_acc: 0.8327
Epoch 37/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2188 - acc: 0.9544 - val_loss: 0.4405 - val_acc: 0.8378
Epoch 38/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2088 - acc: 0.9575 - val_loss: 0.4354 - val_acc: 0.8384
Epoch 39/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1998 - acc: 0.9604 - val_loss: 0.4301 - val_acc: 0.8381
Epoch 40/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1907 - acc: 0.9631 - val_loss: 0.4253 - val_acc: 0.8401
Epoch 41/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1822 - acc: 0.9648 - val_loss: 0.4201 - val_acc: 0.8419
Epoch 42/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1740 - acc: 0.9674 - val_loss: 0.4156 - val_acc: 0.8430
Epoch 43/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1662 - acc: 0.9687 - val_loss: 0.4112 - val_acc: 0.8445
Epoch 44/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1588 - acc: 0.9705 - val_loss: 0.4073 - val_acc: 0.8465
Epoch 45/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1517 - acc: 0.9723 - val_loss: 0.4052 - val_acc: 0.8447
Epoch 46/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1451 - acc: 0.9740 - val_loss: 0.4001 - val_acc: 0.8481
Epoch 47/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1385 - acc: 0.9752 - val_loss: 0.3976 - val_acc: 0.8470
Epoch 48/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1325 - acc: 0.9770 - val_loss: 0.3950 - val_acc: 0.8481
Epoch 49/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1266 - acc: 0.9778 - val_loss: 0.3919 - val_acc: 0.8491
Epoch 50/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1210 - acc: 0.9794 - val_loss: 0.3884 - val_acc: 0.8516
Epoch 51/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1156 - acc: 0.9807 - val_loss: 0.3875 - val_acc: 0.8498
Epoch 52/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1104 - acc: 0.9810 - val_loss: 0.3845 - val_acc: 0.8511
Epoch 53/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1056 - acc: 0.9819 - val_loss: 0.3821 - val_acc: 0.8521
Epoch 54/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1008 - acc: 0.9833 - val_loss: 0.3799 - val_acc: 0.8534
Epoch 55/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0964 - acc: 0.9842 - val_loss: 0.3778 - val_acc: 0.8534
Epoch 56/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0921 - acc: 0.9856 - val_loss: 0.3767 - val_acc: 0.8537
Epoch 57/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0880 - acc: 0.9862 - val_loss: 0.3746 - val_acc: 0.8560
Epoch 58/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0842 - acc: 0.9874 - val_loss: 0.3741 - val_acc: 0.8521
Epoch 59/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0804 - acc: 0.9885 - val_loss: 0.3719 - val_acc: 0.8565
Epoch 60/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0770 - acc: 0.9890 - val_loss: 0.3715 - val_acc: 0.8537
Epoch 61/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0734 - acc: 0.9897 - val_loss: 0.3699 - val_acc: 0.8560
Epoch 62/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0702 - acc: 0.9904 - val_loss: 0.3689 - val_acc: 0.8567
Epoch 63/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0670 - acc: 0.9912 - val_loss: 0.3694 - val_acc: 0.8539
Epoch 64/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0641 - acc: 0.9911 - val_loss: 0.3680 - val_acc: 0.8542
Epoch 65/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0614 - acc: 0.9916 - val_loss: 0.3676 - val_acc: 0.8573
Epoch 66/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0587 - acc: 0.9920 - val_loss: 0.3670 - val_acc: 0.8547
Epoch 67/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0561 - acc: 0.9927 - val_loss: 0.3664 - val_acc: 0.8555
Epoch 68/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0535 - acc: 0.9928 - val_loss: 0.3683 - val_acc: 0.8560
Epoch 69/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0512 - acc: 0.9932 - val_loss: 0.3664 - val_acc: 0.8552
Epoch 70/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0490 - acc: 0.9936 - val_loss: 0.3660 - val_acc: 0.8562
Epoch 71/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0467 - acc: 0.9943 - val_loss: 0.3666 - val_acc: 0.8560
Epoch 72/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0447 - acc: 0.9941 - val_loss: 0.3657 - val_acc: 0.8570
Epoch 73/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0427 - acc: 0.9947 - val_loss: 0.3670 - val_acc: 0.8560
Epoch 74/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0409 - acc: 0.9948 - val_loss: 0.3663 - val_acc: 0.8588
Epoch 75/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0391 - acc: 0.9950 - val_loss: 0.3663 - val_acc: 0.8593
15663/15663 [==============================] - 0s 25us/step
==== Training loss, score are: 0.03774240515516809 0.9960416267637107 =======
3916/3916 [==============================] - 0s 25us/step
==== CV loss, score are: 0.3663391914290963 0.8592951991828396 =======
Running fold 3
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_41 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_41  (None, 20)                0         
_________________________________________________________________
dense_41 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 67us/step
Before training loss, score are: 1.1209678778754624 0.33486560684986316
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 92us/step - loss: 1.0868 - acc: 0.3923 - val_loss: 1.0690 - val_acc: 0.4104
Epoch 2/150
15663/15663 [==============================] - 1s 48us/step - loss: 1.0433 - acc: 0.4504 - val_loss: 1.0325 - val_acc: 0.4617
Epoch 3/150
15663/15663 [==============================] - 1s 48us/step - loss: 1.0055 - acc: 0.5028 - val_loss: 0.9997 - val_acc: 0.5186
Epoch 4/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.9693 - acc: 0.5588 - val_loss: 0.9677 - val_acc: 0.5429
Epoch 5/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.9332 - acc: 0.5978 - val_loss: 0.9375 - val_acc: 0.5697
Epoch 6/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.8973 - acc: 0.6315 - val_loss: 0.9064 - val_acc: 0.6065
Epoch 7/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.8610 - acc: 0.6629 - val_loss: 0.8768 - val_acc: 0.6305
Epoch 8/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.8254 - acc: 0.6889 - val_loss: 0.8484 - val_acc: 0.6563
Epoch 9/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7905 - acc: 0.7132 - val_loss: 0.8213 - val_acc: 0.6662
Epoch 10/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7562 - acc: 0.7319 - val_loss: 0.7954 - val_acc: 0.6821
Epoch 11/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.7232 - acc: 0.7499 - val_loss: 0.7708 - val_acc: 0.6974
Epoch 12/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6913 - acc: 0.7629 - val_loss: 0.7481 - val_acc: 0.7145
Epoch 13/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6604 - acc: 0.7801 - val_loss: 0.7255 - val_acc: 0.7188
Epoch 14/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6307 - acc: 0.7911 - val_loss: 0.7046 - val_acc: 0.7283
Epoch 15/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.6023 - acc: 0.8037 - val_loss: 0.6852 - val_acc: 0.7349
Epoch 16/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5751 - acc: 0.8175 - val_loss: 0.6671 - val_acc: 0.7395
Epoch 17/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5491 - acc: 0.8284 - val_loss: 0.6494 - val_acc: 0.7485
Epoch 18/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.5242 - acc: 0.8365 - val_loss: 0.6330 - val_acc: 0.7597
Epoch 19/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5005 - acc: 0.8472 - val_loss: 0.6172 - val_acc: 0.7653
Epoch 20/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4778 - acc: 0.8568 - val_loss: 0.6023 - val_acc: 0.7704
Epoch 21/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4561 - acc: 0.8639 - val_loss: 0.5883 - val_acc: 0.7776
Epoch 22/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4355 - acc: 0.8737 - val_loss: 0.5753 - val_acc: 0.7824
Epoch 23/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4157 - acc: 0.8823 - val_loss: 0.5626 - val_acc: 0.7870
Epoch 24/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3966 - acc: 0.8888 - val_loss: 0.5514 - val_acc: 0.7901
Epoch 25/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3789 - acc: 0.8964 - val_loss: 0.5401 - val_acc: 0.7952
Epoch 26/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3614 - acc: 0.9014 - val_loss: 0.5289 - val_acc: 0.7978
Epoch 27/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3450 - acc: 0.9086 - val_loss: 0.5191 - val_acc: 0.8018
Epoch 28/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3292 - acc: 0.9139 - val_loss: 0.5103 - val_acc: 0.8046
Epoch 29/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3143 - acc: 0.9199 - val_loss: 0.5004 - val_acc: 0.8118
Epoch 30/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2999 - acc: 0.9252 - val_loss: 0.4920 - val_acc: 0.8113
Epoch 31/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2863 - acc: 0.9286 - val_loss: 0.4842 - val_acc: 0.8126
Epoch 32/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2734 - acc: 0.9336 - val_loss: 0.4758 - val_acc: 0.8187
Epoch 33/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2610 - acc: 0.9375 - val_loss: 0.4687 - val_acc: 0.8220
Epoch 34/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2492 - acc: 0.9415 - val_loss: 0.4618 - val_acc: 0.8241
Epoch 35/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2380 - acc: 0.9454 - val_loss: 0.4553 - val_acc: 0.8266
Epoch 36/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2271 - acc: 0.9487 - val_loss: 0.4493 - val_acc: 0.8266
Epoch 37/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2170 - acc: 0.9528 - val_loss: 0.4442 - val_acc: 0.8317
Epoch 38/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2072 - acc: 0.9556 - val_loss: 0.4376 - val_acc: 0.8317
Epoch 39/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1981 - acc: 0.9579 - val_loss: 0.4325 - val_acc: 0.8361
Epoch 40/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1892 - acc: 0.9607 - val_loss: 0.4270 - val_acc: 0.8386
Epoch 41/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1805 - acc: 0.9638 - val_loss: 0.4231 - val_acc: 0.8399
Epoch 42/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1727 - acc: 0.9650 - val_loss: 0.4179 - val_acc: 0.8407
Epoch 43/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1649 - acc: 0.9674 - val_loss: 0.4134 - val_acc: 0.8424
Epoch 44/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1574 - acc: 0.9692 - val_loss: 0.4098 - val_acc: 0.8427
Epoch 45/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1504 - acc: 0.9714 - val_loss: 0.4068 - val_acc: 0.8450
Epoch 46/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1435 - acc: 0.9728 - val_loss: 0.4042 - val_acc: 0.8455
Epoch 47/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1372 - acc: 0.9752 - val_loss: 0.3985 - val_acc: 0.8458
Epoch 48/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1311 - acc: 0.9762 - val_loss: 0.3957 - val_acc: 0.8455
Epoch 49/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1252 - acc: 0.9775 - val_loss: 0.3925 - val_acc: 0.8465
Epoch 50/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1198 - acc: 0.9787 - val_loss: 0.3894 - val_acc: 0.8468
Epoch 51/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1143 - acc: 0.9804 - val_loss: 0.3868 - val_acc: 0.8481
Epoch 52/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.1092 - acc: 0.9814 - val_loss: 0.3846 - val_acc: 0.8483
Epoch 53/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1044 - acc: 0.9829 - val_loss: 0.3824 - val_acc: 0.8498
Epoch 54/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0997 - acc: 0.9835 - val_loss: 0.3804 - val_acc: 0.8506
Epoch 55/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0952 - acc: 0.9854 - val_loss: 0.3784 - val_acc: 0.8504
Epoch 56/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0911 - acc: 0.9855 - val_loss: 0.3781 - val_acc: 0.8524
Epoch 57/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0869 - acc: 0.9863 - val_loss: 0.3743 - val_acc: 0.8514
Epoch 58/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0831 - acc: 0.9876 - val_loss: 0.3729 - val_acc: 0.8516
Epoch 59/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0793 - acc: 0.9882 - val_loss: 0.3723 - val_acc: 0.8532
Epoch 60/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0760 - acc: 0.9889 - val_loss: 0.3701 - val_acc: 0.8527
Epoch 61/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0726 - acc: 0.9893 - val_loss: 0.3684 - val_acc: 0.8542
Epoch 62/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0694 - acc: 0.9897 - val_loss: 0.3674 - val_acc: 0.8542
Epoch 63/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0663 - acc: 0.9907 - val_loss: 0.3665 - val_acc: 0.8555
Epoch 64/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0633 - acc: 0.9913 - val_loss: 0.3652 - val_acc: 0.8560
Epoch 65/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0606 - acc: 0.9914 - val_loss: 0.3646 - val_acc: 0.8580
Epoch 66/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0579 - acc: 0.9923 - val_loss: 0.3639 - val_acc: 0.8583
Epoch 67/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0552 - acc: 0.9927 - val_loss: 0.3642 - val_acc: 0.8583
Epoch 68/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0528 - acc: 0.9930 - val_loss: 0.3624 - val_acc: 0.8596
Epoch 69/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0504 - acc: 0.9934 - val_loss: 0.3632 - val_acc: 0.8585
Epoch 70/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0482 - acc: 0.9938 - val_loss: 0.3617 - val_acc: 0.8598
Epoch 71/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0461 - acc: 0.9942 - val_loss: 0.3627 - val_acc: 0.8588
Epoch 72/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0442 - acc: 0.9946 - val_loss: 0.3612 - val_acc: 0.8611
Epoch 73/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0421 - acc: 0.9951 - val_loss: 0.3619 - val_acc: 0.8590
Epoch 74/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0404 - acc: 0.9953 - val_loss: 0.3624 - val_acc: 0.8601
Epoch 75/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0384 - acc: 0.9956 - val_loss: 0.3626 - val_acc: 0.8606
15663/15663 [==============================] - 0s 25us/step
==== Training loss, score are: 0.037526346300010636 0.9963608504117986 =======
3916/3916 [==============================] - 0s 26us/step
==== CV loss, score are: 0.362552332500637 0.8605720122574055 =======
Running fold 4
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_42 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_42  (None, 20)                0         
_________________________________________________________________
dense_42 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15663/15663 [==============================] - 1s 69us/step
Before training loss, score are: 1.1164819725261659 0.3409947008931499
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 94us/step - loss: 1.0487 - acc: 0.4497 - val_loss: 1.0208 - val_acc: 0.4939
Epoch 2/150
15663/15663 [==============================] - 1s 48us/step - loss: 1.0059 - acc: 0.5025 - val_loss: 0.9904 - val_acc: 0.5306
Epoch 3/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.9729 - acc: 0.5472 - val_loss: 0.9608 - val_acc: 0.5659
Epoch 4/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.9398 - acc: 0.5864 - val_loss: 0.9310 - val_acc: 0.5919
Epoch 5/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.9061 - acc: 0.6199 - val_loss: 0.9023 - val_acc: 0.6213
Epoch 6/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.8720 - acc: 0.6420 - val_loss: 0.8739 - val_acc: 0.6412
Epoch 7/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.8377 - acc: 0.6715 - val_loss: 0.8469 - val_acc: 0.6632
Epoch 8/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.8039 - acc: 0.6937 - val_loss: 0.8206 - val_acc: 0.6818
Epoch 9/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.7702 - acc: 0.7138 - val_loss: 0.7950 - val_acc: 0.6925
Epoch 10/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.7375 - acc: 0.7313 - val_loss: 0.7706 - val_acc: 0.7061
Epoch 11/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.7055 - acc: 0.7494 - val_loss: 0.7472 - val_acc: 0.7140
Epoch 12/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.6745 - acc: 0.7624 - val_loss: 0.7259 - val_acc: 0.7298
Epoch 13/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.6445 - acc: 0.7765 - val_loss: 0.7053 - val_acc: 0.7403
Epoch 14/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.6155 - acc: 0.7939 - val_loss: 0.6852 - val_acc: 0.7441
Epoch 15/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5876 - acc: 0.8058 - val_loss: 0.6671 - val_acc: 0.7510
Epoch 16/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5610 - acc: 0.8174 - val_loss: 0.6484 - val_acc: 0.7605
Epoch 17/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5357 - acc: 0.8282 - val_loss: 0.6311 - val_acc: 0.7615
Epoch 18/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.5110 - acc: 0.8390 - val_loss: 0.6150 - val_acc: 0.7692
Epoch 19/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4878 - acc: 0.8485 - val_loss: 0.6003 - val_acc: 0.7783
Epoch 20/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4654 - acc: 0.8587 - val_loss: 0.5858 - val_acc: 0.7812
Epoch 21/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4441 - acc: 0.8676 - val_loss: 0.5728 - val_acc: 0.7855
Epoch 22/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.4237 - acc: 0.8756 - val_loss: 0.5601 - val_acc: 0.7891
Epoch 23/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.4041 - acc: 0.8832 - val_loss: 0.5481 - val_acc: 0.7932
Epoch 24/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3856 - acc: 0.8915 - val_loss: 0.5367 - val_acc: 0.7955
Epoch 25/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3681 - acc: 0.8971 - val_loss: 0.5255 - val_acc: 0.8034
Epoch 26/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3509 - acc: 0.9044 - val_loss: 0.5155 - val_acc: 0.8062
Epoch 27/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3349 - acc: 0.9112 - val_loss: 0.5054 - val_acc: 0.8108
Epoch 28/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.3198 - acc: 0.9167 - val_loss: 0.4963 - val_acc: 0.8136
Epoch 29/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.3048 - acc: 0.9221 - val_loss: 0.4881 - val_acc: 0.8151
Epoch 30/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2909 - acc: 0.9284 - val_loss: 0.4793 - val_acc: 0.8192
Epoch 31/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2771 - acc: 0.9320 - val_loss: 0.4751 - val_acc: 0.8210
Epoch 32/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2652 - acc: 0.9366 - val_loss: 0.4659 - val_acc: 0.8230
Epoch 33/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2531 - acc: 0.9414 - val_loss: 0.4574 - val_acc: 0.8281
Epoch 34/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2413 - acc: 0.9457 - val_loss: 0.4519 - val_acc: 0.8292
Epoch 35/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.2305 - acc: 0.9487 - val_loss: 0.4452 - val_acc: 0.8348
Epoch 36/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2200 - acc: 0.9520 - val_loss: 0.4388 - val_acc: 0.8363
Epoch 37/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.2103 - acc: 0.9554 - val_loss: 0.4346 - val_acc: 0.8376
Epoch 38/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.2007 - acc: 0.9575 - val_loss: 0.4281 - val_acc: 0.8394
Epoch 39/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1917 - acc: 0.9589 - val_loss: 0.4238 - val_acc: 0.8412
Epoch 40/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1830 - acc: 0.9623 - val_loss: 0.4195 - val_acc: 0.8417
Epoch 41/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1748 - acc: 0.9651 - val_loss: 0.4143 - val_acc: 0.8450
Epoch 42/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1669 - acc: 0.9670 - val_loss: 0.4109 - val_acc: 0.8460
Epoch 43/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.1595 - acc: 0.9695 - val_loss: 0.4068 - val_acc: 0.8432
Epoch 44/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1523 - acc: 0.9718 - val_loss: 0.4025 - val_acc: 0.8468
Epoch 45/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1455 - acc: 0.9740 - val_loss: 0.3998 - val_acc: 0.8501
Epoch 46/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1391 - acc: 0.9762 - val_loss: 0.3964 - val_acc: 0.8491
Epoch 47/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1330 - acc: 0.9767 - val_loss: 0.3928 - val_acc: 0.8496
Epoch 48/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1270 - acc: 0.9784 - val_loss: 0.3901 - val_acc: 0.8529
Epoch 49/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.1212 - acc: 0.9799 - val_loss: 0.3893 - val_acc: 0.8529
Epoch 50/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1160 - acc: 0.9810 - val_loss: 0.3847 - val_acc: 0.8537
Epoch 51/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1109 - acc: 0.9819 - val_loss: 0.3828 - val_acc: 0.8544
Epoch 52/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.1058 - acc: 0.9829 - val_loss: 0.3799 - val_acc: 0.8537
Epoch 53/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.1011 - acc: 0.9835 - val_loss: 0.3790 - val_acc: 0.8501
Epoch 54/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.0967 - acc: 0.9842 - val_loss: 0.3769 - val_acc: 0.8547
Epoch 55/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0924 - acc: 0.9856 - val_loss: 0.3746 - val_acc: 0.8560
Epoch 56/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0885 - acc: 0.9865 - val_loss: 0.3729 - val_acc: 0.8565
Epoch 57/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0844 - acc: 0.9874 - val_loss: 0.3715 - val_acc: 0.8588
Epoch 58/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.0807 - acc: 0.9880 - val_loss: 0.3710 - val_acc: 0.8560
Epoch 59/150
15663/15663 [==============================] - 1s 49us/step - loss: 0.0771 - acc: 0.9885 - val_loss: 0.3696 - val_acc: 0.8578
Epoch 60/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0737 - acc: 0.9894 - val_loss: 0.3680 - val_acc: 0.8575
Epoch 61/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0705 - acc: 0.9897 - val_loss: 0.3676 - val_acc: 0.8573
Epoch 62/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0673 - acc: 0.9899 - val_loss: 0.3660 - val_acc: 0.8593
Epoch 63/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0645 - acc: 0.9906 - val_loss: 0.3654 - val_acc: 0.8606
Epoch 64/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0617 - acc: 0.9908 - val_loss: 0.3650 - val_acc: 0.8593
Epoch 65/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0590 - acc: 0.9912 - val_loss: 0.3638 - val_acc: 0.8585
Epoch 66/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0564 - acc: 0.9918 - val_loss: 0.3632 - val_acc: 0.8590
Epoch 67/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0538 - acc: 0.9922 - val_loss: 0.3628 - val_acc: 0.8585
Epoch 68/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0515 - acc: 0.9928 - val_loss: 0.3625 - val_acc: 0.8588
Epoch 69/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0492 - acc: 0.9930 - val_loss: 0.3620 - val_acc: 0.8593
Epoch 70/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0470 - acc: 0.9934 - val_loss: 0.3621 - val_acc: 0.8588
Epoch 71/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0450 - acc: 0.9936 - val_loss: 0.3618 - val_acc: 0.8601
Epoch 72/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0430 - acc: 0.9944 - val_loss: 0.3622 - val_acc: 0.8598
Epoch 73/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0412 - acc: 0.9945 - val_loss: 0.3617 - val_acc: 0.8593
Epoch 74/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0394 - acc: 0.9952 - val_loss: 0.3625 - val_acc: 0.8598
Epoch 75/150
15663/15663 [==============================] - 1s 48us/step - loss: 0.0376 - acc: 0.9955 - val_loss: 0.3631 - val_acc: 0.8596
Epoch 76/150
15663/15663 [==============================] - 1s 47us/step - loss: 0.0362 - acc: 0.9959 - val_loss: 0.3626 - val_acc: 0.8598
15663/15663 [==============================] - 0s 29us/step
==== Training loss, score are: 0.03467361978794812 0.9958500925748579 =======
3916/3916 [==============================] - 0s 25us/step
==== CV loss, score are: 0.3626462497707529 0.8598059244735492 =======
Running fold 5
Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_43 (Embedding)     (None, 161, 20)           4946920   
_________________________________________________________________
global_average_pooling1d_43  (None, 20)                0         
_________________________________________________________________
dense_43 (Dense)             (None, 3)                 63        
=================================================================
Total params: 4,946,983
Trainable params: 4,946,983
Non-trainable params: 0
_________________________________________________________________
15664/15664 [==============================] - 1s 68us/step
Before training loss, score are: 1.1358444641997791 0.3150536261491318
Train on 15664 samples, validate on 3915 samples
Epoch 1/150
15664/15664 [==============================] - 1s 94us/step - loss: 1.0948 - acc: 0.3810 - val_loss: 1.0665 - val_acc: 0.4345
Epoch 2/150
15664/15664 [==============================] - 1s 47us/step - loss: 1.0491 - acc: 0.4484 - val_loss: 1.0269 - val_acc: 0.4713
Epoch 3/150
15664/15664 [==============================] - 1s 47us/step - loss: 1.0107 - acc: 0.5015 - val_loss: 0.9923 - val_acc: 0.5333
Epoch 4/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.9737 - acc: 0.5550 - val_loss: 0.9587 - val_acc: 0.5839
Epoch 5/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.9371 - acc: 0.5984 - val_loss: 0.9262 - val_acc: 0.6179
Epoch 6/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.9002 - acc: 0.6385 - val_loss: 0.8930 - val_acc: 0.6388
Epoch 7/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.8630 - acc: 0.6643 - val_loss: 0.8624 - val_acc: 0.6669
Epoch 8/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.8257 - acc: 0.6909 - val_loss: 0.8305 - val_acc: 0.6764
Epoch 9/150
15664/15664 [==============================] - 1s 51us/step - loss: 0.7891 - acc: 0.7141 - val_loss: 0.8011 - val_acc: 0.6996
Epoch 10/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.7533 - acc: 0.7365 - val_loss: 0.7733 - val_acc: 0.7114
Epoch 11/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.7183 - acc: 0.7527 - val_loss: 0.7472 - val_acc: 0.7282
Epoch 12/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.6853 - acc: 0.7684 - val_loss: 0.7221 - val_acc: 0.7382
Epoch 13/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.6528 - acc: 0.7845 - val_loss: 0.6981 - val_acc: 0.7443
Epoch 14/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.6223 - acc: 0.7980 - val_loss: 0.6769 - val_acc: 0.7545
Epoch 15/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.5928 - acc: 0.8114 - val_loss: 0.6561 - val_acc: 0.7612
Epoch 16/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.5651 - acc: 0.8249 - val_loss: 0.6368 - val_acc: 0.7640
Epoch 17/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.5385 - acc: 0.8333 - val_loss: 0.6205 - val_acc: 0.7745
Epoch 18/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.5132 - acc: 0.8447 - val_loss: 0.6029 - val_acc: 0.7783
Epoch 19/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.4891 - acc: 0.8551 - val_loss: 0.5869 - val_acc: 0.7842
Epoch 20/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.4665 - acc: 0.8663 - val_loss: 0.5727 - val_acc: 0.7921
Epoch 21/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.4447 - acc: 0.8749 - val_loss: 0.5590 - val_acc: 0.7941
Epoch 22/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.4245 - acc: 0.8825 - val_loss: 0.5452 - val_acc: 0.7990
Epoch 23/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.4047 - acc: 0.8885 - val_loss: 0.5343 - val_acc: 0.8033
Epoch 24/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.3860 - acc: 0.8954 - val_loss: 0.5224 - val_acc: 0.8066
Epoch 25/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.3682 - acc: 0.9042 - val_loss: 0.5114 - val_acc: 0.8110
Epoch 26/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.3516 - acc: 0.9081 - val_loss: 0.5015 - val_acc: 0.8133
Epoch 27/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.3354 - acc: 0.9147 - val_loss: 0.4917 - val_acc: 0.8176
Epoch 28/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.3203 - acc: 0.9197 - val_loss: 0.4826 - val_acc: 0.8199
Epoch 29/150
15664/15664 [==============================] - 1s 51us/step - loss: 0.3057 - acc: 0.9245 - val_loss: 0.4741 - val_acc: 0.8238
Epoch 30/150
15664/15664 [==============================] - 1s 51us/step - loss: 0.2918 - acc: 0.9302 - val_loss: 0.4669 - val_acc: 0.8253
Epoch 31/150
15664/15664 [==============================] - 1s 52us/step - loss: 0.2788 - acc: 0.9340 - val_loss: 0.4593 - val_acc: 0.8301
Epoch 32/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.2662 - acc: 0.9390 - val_loss: 0.4518 - val_acc: 0.8314
Epoch 33/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.2542 - acc: 0.9438 - val_loss: 0.4446 - val_acc: 0.8312
Epoch 34/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.2430 - acc: 0.9462 - val_loss: 0.4384 - val_acc: 0.8330
Epoch 35/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.2320 - acc: 0.9498 - val_loss: 0.4325 - val_acc: 0.8352
Epoch 36/150
15664/15664 [==============================] - 1s 51us/step - loss: 0.2216 - acc: 0.9537 - val_loss: 0.4271 - val_acc: 0.8388
Epoch 37/150
15664/15664 [==============================] - 1s 51us/step - loss: 0.2118 - acc: 0.9563 - val_loss: 0.4214 - val_acc: 0.8406
Epoch 38/150
15664/15664 [==============================] - 1s 51us/step - loss: 0.2024 - acc: 0.9590 - val_loss: 0.4163 - val_acc: 0.8421
Epoch 39/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.1934 - acc: 0.9603 - val_loss: 0.4117 - val_acc: 0.8457
Epoch 40/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.1847 - acc: 0.9630 - val_loss: 0.4071 - val_acc: 0.8442
Epoch 41/150
15664/15664 [==============================] - 1s 51us/step - loss: 0.1765 - acc: 0.9646 - val_loss: 0.4033 - val_acc: 0.8480
Epoch 42/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.1688 - acc: 0.9675 - val_loss: 0.3988 - val_acc: 0.8493
Epoch 43/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1612 - acc: 0.9695 - val_loss: 0.3951 - val_acc: 0.8501
Epoch 44/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1541 - acc: 0.9712 - val_loss: 0.3930 - val_acc: 0.8511
Epoch 45/150
15664/15664 [==============================] - 1s 51us/step - loss: 0.1474 - acc: 0.9734 - val_loss: 0.3881 - val_acc: 0.8521
Epoch 46/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.1410 - acc: 0.9761 - val_loss: 0.3852 - val_acc: 0.8521
Epoch 47/150
15664/15664 [==============================] - 1s 51us/step - loss: 0.1346 - acc: 0.9767 - val_loss: 0.3823 - val_acc: 0.8524
Epoch 48/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.1286 - acc: 0.9779 - val_loss: 0.3801 - val_acc: 0.8547
Epoch 49/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.1229 - acc: 0.9789 - val_loss: 0.3776 - val_acc: 0.8529
Epoch 50/150
15664/15664 [==============================] - 1s 50us/step - loss: 0.1176 - acc: 0.9805 - val_loss: 0.3753 - val_acc: 0.8547
Epoch 51/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.1123 - acc: 0.9817 - val_loss: 0.3733 - val_acc: 0.8547
Epoch 52/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.1073 - acc: 0.9831 - val_loss: 0.3717 - val_acc: 0.8559
Epoch 53/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.1027 - acc: 0.9838 - val_loss: 0.3692 - val_acc: 0.8534
Epoch 54/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0982 - acc: 0.9849 - val_loss: 0.3669 - val_acc: 0.8552
Epoch 55/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0940 - acc: 0.9859 - val_loss: 0.3654 - val_acc: 0.8554
Epoch 56/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0897 - acc: 0.9866 - val_loss: 0.3638 - val_acc: 0.8552
Epoch 57/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0858 - acc: 0.9870 - val_loss: 0.3625 - val_acc: 0.8562
Epoch 58/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0821 - acc: 0.9881 - val_loss: 0.3613 - val_acc: 0.8557
Epoch 59/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.0784 - acc: 0.9886 - val_loss: 0.3602 - val_acc: 0.8559
Epoch 60/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0749 - acc: 0.9887 - val_loss: 0.3592 - val_acc: 0.8552
Epoch 61/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0717 - acc: 0.9897 - val_loss: 0.3585 - val_acc: 0.8547
Epoch 62/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0686 - acc: 0.9900 - val_loss: 0.3582 - val_acc: 0.8567
Epoch 63/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0656 - acc: 0.9906 - val_loss: 0.3572 - val_acc: 0.8559
Epoch 64/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0627 - acc: 0.9908 - val_loss: 0.3575 - val_acc: 0.8562
Epoch 65/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0600 - acc: 0.9921 - val_loss: 0.3573 - val_acc: 0.8582
Epoch 66/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0575 - acc: 0.9921 - val_loss: 0.3557 - val_acc: 0.8570
Epoch 67/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0548 - acc: 0.9929 - val_loss: 0.3570 - val_acc: 0.8582
Epoch 68/150
15664/15664 [==============================] - 1s 49us/step - loss: 0.0526 - acc: 0.9928 - val_loss: 0.3554 - val_acc: 0.8567
Epoch 69/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0502 - acc: 0.9937 - val_loss: 0.3550 - val_acc: 0.8582
Epoch 70/150
15664/15664 [==============================] - 1s 47us/step - loss: 0.0480 - acc: 0.9936 - val_loss: 0.3555 - val_acc: 0.8595
Epoch 71/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0460 - acc: 0.9941 - val_loss: 0.3551 - val_acc: 0.8593
Epoch 72/150
15664/15664 [==============================] - 1s 48us/step - loss: 0.0440 - acc: 0.9944 - val_loss: 0.3561 - val_acc: 0.8590
15664/15664 [==============================] - 0s 25us/step
==== Training loss, score are: 0.04242006542714764 0.9948289070480082 =======
3915/3915 [==============================] - 0s 26us/step
==== CV loss, score are: 0.356064371366915 0.8590038314937479 =======


===== Model: fast_text_char_gensim  ========:
 Cross-val log losses are: [0.36850243459488807, 0.36633918897628798, 0.36255233081937394, 0.36264624448035826, 0.35606436668685643]
====== Mean cross-val log loss is: 0.3632209131115529 =========


Timestamp: 2018-Jan-14 03:18:30
Running kfold training with model fast_text_char_glove
Shapes: x_train_raw.shape (19579, 42), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 41)
Running preprocess: &lt;function run_NN_preprocess at 0x7f620ee8eea0&gt;
No of unique words found is 247346
No of words occuring more than 2 times is 75104
No of unique words found is after capping is 247346
max sentence length is 1439
mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000
After padding: max sentence size is 161
After padding: mean, median length of sentence is 161.000000 161.000000
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>4954it [00:00, 49533.55it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Creating word embedding matrix for glove vectors
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>400000it [00:08, 49388.54it/s]
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Found 400000 word vectors.
of--mirth 247345
None
(247346, 100)
Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)
Running fold 1
Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_44 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
global_average_pooling1d_44  (None, 100)               0         
_________________________________________________________________
dense_44 (Dense)             (None, 3)                 303       
=================================================================
Total params: 24,734,903
Trainable params: 303
Non-trainable params: 24,734,600
_________________________________________________________________
15663/15663 [==============================] - 1s 73us/step
Before training loss, score are: 1.1214955906023782 0.28640745707107884
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 63us/step - loss: 1.0913 - acc: 0.3850 - val_loss: 1.0819 - val_acc: 0.3910
Epoch 2/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0740 - acc: 0.4084 - val_loss: 1.0707 - val_acc: 0.3956
Epoch 3/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0631 - acc: 0.4181 - val_loss: 1.0604 - val_acc: 0.4104
Epoch 4/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0530 - acc: 0.4373 - val_loss: 1.0505 - val_acc: 0.4367
Epoch 5/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0436 - acc: 0.4530 - val_loss: 1.0414 - val_acc: 0.4660
Epoch 6/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0350 - acc: 0.4691 - val_loss: 1.0330 - val_acc: 0.4888
Epoch 7/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0268 - acc: 0.4844 - val_loss: 1.0255 - val_acc: 0.4857
Epoch 8/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0193 - acc: 0.4950 - val_loss: 1.0184 - val_acc: 0.4918
Epoch 9/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0123 - acc: 0.5000 - val_loss: 1.0112 - val_acc: 0.5082
Epoch 10/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0057 - acc: 0.5106 - val_loss: 1.0049 - val_acc: 0.5151
Epoch 11/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9994 - acc: 0.5171 - val_loss: 0.9992 - val_acc: 0.5273
Epoch 12/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9934 - acc: 0.5264 - val_loss: 0.9931 - val_acc: 0.5324
Epoch 13/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9879 - acc: 0.5326 - val_loss: 0.9878 - val_acc: 0.5337
Epoch 14/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9825 - acc: 0.5368 - val_loss: 0.9823 - val_acc: 0.5383
Epoch 15/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9776 - acc: 0.5438 - val_loss: 0.9778 - val_acc: 0.5449
Epoch 16/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9727 - acc: 0.5473 - val_loss: 0.9727 - val_acc: 0.5529
Epoch 17/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9680 - acc: 0.5541 - val_loss: 0.9684 - val_acc: 0.5531
Epoch 18/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9636 - acc: 0.5585 - val_loss: 0.9642 - val_acc: 0.5554
Epoch 19/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9595 - acc: 0.5593 - val_loss: 0.9599 - val_acc: 0.5603
Epoch 20/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9554 - acc: 0.5662 - val_loss: 0.9560 - val_acc: 0.5644
Epoch 21/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9516 - acc: 0.5668 - val_loss: 0.9521 - val_acc: 0.5674
Epoch 22/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9478 - acc: 0.5731 - val_loss: 0.9485 - val_acc: 0.5692
Epoch 23/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9443 - acc: 0.5748 - val_loss: 0.9449 - val_acc: 0.5741
Epoch 24/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9409 - acc: 0.5778 - val_loss: 0.9417 - val_acc: 0.5753
Epoch 25/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9373 - acc: 0.5817 - val_loss: 0.9398 - val_acc: 0.5695
Epoch 26/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9344 - acc: 0.5811 - val_loss: 0.9352 - val_acc: 0.5774
Epoch 27/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9312 - acc: 0.5867 - val_loss: 0.9330 - val_acc: 0.5728
Epoch 28/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9282 - acc: 0.5847 - val_loss: 0.9290 - val_acc: 0.5850
Epoch 29/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9254 - acc: 0.5908 - val_loss: 0.9262 - val_acc: 0.5866
Epoch 30/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9225 - acc: 0.5899 - val_loss: 0.9237 - val_acc: 0.5909
Epoch 31/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9199 - acc: 0.5941 - val_loss: 0.9211 - val_acc: 0.5889
Epoch 32/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9173 - acc: 0.5943 - val_loss: 0.9184 - val_acc: 0.5912
Epoch 33/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9147 - acc: 0.5941 - val_loss: 0.9159 - val_acc: 0.5953
Epoch 34/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9122 - acc: 0.5980 - val_loss: 0.9139 - val_acc: 0.5922
Epoch 35/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9099 - acc: 0.5977 - val_loss: 0.9112 - val_acc: 0.5955
Epoch 36/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9075 - acc: 0.6003 - val_loss: 0.9093 - val_acc: 0.5955
Epoch 37/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9054 - acc: 0.6008 - val_loss: 0.9066 - val_acc: 0.5986
Epoch 38/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9033 - acc: 0.6028 - val_loss: 0.9043 - val_acc: 0.5981
Epoch 39/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9009 - acc: 0.6027 - val_loss: 0.9024 - val_acc: 0.5986
Epoch 40/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8991 - acc: 0.6057 - val_loss: 0.9005 - val_acc: 0.6006
Epoch 41/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8970 - acc: 0.6051 - val_loss: 0.8985 - val_acc: 0.5998
Epoch 42/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8949 - acc: 0.6068 - val_loss: 0.8964 - val_acc: 0.6062
Epoch 43/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8931 - acc: 0.6088 - val_loss: 0.8952 - val_acc: 0.6004
Epoch 44/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8914 - acc: 0.6075 - val_loss: 0.8928 - val_acc: 0.6044
Epoch 45/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8895 - acc: 0.6117 - val_loss: 0.8914 - val_acc: 0.6029
Epoch 46/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8879 - acc: 0.6108 - val_loss: 0.8892 - val_acc: 0.6090
Epoch 47/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8860 - acc: 0.6137 - val_loss: 0.8880 - val_acc: 0.6055
Epoch 48/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8846 - acc: 0.6122 - val_loss: 0.8860 - val_acc: 0.6103
Epoch 49/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8828 - acc: 0.6140 - val_loss: 0.8843 - val_acc: 0.6126
Epoch 50/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8813 - acc: 0.6156 - val_loss: 0.8827 - val_acc: 0.6144
Epoch 51/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8797 - acc: 0.6153 - val_loss: 0.8812 - val_acc: 0.6152
Epoch 52/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8781 - acc: 0.6181 - val_loss: 0.8799 - val_acc: 0.6157
Epoch 53/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8766 - acc: 0.6178 - val_loss: 0.8786 - val_acc: 0.6147
Epoch 54/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8753 - acc: 0.6192 - val_loss: 0.8769 - val_acc: 0.6159
Epoch 55/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8739 - acc: 0.6203 - val_loss: 0.8756 - val_acc: 0.6170
Epoch 56/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8725 - acc: 0.6211 - val_loss: 0.8747 - val_acc: 0.6159
Epoch 57/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8711 - acc: 0.6212 - val_loss: 0.8732 - val_acc: 0.6185
Epoch 58/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8700 - acc: 0.6217 - val_loss: 0.8719 - val_acc: 0.6221
Epoch 59/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8686 - acc: 0.6228 - val_loss: 0.8704 - val_acc: 0.6221
Epoch 60/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8674 - acc: 0.6234 - val_loss: 0.8692 - val_acc: 0.6216
Epoch 61/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8661 - acc: 0.6234 - val_loss: 0.8681 - val_acc: 0.6190
Epoch 62/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8648 - acc: 0.6244 - val_loss: 0.8671 - val_acc: 0.6193
Epoch 63/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8638 - acc: 0.6240 - val_loss: 0.8656 - val_acc: 0.6190
Epoch 64/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8625 - acc: 0.6249 - val_loss: 0.8646 - val_acc: 0.6236
Epoch 65/150
15663/15663 [==============================] - 0s 19us/step - loss: 0.8615 - acc: 0.6250 - val_loss: 0.8638 - val_acc: 0.6198
Epoch 66/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8603 - acc: 0.6262 - val_loss: 0.8625 - val_acc: 0.6251
Epoch 67/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8593 - acc: 0.6257 - val_loss: 0.8614 - val_acc: 0.6221
Epoch 68/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8583 - acc: 0.6283 - val_loss: 0.8608 - val_acc: 0.6261
Epoch 69/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8572 - acc: 0.6264 - val_loss: 0.8592 - val_acc: 0.6244
Epoch 70/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8562 - acc: 0.6291 - val_loss: 0.8584 - val_acc: 0.6269
Epoch 71/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8550 - acc: 0.6275 - val_loss: 0.8573 - val_acc: 0.6274
Epoch 72/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8542 - acc: 0.6288 - val_loss: 0.8569 - val_acc: 0.6239
Epoch 73/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8533 - acc: 0.6293 - val_loss: 0.8556 - val_acc: 0.6287
Epoch 74/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8522 - acc: 0.6287 - val_loss: 0.8550 - val_acc: 0.6264
Epoch 75/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8515 - acc: 0.6305 - val_loss: 0.8540 - val_acc: 0.6290
Epoch 76/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8505 - acc: 0.6295 - val_loss: 0.8529 - val_acc: 0.6295
Epoch 77/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8496 - acc: 0.6317 - val_loss: 0.8522 - val_acc: 0.6305
Epoch 78/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8487 - acc: 0.6304 - val_loss: 0.8515 - val_acc: 0.6305
Epoch 79/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8478 - acc: 0.6310 - val_loss: 0.8504 - val_acc: 0.6328
Epoch 80/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8469 - acc: 0.6316 - val_loss: 0.8506 - val_acc: 0.6315
Epoch 81/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8463 - acc: 0.6320 - val_loss: 0.8487 - val_acc: 0.6320
Epoch 82/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8454 - acc: 0.6314 - val_loss: 0.8483 - val_acc: 0.6313
Epoch 83/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8445 - acc: 0.6328 - val_loss: 0.8470 - val_acc: 0.6353
Epoch 84/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8437 - acc: 0.6324 - val_loss: 0.8461 - val_acc: 0.6364
Epoch 85/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8431 - acc: 0.6340 - val_loss: 0.8461 - val_acc: 0.6353
Epoch 86/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8423 - acc: 0.6337 - val_loss: 0.8450 - val_acc: 0.6361
Epoch 87/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8414 - acc: 0.6346 - val_loss: 0.8453 - val_acc: 0.6361
Epoch 88/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8410 - acc: 0.6329 - val_loss: 0.8435 - val_acc: 0.6356
Epoch 89/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8401 - acc: 0.6335 - val_loss: 0.8426 - val_acc: 0.6387
Epoch 90/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8393 - acc: 0.6346 - val_loss: 0.8420 - val_acc: 0.6379
Epoch 91/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8387 - acc: 0.6339 - val_loss: 0.8412 - val_acc: 0.6389
Epoch 92/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8380 - acc: 0.6351 - val_loss: 0.8407 - val_acc: 0.6382
Epoch 93/150
15663/15663 [==============================] - 0s 18us/step - loss: 0.8374 - acc: 0.6364 - val_loss: 0.8400 - val_acc: 0.6394
Epoch 94/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8368 - acc: 0.6344 - val_loss: 0.8393 - val_acc: 0.6382
Epoch 95/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8359 - acc: 0.6350 - val_loss: 0.8386 - val_acc: 0.6392
Epoch 96/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8353 - acc: 0.6371 - val_loss: 0.8383 - val_acc: 0.6389
Epoch 97/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8348 - acc: 0.6376 - val_loss: 0.8377 - val_acc: 0.6404
Epoch 98/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8341 - acc: 0.6368 - val_loss: 0.8371 - val_acc: 0.6394
Epoch 99/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8335 - acc: 0.6367 - val_loss: 0.8366 - val_acc: 0.6412
Epoch 100/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8328 - acc: 0.6383 - val_loss: 0.8360 - val_acc: 0.6430
Epoch 101/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8323 - acc: 0.6369 - val_loss: 0.8352 - val_acc: 0.6420
Epoch 102/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8316 - acc: 0.6386 - val_loss: 0.8350 - val_acc: 0.6443
Epoch 103/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8312 - acc: 0.6379 - val_loss: 0.8341 - val_acc: 0.6430
Epoch 104/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8305 - acc: 0.6391 - val_loss: 0.8335 - val_acc: 0.6427
Epoch 105/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8300 - acc: 0.6391 - val_loss: 0.8332 - val_acc: 0.6435
Epoch 106/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8294 - acc: 0.6387 - val_loss: 0.8326 - val_acc: 0.6440
Epoch 107/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8288 - acc: 0.6378 - val_loss: 0.8319 - val_acc: 0.6438
Epoch 108/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8284 - acc: 0.6395 - val_loss: 0.8315 - val_acc: 0.6448
Epoch 109/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8279 - acc: 0.6398 - val_loss: 0.8311 - val_acc: 0.6458
Epoch 110/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8274 - acc: 0.6403 - val_loss: 0.8306 - val_acc: 0.6450
Epoch 111/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8268 - acc: 0.6400 - val_loss: 0.8301 - val_acc: 0.6456
Epoch 112/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8263 - acc: 0.6411 - val_loss: 0.8297 - val_acc: 0.6463
Epoch 113/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8258 - acc: 0.6406 - val_loss: 0.8290 - val_acc: 0.6473
Epoch 114/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8253 - acc: 0.6425 - val_loss: 0.8287 - val_acc: 0.6481
Epoch 115/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8249 - acc: 0.6418 - val_loss: 0.8284 - val_acc: 0.6491
Epoch 116/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8244 - acc: 0.6413 - val_loss: 0.8277 - val_acc: 0.6461
Epoch 117/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8239 - acc: 0.6427 - val_loss: 0.8273 - val_acc: 0.6484
Epoch 118/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8235 - acc: 0.6422 - val_loss: 0.8269 - val_acc: 0.6499
Epoch 119/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8231 - acc: 0.6423 - val_loss: 0.8267 - val_acc: 0.6489
Epoch 120/150
15663/15663 [==============================] - 0s 20us/step - loss: 0.8225 - acc: 0.6420 - val_loss: 0.8264 - val_acc: 0.6486
Epoch 121/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8220 - acc: 0.6427 - val_loss: 0.8262 - val_acc: 0.6496
Epoch 122/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8216 - acc: 0.6430 - val_loss: 0.8255 - val_acc: 0.6504
Epoch 123/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8211 - acc: 0.6416 - val_loss: 0.8252 - val_acc: 0.6509
Epoch 124/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8207 - acc: 0.6423 - val_loss: 0.8243 - val_acc: 0.6499
Epoch 125/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8203 - acc: 0.6427 - val_loss: 0.8239 - val_acc: 0.6496
Epoch 126/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8199 - acc: 0.6435 - val_loss: 0.8238 - val_acc: 0.6519
Epoch 127/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8196 - acc: 0.6429 - val_loss: 0.8232 - val_acc: 0.6504
Epoch 128/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8190 - acc: 0.6439 - val_loss: 0.8233 - val_acc: 0.6514
Epoch 129/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8187 - acc: 0.6431 - val_loss: 0.8223 - val_acc: 0.6509
Epoch 130/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8183 - acc: 0.6439 - val_loss: 0.8220 - val_acc: 0.6522
Epoch 131/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8179 - acc: 0.6446 - val_loss: 0.8217 - val_acc: 0.6527
Epoch 132/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8175 - acc: 0.6438 - val_loss: 0.8213 - val_acc: 0.6512
Epoch 133/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8171 - acc: 0.6445 - val_loss: 0.8210 - val_acc: 0.6522
Epoch 134/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8168 - acc: 0.6447 - val_loss: 0.8207 - val_acc: 0.6527
Epoch 135/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8164 - acc: 0.6444 - val_loss: 0.8208 - val_acc: 0.6525
Epoch 136/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8159 - acc: 0.6458 - val_loss: 0.8203 - val_acc: 0.6519
Epoch 137/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8156 - acc: 0.6445 - val_loss: 0.8198 - val_acc: 0.6530
Epoch 138/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8153 - acc: 0.6452 - val_loss: 0.8195 - val_acc: 0.6527
Epoch 139/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8150 - acc: 0.6452 - val_loss: 0.8190 - val_acc: 0.6532
Epoch 140/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8145 - acc: 0.6457 - val_loss: 0.8192 - val_acc: 0.6522
Epoch 141/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8143 - acc: 0.6452 - val_loss: 0.8185 - val_acc: 0.6519
Epoch 142/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8139 - acc: 0.6457 - val_loss: 0.8181 - val_acc: 0.6530
Epoch 143/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8136 - acc: 0.6457 - val_loss: 0.8177 - val_acc: 0.6542
Epoch 144/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8133 - acc: 0.6462 - val_loss: 0.8175 - val_acc: 0.6532
Epoch 145/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8128 - acc: 0.6469 - val_loss: 0.8172 - val_acc: 0.6535
Epoch 146/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8125 - acc: 0.6471 - val_loss: 0.8173 - val_acc: 0.6525
Epoch 147/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8123 - acc: 0.6454 - val_loss: 0.8165 - val_acc: 0.6542
Epoch 148/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8119 - acc: 0.6466 - val_loss: 0.8162 - val_acc: 0.6540
Epoch 149/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8117 - acc: 0.6471 - val_loss: 0.8161 - val_acc: 0.6540
Epoch 150/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8113 - acc: 0.6462 - val_loss: 0.8160 - val_acc: 0.6537
15663/15663 [==============================] - 0s 26us/step
==== Training loss, score are: 0.8109515775119954 0.6466194215933878 =======
3916/3916 [==============================] - 0s 25us/step
==== CV loss, score are: 0.8159530744245761 0.6537282942386156 =======
Running fold 2
Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_45 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
global_average_pooling1d_45  (None, 100)               0         
_________________________________________________________________
dense_45 (Dense)             (None, 3)                 303       
=================================================================
Total params: 24,734,903
Trainable params: 303
Non-trainable params: 24,734,600
_________________________________________________________________
15663/15663 [==============================] - 1s 72us/step
Before training loss, score are: 1.1027436079912272 0.2971972163831077
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 62us/step - loss: 1.0891 - acc: 0.3957 - val_loss: 1.0830 - val_acc: 0.3943
Epoch 2/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0751 - acc: 0.4061 - val_loss: 1.0711 - val_acc: 0.3950
Epoch 3/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0633 - acc: 0.4152 - val_loss: 1.0612 - val_acc: 0.3981
Epoch 4/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0526 - acc: 0.4289 - val_loss: 1.0511 - val_acc: 0.4377
Epoch 5/150
15663/15663 [==============================] - 0s 15us/step - loss: 1.0430 - acc: 0.4550 - val_loss: 1.0425 - val_acc: 0.4517
Epoch 6/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0340 - acc: 0.4695 - val_loss: 1.0342 - val_acc: 0.4696
Epoch 7/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0257 - acc: 0.4878 - val_loss: 1.0268 - val_acc: 0.4768
Epoch 8/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0181 - acc: 0.4992 - val_loss: 1.0195 - val_acc: 0.4903
Epoch 9/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0108 - acc: 0.5090 - val_loss: 1.0130 - val_acc: 0.4923
Epoch 10/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0041 - acc: 0.5175 - val_loss: 1.0074 - val_acc: 0.4923
Epoch 11/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9979 - acc: 0.5217 - val_loss: 1.0010 - val_acc: 0.5033
Epoch 12/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9918 - acc: 0.5314 - val_loss: 0.9949 - val_acc: 0.5217
Epoch 13/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9861 - acc: 0.5379 - val_loss: 0.9894 - val_acc: 0.5386
Epoch 14/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9809 - acc: 0.5427 - val_loss: 0.9845 - val_acc: 0.5360
Epoch 15/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9757 - acc: 0.5477 - val_loss: 0.9796 - val_acc: 0.5485
Epoch 16/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9708 - acc: 0.5547 - val_loss: 0.9751 - val_acc: 0.5513
Epoch 17/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9662 - acc: 0.5614 - val_loss: 0.9712 - val_acc: 0.5539
Epoch 18/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.9618 - acc: 0.5628 - val_loss: 0.9665 - val_acc: 0.5564
Epoch 19/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.9576 - acc: 0.5664 - val_loss: 0.9627 - val_acc: 0.5564
Epoch 20/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9535 - acc: 0.5705 - val_loss: 0.9588 - val_acc: 0.5600
Epoch 21/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9496 - acc: 0.5708 - val_loss: 0.9554 - val_acc: 0.5649
Epoch 22/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9460 - acc: 0.5745 - val_loss: 0.9517 - val_acc: 0.5651
Epoch 23/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9423 - acc: 0.5765 - val_loss: 0.9481 - val_acc: 0.5705
Epoch 24/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9389 - acc: 0.5802 - val_loss: 0.9448 - val_acc: 0.5728
Epoch 25/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9354 - acc: 0.5814 - val_loss: 0.9418 - val_acc: 0.5751
Epoch 26/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9322 - acc: 0.5841 - val_loss: 0.9387 - val_acc: 0.5784
Epoch 27/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9291 - acc: 0.5867 - val_loss: 0.9352 - val_acc: 0.5830
Epoch 28/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9260 - acc: 0.5895 - val_loss: 0.9325 - val_acc: 0.5845
Epoch 29/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9231 - acc: 0.5948 - val_loss: 0.9300 - val_acc: 0.5840
Epoch 30/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9203 - acc: 0.5924 - val_loss: 0.9269 - val_acc: 0.5855
Epoch 31/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9176 - acc: 0.5967 - val_loss: 0.9244 - val_acc: 0.5878
Epoch 32/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9150 - acc: 0.5980 - val_loss: 0.9219 - val_acc: 0.5889
Epoch 33/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9124 - acc: 0.5996 - val_loss: 0.9197 - val_acc: 0.5863
Epoch 34/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9099 - acc: 0.6019 - val_loss: 0.9175 - val_acc: 0.5876
Epoch 35/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9076 - acc: 0.6042 - val_loss: 0.9149 - val_acc: 0.5919
Epoch 36/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.9051 - acc: 0.6048 - val_loss: 0.9125 - val_acc: 0.5978
Epoch 37/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9029 - acc: 0.6054 - val_loss: 0.9103 - val_acc: 0.5986
Epoch 38/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9007 - acc: 0.6091 - val_loss: 0.9089 - val_acc: 0.5907
Epoch 39/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8986 - acc: 0.6076 - val_loss: 0.9063 - val_acc: 0.6034
Epoch 40/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8964 - acc: 0.6139 - val_loss: 0.9047 - val_acc: 0.5998
Epoch 41/150
15663/15663 [==============================] - 0s 18us/step - loss: 0.8944 - acc: 0.6097 - val_loss: 0.9027 - val_acc: 0.6034
Epoch 42/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8925 - acc: 0.6125 - val_loss: 0.9005 - val_acc: 0.6034
Epoch 43/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8905 - acc: 0.6137 - val_loss: 0.8987 - val_acc: 0.6037
Epoch 44/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8887 - acc: 0.6144 - val_loss: 0.8968 - val_acc: 0.6065
Epoch 45/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8869 - acc: 0.6164 - val_loss: 0.8954 - val_acc: 0.6060
Epoch 46/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8850 - acc: 0.6155 - val_loss: 0.8937 - val_acc: 0.6062
Epoch 47/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8833 - acc: 0.6158 - val_loss: 0.8921 - val_acc: 0.6070
Epoch 48/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8818 - acc: 0.6173 - val_loss: 0.8907 - val_acc: 0.6108
Epoch 49/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8802 - acc: 0.6180 - val_loss: 0.8889 - val_acc: 0.6098
Epoch 50/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8784 - acc: 0.6180 - val_loss: 0.8874 - val_acc: 0.6108
Epoch 51/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8770 - acc: 0.6190 - val_loss: 0.8858 - val_acc: 0.6113
Epoch 52/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8754 - acc: 0.6192 - val_loss: 0.8852 - val_acc: 0.6124
Epoch 53/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8740 - acc: 0.6212 - val_loss: 0.8831 - val_acc: 0.6118
Epoch 54/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8726 - acc: 0.6205 - val_loss: 0.8818 - val_acc: 0.6131
Epoch 55/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8712 - acc: 0.6213 - val_loss: 0.8807 - val_acc: 0.6134
Epoch 56/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8698 - acc: 0.6212 - val_loss: 0.8790 - val_acc: 0.6157
Epoch 57/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8685 - acc: 0.6231 - val_loss: 0.8779 - val_acc: 0.6162
Epoch 58/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8672 - acc: 0.6238 - val_loss: 0.8766 - val_acc: 0.6172
Epoch 59/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8658 - acc: 0.6238 - val_loss: 0.8753 - val_acc: 0.6172
Epoch 60/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8646 - acc: 0.6246 - val_loss: 0.8744 - val_acc: 0.6167
Epoch 61/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8634 - acc: 0.6265 - val_loss: 0.8736 - val_acc: 0.6162
Epoch 62/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8622 - acc: 0.6272 - val_loss: 0.8723 - val_acc: 0.6177
Epoch 63/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8611 - acc: 0.6259 - val_loss: 0.8709 - val_acc: 0.6200
Epoch 64/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8599 - acc: 0.6261 - val_loss: 0.8699 - val_acc: 0.6175
Epoch 65/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8588 - acc: 0.6267 - val_loss: 0.8689 - val_acc: 0.6203
Epoch 66/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8577 - acc: 0.6287 - val_loss: 0.8680 - val_acc: 0.6187
Epoch 67/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8566 - acc: 0.6284 - val_loss: 0.8668 - val_acc: 0.6190
Epoch 68/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8556 - acc: 0.6284 - val_loss: 0.8659 - val_acc: 0.6177
Epoch 69/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8545 - acc: 0.6297 - val_loss: 0.8651 - val_acc: 0.6193
Epoch 70/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8536 - acc: 0.6292 - val_loss: 0.8639 - val_acc: 0.6170
Epoch 71/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8526 - acc: 0.6303 - val_loss: 0.8630 - val_acc: 0.6218
Epoch 72/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8515 - acc: 0.6303 - val_loss: 0.8620 - val_acc: 0.6210
Epoch 73/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8505 - acc: 0.6313 - val_loss: 0.8615 - val_acc: 0.6198
Epoch 74/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8496 - acc: 0.6319 - val_loss: 0.8607 - val_acc: 0.6223
Epoch 75/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8487 - acc: 0.6316 - val_loss: 0.8593 - val_acc: 0.6236
Epoch 76/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8478 - acc: 0.6323 - val_loss: 0.8587 - val_acc: 0.6218
Epoch 77/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8469 - acc: 0.6324 - val_loss: 0.8584 - val_acc: 0.6216
Epoch 78/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8460 - acc: 0.6328 - val_loss: 0.8568 - val_acc: 0.6239
Epoch 79/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8452 - acc: 0.6344 - val_loss: 0.8564 - val_acc: 0.6239
Epoch 80/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8444 - acc: 0.6342 - val_loss: 0.8554 - val_acc: 0.6223
Epoch 81/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8435 - acc: 0.6365 - val_loss: 0.8547 - val_acc: 0.6261
Epoch 82/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8428 - acc: 0.6341 - val_loss: 0.8536 - val_acc: 0.6246
Epoch 83/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8419 - acc: 0.6358 - val_loss: 0.8528 - val_acc: 0.6244
Epoch 84/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8412 - acc: 0.6374 - val_loss: 0.8528 - val_acc: 0.6236
Epoch 85/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8404 - acc: 0.6362 - val_loss: 0.8518 - val_acc: 0.6246
Epoch 86/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8396 - acc: 0.6355 - val_loss: 0.8510 - val_acc: 0.6261
Epoch 87/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8388 - acc: 0.6388 - val_loss: 0.8503 - val_acc: 0.6256
Epoch 88/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8382 - acc: 0.6383 - val_loss: 0.8494 - val_acc: 0.6251
Epoch 89/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8375 - acc: 0.6388 - val_loss: 0.8489 - val_acc: 0.6236
Epoch 90/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8367 - acc: 0.6371 - val_loss: 0.8482 - val_acc: 0.6236
Epoch 91/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8362 - acc: 0.6392 - val_loss: 0.8474 - val_acc: 0.6279
Epoch 92/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8355 - acc: 0.6399 - val_loss: 0.8473 - val_acc: 0.6269
Epoch 93/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8347 - acc: 0.6384 - val_loss: 0.8461 - val_acc: 0.6256
Epoch 94/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8341 - acc: 0.6405 - val_loss: 0.8462 - val_acc: 0.6264
Epoch 95/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8334 - acc: 0.6398 - val_loss: 0.8449 - val_acc: 0.6261
Epoch 96/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8328 - acc: 0.6416 - val_loss: 0.8451 - val_acc: 0.6254
Epoch 97/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8322 - acc: 0.6403 - val_loss: 0.8438 - val_acc: 0.6267
Epoch 98/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8316 - acc: 0.6412 - val_loss: 0.8435 - val_acc: 0.6261
Epoch 99/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8308 - acc: 0.6431 - val_loss: 0.8431 - val_acc: 0.6241
Epoch 100/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8303 - acc: 0.6414 - val_loss: 0.8422 - val_acc: 0.6272
Epoch 101/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8298 - acc: 0.6414 - val_loss: 0.8415 - val_acc: 0.6269
Epoch 102/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8292 - acc: 0.6422 - val_loss: 0.8417 - val_acc: 0.6259
Epoch 103/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8287 - acc: 0.6423 - val_loss: 0.8408 - val_acc: 0.6261
Epoch 104/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8281 - acc: 0.6428 - val_loss: 0.8400 - val_acc: 0.6272
Epoch 105/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8275 - acc: 0.6434 - val_loss: 0.8399 - val_acc: 0.6267
Epoch 106/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8269 - acc: 0.6429 - val_loss: 0.8392 - val_acc: 0.6287
Epoch 107/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8266 - acc: 0.6439 - val_loss: 0.8386 - val_acc: 0.6269
Epoch 108/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8259 - acc: 0.6446 - val_loss: 0.8383 - val_acc: 0.6241
Epoch 109/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8253 - acc: 0.6439 - val_loss: 0.8375 - val_acc: 0.6287
Epoch 110/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8250 - acc: 0.6452 - val_loss: 0.8373 - val_acc: 0.6267
Epoch 111/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8244 - acc: 0.6445 - val_loss: 0.8371 - val_acc: 0.6256
Epoch 112/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8239 - acc: 0.6436 - val_loss: 0.8361 - val_acc: 0.6287
Epoch 113/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8235 - acc: 0.6450 - val_loss: 0.8360 - val_acc: 0.6277
Epoch 114/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8229 - acc: 0.6462 - val_loss: 0.8357 - val_acc: 0.6264
Epoch 115/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8225 - acc: 0.6451 - val_loss: 0.8350 - val_acc: 0.6272
Epoch 116/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8220 - acc: 0.6468 - val_loss: 0.8346 - val_acc: 0.6274
Epoch 117/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8215 - acc: 0.6464 - val_loss: 0.8341 - val_acc: 0.6284
Epoch 118/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8211 - acc: 0.6459 - val_loss: 0.8335 - val_acc: 0.6290
Epoch 119/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8206 - acc: 0.6464 - val_loss: 0.8335 - val_acc: 0.6279
Epoch 120/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8201 - acc: 0.6472 - val_loss: 0.8330 - val_acc: 0.6287
Epoch 121/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8198 - acc: 0.6475 - val_loss: 0.8325 - val_acc: 0.6284
Epoch 122/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8195 - acc: 0.6462 - val_loss: 0.8320 - val_acc: 0.6292
Epoch 123/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8189 - acc: 0.6475 - val_loss: 0.8316 - val_acc: 0.6292
Epoch 124/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8185 - acc: 0.6480 - val_loss: 0.8312 - val_acc: 0.6292
Epoch 125/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8181 - acc: 0.6473 - val_loss: 0.8311 - val_acc: 0.6300
Epoch 126/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8176 - acc: 0.6486 - val_loss: 0.8307 - val_acc: 0.6292
Epoch 127/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8171 - acc: 0.6481 - val_loss: 0.8303 - val_acc: 0.6320
Epoch 128/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8170 - acc: 0.6482 - val_loss: 0.8299 - val_acc: 0.6302
Epoch 129/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8166 - acc: 0.6476 - val_loss: 0.8295 - val_acc: 0.6302
Epoch 130/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8162 - acc: 0.6489 - val_loss: 0.8293 - val_acc: 0.6315
Epoch 131/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8157 - acc: 0.6483 - val_loss: 0.8289 - val_acc: 0.6302
Epoch 132/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8153 - acc: 0.6488 - val_loss: 0.8285 - val_acc: 0.6310
Epoch 133/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8149 - acc: 0.6485 - val_loss: 0.8282 - val_acc: 0.6315
Epoch 134/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8146 - acc: 0.6484 - val_loss: 0.8277 - val_acc: 0.6302
Epoch 135/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8142 - acc: 0.6496 - val_loss: 0.8276 - val_acc: 0.6313
Epoch 136/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8138 - acc: 0.6489 - val_loss: 0.8279 - val_acc: 0.6292
Epoch 137/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8134 - acc: 0.6488 - val_loss: 0.8267 - val_acc: 0.6320
Epoch 138/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8132 - acc: 0.6503 - val_loss: 0.8264 - val_acc: 0.6318
Epoch 139/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8127 - acc: 0.6494 - val_loss: 0.8260 - val_acc: 0.6336
Epoch 140/150
15663/15663 [==============================] - 0s 19us/step - loss: 0.8124 - acc: 0.6500 - val_loss: 0.8259 - val_acc: 0.6328
Epoch 141/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8122 - acc: 0.6494 - val_loss: 0.8256 - val_acc: 0.6328
Epoch 142/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8118 - acc: 0.6498 - val_loss: 0.8254 - val_acc: 0.6320
Epoch 143/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8114 - acc: 0.6506 - val_loss: 0.8251 - val_acc: 0.6333
Epoch 144/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8111 - acc: 0.6505 - val_loss: 0.8247 - val_acc: 0.6323
Epoch 145/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8108 - acc: 0.6510 - val_loss: 0.8243 - val_acc: 0.6325
Epoch 146/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8105 - acc: 0.6505 - val_loss: 0.8244 - val_acc: 0.6333
Epoch 147/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8102 - acc: 0.6506 - val_loss: 0.8239 - val_acc: 0.6325
Epoch 148/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8098 - acc: 0.6517 - val_loss: 0.8236 - val_acc: 0.6348
Epoch 149/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8095 - acc: 0.6517 - val_loss: 0.8230 - val_acc: 0.6343
Epoch 150/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8093 - acc: 0.6498 - val_loss: 0.8231 - val_acc: 0.6320
15663/15663 [==============================] - 0s 28us/step
==== Training loss, score are: 0.808918109964093 0.6514716210443231 =======
3916/3916 [==============================] - 0s 27us/step
==== CV loss, score are: 0.8230905103610412 0.6320224719101124 =======
Running fold 3
Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_46 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
global_average_pooling1d_46  (None, 100)               0         
_________________________________________________________________
dense_46 (Dense)             (None, 3)                 303       
=================================================================
Total params: 24,734,903
Trainable params: 303
Non-trainable params: 24,734,600
_________________________________________________________________
15663/15663 [==============================] - 1s 76us/step
Before training loss, score are: 1.110431992002262 0.28742897274496
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 67us/step - loss: 1.0920 - acc: 0.3882 - val_loss: 1.0842 - val_acc: 0.3996
Epoch 2/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0770 - acc: 0.4052 - val_loss: 1.0723 - val_acc: 0.4045
Epoch 3/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0654 - acc: 0.4194 - val_loss: 1.0615 - val_acc: 0.4323
Epoch 4/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0548 - acc: 0.4371 - val_loss: 1.0516 - val_acc: 0.4433
Epoch 5/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0452 - acc: 0.4547 - val_loss: 1.0425 - val_acc: 0.4563
Epoch 6/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0362 - acc: 0.4656 - val_loss: 1.0340 - val_acc: 0.4742
Epoch 7/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0279 - acc: 0.4832 - val_loss: 1.0264 - val_acc: 0.4734
Epoch 8/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0201 - acc: 0.4949 - val_loss: 1.0190 - val_acc: 0.4895
Epoch 9/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0131 - acc: 0.5012 - val_loss: 1.0121 - val_acc: 0.4959
Epoch 10/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0063 - acc: 0.5125 - val_loss: 1.0056 - val_acc: 0.5046
Epoch 11/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0001 - acc: 0.5213 - val_loss: 0.9995 - val_acc: 0.5163
Epoch 12/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9940 - acc: 0.5317 - val_loss: 0.9938 - val_acc: 0.5161
Epoch 13/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9882 - acc: 0.5353 - val_loss: 0.9883 - val_acc: 0.5235
Epoch 14/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9829 - acc: 0.5390 - val_loss: 0.9830 - val_acc: 0.5444
Epoch 15/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9777 - acc: 0.5507 - val_loss: 0.9780 - val_acc: 0.5455
Epoch 16/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9728 - acc: 0.5539 - val_loss: 0.9732 - val_acc: 0.5521
Epoch 17/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9681 - acc: 0.5600 - val_loss: 0.9687 - val_acc: 0.5523
Epoch 18/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9636 - acc: 0.5613 - val_loss: 0.9643 - val_acc: 0.5544
Epoch 19/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9592 - acc: 0.5682 - val_loss: 0.9604 - val_acc: 0.5536
Epoch 20/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9551 - acc: 0.5697 - val_loss: 0.9563 - val_acc: 0.5587
Epoch 21/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9512 - acc: 0.5740 - val_loss: 0.9526 - val_acc: 0.5649
Epoch 22/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9473 - acc: 0.5773 - val_loss: 0.9488 - val_acc: 0.5669
Epoch 23/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9438 - acc: 0.5779 - val_loss: 0.9452 - val_acc: 0.5695
Epoch 24/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9402 - acc: 0.5808 - val_loss: 0.9418 - val_acc: 0.5720
Epoch 25/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9368 - acc: 0.5835 - val_loss: 0.9386 - val_acc: 0.5771
Epoch 26/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9336 - acc: 0.5879 - val_loss: 0.9356 - val_acc: 0.5725
Epoch 27/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9304 - acc: 0.5877 - val_loss: 0.9324 - val_acc: 0.5820
Epoch 28/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9275 - acc: 0.5931 - val_loss: 0.9297 - val_acc: 0.5794
Epoch 29/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9245 - acc: 0.5931 - val_loss: 0.9267 - val_acc: 0.5792
Epoch 30/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9216 - acc: 0.5958 - val_loss: 0.9240 - val_acc: 0.5809
Epoch 31/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9190 - acc: 0.5978 - val_loss: 0.9214 - val_acc: 0.5843
Epoch 32/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9164 - acc: 0.5992 - val_loss: 0.9189 - val_acc: 0.5850
Epoch 33/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9138 - acc: 0.6017 - val_loss: 0.9164 - val_acc: 0.5873
Epoch 34/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9113 - acc: 0.6022 - val_loss: 0.9142 - val_acc: 0.5884
Epoch 35/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9090 - acc: 0.6043 - val_loss: 0.9119 - val_acc: 0.5901
Epoch 36/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9066 - acc: 0.6045 - val_loss: 0.9097 - val_acc: 0.5894
Epoch 37/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9044 - acc: 0.6052 - val_loss: 0.9074 - val_acc: 0.5912
Epoch 38/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9021 - acc: 0.6085 - val_loss: 0.9059 - val_acc: 0.5917
Epoch 39/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9001 - acc: 0.6083 - val_loss: 0.9033 - val_acc: 0.5935
Epoch 40/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8980 - acc: 0.6101 - val_loss: 0.9014 - val_acc: 0.5937
Epoch 41/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8961 - acc: 0.6112 - val_loss: 0.8994 - val_acc: 0.5953
Epoch 42/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8940 - acc: 0.6111 - val_loss: 0.8976 - val_acc: 0.5965
Epoch 43/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8921 - acc: 0.6125 - val_loss: 0.8959 - val_acc: 0.5958
Epoch 44/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8903 - acc: 0.6141 - val_loss: 0.8940 - val_acc: 0.6037
Epoch 45/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8885 - acc: 0.6161 - val_loss: 0.8923 - val_acc: 0.6009
Epoch 46/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8868 - acc: 0.6155 - val_loss: 0.8907 - val_acc: 0.6050
Epoch 47/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8850 - acc: 0.6173 - val_loss: 0.8893 - val_acc: 0.6004
Epoch 48/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8833 - acc: 0.6181 - val_loss: 0.8874 - val_acc: 0.6034
Epoch 49/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8818 - acc: 0.6190 - val_loss: 0.8860 - val_acc: 0.6062
Epoch 50/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8800 - acc: 0.6204 - val_loss: 0.8847 - val_acc: 0.6055
Epoch 51/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8785 - acc: 0.6202 - val_loss: 0.8830 - val_acc: 0.6083
Epoch 52/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8769 - acc: 0.6218 - val_loss: 0.8815 - val_acc: 0.6073
Epoch 53/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8754 - acc: 0.6221 - val_loss: 0.8802 - val_acc: 0.6062
Epoch 54/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8740 - acc: 0.6235 - val_loss: 0.8789 - val_acc: 0.6070
Epoch 55/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8726 - acc: 0.6230 - val_loss: 0.8776 - val_acc: 0.6067
Epoch 56/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8713 - acc: 0.6233 - val_loss: 0.8762 - val_acc: 0.6075
Epoch 57/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8698 - acc: 0.6247 - val_loss: 0.8752 - val_acc: 0.6080
Epoch 58/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8686 - acc: 0.6245 - val_loss: 0.8737 - val_acc: 0.6090
Epoch 59/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8673 - acc: 0.6248 - val_loss: 0.8730 - val_acc: 0.6075
Epoch 60/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8661 - acc: 0.6259 - val_loss: 0.8713 - val_acc: 0.6090
Epoch 61/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8648 - acc: 0.6246 - val_loss: 0.8703 - val_acc: 0.6098
Epoch 62/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8636 - acc: 0.6258 - val_loss: 0.8691 - val_acc: 0.6111
Epoch 63/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8624 - acc: 0.6273 - val_loss: 0.8680 - val_acc: 0.6118
Epoch 64/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8612 - acc: 0.6296 - val_loss: 0.8670 - val_acc: 0.6134
Epoch 65/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8602 - acc: 0.6282 - val_loss: 0.8660 - val_acc: 0.6124
Epoch 66/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8590 - acc: 0.6291 - val_loss: 0.8652 - val_acc: 0.6093
Epoch 67/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8579 - acc: 0.6291 - val_loss: 0.8638 - val_acc: 0.6157
Epoch 68/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8568 - acc: 0.6301 - val_loss: 0.8629 - val_acc: 0.6141
Epoch 69/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8558 - acc: 0.6296 - val_loss: 0.8619 - val_acc: 0.6162
Epoch 70/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8548 - acc: 0.6305 - val_loss: 0.8610 - val_acc: 0.6162
Epoch 71/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8537 - acc: 0.6308 - val_loss: 0.8601 - val_acc: 0.6167
Epoch 72/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8528 - acc: 0.6320 - val_loss: 0.8591 - val_acc: 0.6180
Epoch 73/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8519 - acc: 0.6312 - val_loss: 0.8584 - val_acc: 0.6154
Epoch 74/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8508 - acc: 0.6333 - val_loss: 0.8574 - val_acc: 0.6193
Epoch 75/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8500 - acc: 0.6333 - val_loss: 0.8567 - val_acc: 0.6170
Epoch 76/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8490 - acc: 0.6341 - val_loss: 0.8560 - val_acc: 0.6167
Epoch 77/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8481 - acc: 0.6337 - val_loss: 0.8550 - val_acc: 0.6187
Epoch 78/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8473 - acc: 0.6334 - val_loss: 0.8542 - val_acc: 0.6185
Epoch 79/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8465 - acc: 0.6346 - val_loss: 0.8533 - val_acc: 0.6210
Epoch 80/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8455 - acc: 0.6355 - val_loss: 0.8526 - val_acc: 0.6213
Epoch 81/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8447 - acc: 0.6365 - val_loss: 0.8521 - val_acc: 0.6182
Epoch 82/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8439 - acc: 0.6357 - val_loss: 0.8512 - val_acc: 0.6195
Epoch 83/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8431 - acc: 0.6358 - val_loss: 0.8503 - val_acc: 0.6216
Epoch 84/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8423 - acc: 0.6369 - val_loss: 0.8497 - val_acc: 0.6213
Epoch 85/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8416 - acc: 0.6371 - val_loss: 0.8489 - val_acc: 0.6221
Epoch 86/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8408 - acc: 0.6376 - val_loss: 0.8483 - val_acc: 0.6241
Epoch 87/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8400 - acc: 0.6374 - val_loss: 0.8475 - val_acc: 0.6246
Epoch 88/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8394 - acc: 0.6386 - val_loss: 0.8469 - val_acc: 0.6223
Epoch 89/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8387 - acc: 0.6383 - val_loss: 0.8462 - val_acc: 0.6241
Epoch 90/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8378 - acc: 0.6385 - val_loss: 0.8456 - val_acc: 0.6244
Epoch 91/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8369 - acc: 0.6384 - val_loss: 0.8453 - val_acc: 0.6190
Epoch 92/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8366 - acc: 0.6402 - val_loss: 0.8444 - val_acc: 0.6218
Epoch 93/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8357 - acc: 0.6407 - val_loss: 0.8442 - val_acc: 0.6213
Epoch 94/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8351 - acc: 0.6400 - val_loss: 0.8433 - val_acc: 0.6236
Epoch 95/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8344 - acc: 0.6394 - val_loss: 0.8426 - val_acc: 0.6231
Epoch 96/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8339 - acc: 0.6394 - val_loss: 0.8420 - val_acc: 0.6236
Epoch 97/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8332 - acc: 0.6410 - val_loss: 0.8414 - val_acc: 0.6259
Epoch 98/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8327 - acc: 0.6413 - val_loss: 0.8411 - val_acc: 0.6246
Epoch 99/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8320 - acc: 0.6409 - val_loss: 0.8403 - val_acc: 0.6249
Epoch 100/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8313 - acc: 0.6421 - val_loss: 0.8399 - val_acc: 0.6239
Epoch 101/150
15663/15663 [==============================] - 0s 21us/step - loss: 0.8307 - acc: 0.6422 - val_loss: 0.8392 - val_acc: 0.6251
Epoch 102/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8301 - acc: 0.6429 - val_loss: 0.8391 - val_acc: 0.6239
Epoch 103/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8296 - acc: 0.6429 - val_loss: 0.8383 - val_acc: 0.6249
Epoch 104/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8290 - acc: 0.6426 - val_loss: 0.8377 - val_acc: 0.6236
Epoch 105/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8284 - acc: 0.6430 - val_loss: 0.8373 - val_acc: 0.6254
Epoch 106/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8278 - acc: 0.6443 - val_loss: 0.8367 - val_acc: 0.6249
Epoch 107/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8272 - acc: 0.6429 - val_loss: 0.8366 - val_acc: 0.6267
Epoch 108/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8266 - acc: 0.6423 - val_loss: 0.8362 - val_acc: 0.6261
Epoch 109/150
15663/15663 [==============================] - 0s 18us/step - loss: 0.8262 - acc: 0.6423 - val_loss: 0.8354 - val_acc: 0.6267
Epoch 110/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8257 - acc: 0.6440 - val_loss: 0.8349 - val_acc: 0.6269
Epoch 111/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8252 - acc: 0.6435 - val_loss: 0.8344 - val_acc: 0.6274
Epoch 112/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8247 - acc: 0.6449 - val_loss: 0.8341 - val_acc: 0.6277
Epoch 113/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8243 - acc: 0.6442 - val_loss: 0.8335 - val_acc: 0.6277
Epoch 114/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8237 - acc: 0.6442 - val_loss: 0.8331 - val_acc: 0.6284
Epoch 115/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8232 - acc: 0.6443 - val_loss: 0.8326 - val_acc: 0.6292
Epoch 116/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8227 - acc: 0.6448 - val_loss: 0.8324 - val_acc: 0.6284
Epoch 117/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8223 - acc: 0.6450 - val_loss: 0.8319 - val_acc: 0.6256
Epoch 118/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8218 - acc: 0.6459 - val_loss: 0.8314 - val_acc: 0.6272
Epoch 119/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8215 - acc: 0.6454 - val_loss: 0.8310 - val_acc: 0.6272
Epoch 120/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8209 - acc: 0.6464 - val_loss: 0.8306 - val_acc: 0.6292
Epoch 121/150
15663/15663 [==============================] - 0s 18us/step - loss: 0.8205 - acc: 0.6457 - val_loss: 0.8301 - val_acc: 0.6305
Epoch 122/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8200 - acc: 0.6457 - val_loss: 0.8299 - val_acc: 0.6302
Epoch 123/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8195 - acc: 0.6465 - val_loss: 0.8294 - val_acc: 0.6297
Epoch 124/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8191 - acc: 0.6467 - val_loss: 0.8291 - val_acc: 0.6307
Epoch 125/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8187 - acc: 0.6469 - val_loss: 0.8286 - val_acc: 0.6297
Epoch 126/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8183 - acc: 0.6467 - val_loss: 0.8284 - val_acc: 0.6305
Epoch 127/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8179 - acc: 0.6474 - val_loss: 0.8279 - val_acc: 0.6315
Epoch 128/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8174 - acc: 0.6485 - val_loss: 0.8277 - val_acc: 0.6315
Epoch 129/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8171 - acc: 0.6466 - val_loss: 0.8277 - val_acc: 0.6300
Epoch 130/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8166 - acc: 0.6480 - val_loss: 0.8270 - val_acc: 0.6305
Epoch 131/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8162 - acc: 0.6476 - val_loss: 0.8266 - val_acc: 0.6323
Epoch 132/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8159 - acc: 0.6485 - val_loss: 0.8265 - val_acc: 0.6297
Epoch 133/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8155 - acc: 0.6473 - val_loss: 0.8259 - val_acc: 0.6313
Epoch 134/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8152 - acc: 0.6478 - val_loss: 0.8255 - val_acc: 0.6307
Epoch 135/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8147 - acc: 0.6487 - val_loss: 0.8252 - val_acc: 0.6323
Epoch 136/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8143 - acc: 0.6488 - val_loss: 0.8249 - val_acc: 0.6313
Epoch 137/150
15663/15663 [==============================] - 0s 18us/step - loss: 0.8140 - acc: 0.6496 - val_loss: 0.8247 - val_acc: 0.6318
Epoch 138/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8136 - acc: 0.6492 - val_loss: 0.8244 - val_acc: 0.6315
Epoch 139/150
15663/15663 [==============================] - 0s 18us/step - loss: 0.8133 - acc: 0.6490 - val_loss: 0.8241 - val_acc: 0.6320
Epoch 140/150
15663/15663 [==============================] - 0s 18us/step - loss: 0.8129 - acc: 0.6496 - val_loss: 0.8236 - val_acc: 0.6318
Epoch 141/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8125 - acc: 0.6503 - val_loss: 0.8235 - val_acc: 0.6318
Epoch 142/150
15663/15663 [==============================] - 0s 20us/step - loss: 0.8123 - acc: 0.6499 - val_loss: 0.8232 - val_acc: 0.6320
Epoch 143/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8118 - acc: 0.6496 - val_loss: 0.8227 - val_acc: 0.6313
Epoch 144/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8116 - acc: 0.6507 - val_loss: 0.8225 - val_acc: 0.6310
Epoch 145/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8112 - acc: 0.6504 - val_loss: 0.8224 - val_acc: 0.6318
Epoch 146/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8108 - acc: 0.6504 - val_loss: 0.8219 - val_acc: 0.6313
Epoch 147/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8106 - acc: 0.6500 - val_loss: 0.8216 - val_acc: 0.6320
Epoch 148/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8103 - acc: 0.6515 - val_loss: 0.8215 - val_acc: 0.6313
Epoch 149/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8099 - acc: 0.6516 - val_loss: 0.8211 - val_acc: 0.6307
Epoch 150/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8096 - acc: 0.6514 - val_loss: 0.8208 - val_acc: 0.6328
15663/15663 [==============================] - 0s 25us/step
==== Training loss, score are: 0.8093209221834485 0.6517269999627934 =======
3916/3916 [==============================] - 0s 27us/step
==== CV loss, score are: 0.820845923884044 0.6327885597548519 =======
Running fold 4
Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_47 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
global_average_pooling1d_47  (None, 100)               0         
_________________________________________________________________
dense_47 (Dense)             (None, 3)                 303       
=================================================================
Total params: 24,734,903
Trainable params: 303
Non-trainable params: 24,734,600
_________________________________________________________________
15663/15663 [==============================] - 1s 78us/step
Before training loss, score are: 1.109358027315776 0.29125965651820873
Train on 15663 samples, validate on 3916 samples
Epoch 1/150
15663/15663 [==============================] - 1s 70us/step - loss: 1.0894 - acc: 0.3925 - val_loss: 1.0789 - val_acc: 0.4147
Epoch 2/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0748 - acc: 0.4068 - val_loss: 1.0671 - val_acc: 0.4346
Epoch 3/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0636 - acc: 0.4302 - val_loss: 1.0560 - val_acc: 0.4436
Epoch 4/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0534 - acc: 0.4424 - val_loss: 1.0466 - val_acc: 0.4686
Epoch 5/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0441 - acc: 0.4603 - val_loss: 1.0373 - val_acc: 0.4821
Epoch 6/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0356 - acc: 0.4774 - val_loss: 1.0289 - val_acc: 0.4842
Epoch 7/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0276 - acc: 0.4891 - val_loss: 1.0212 - val_acc: 0.4987
Epoch 8/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0200 - acc: 0.4993 - val_loss: 1.0143 - val_acc: 0.5158
Epoch 9/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0131 - acc: 0.5101 - val_loss: 1.0072 - val_acc: 0.5148
Epoch 10/150
15663/15663 [==============================] - 0s 17us/step - loss: 1.0066 - acc: 0.5126 - val_loss: 1.0009 - val_acc: 0.5294
Epoch 11/150
15663/15663 [==============================] - 0s 16us/step - loss: 1.0003 - acc: 0.5247 - val_loss: 0.9950 - val_acc: 0.5388
Epoch 12/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9945 - acc: 0.5295 - val_loss: 0.9894 - val_acc: 0.5439
Epoch 13/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9889 - acc: 0.5372 - val_loss: 0.9838 - val_acc: 0.5470
Epoch 14/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9837 - acc: 0.5402 - val_loss: 0.9788 - val_acc: 0.5559
Epoch 15/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9787 - acc: 0.5482 - val_loss: 0.9740 - val_acc: 0.5621
Epoch 16/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9739 - acc: 0.5540 - val_loss: 0.9695 - val_acc: 0.5595
Epoch 17/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9693 - acc: 0.5564 - val_loss: 0.9650 - val_acc: 0.5674
Epoch 18/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9649 - acc: 0.5639 - val_loss: 0.9608 - val_acc: 0.5664
Epoch 19/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9607 - acc: 0.5632 - val_loss: 0.9569 - val_acc: 0.5764
Epoch 20/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9566 - acc: 0.5714 - val_loss: 0.9527 - val_acc: 0.5746
Epoch 21/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9528 - acc: 0.5721 - val_loss: 0.9494 - val_acc: 0.5835
Epoch 22/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9490 - acc: 0.5772 - val_loss: 0.9455 - val_acc: 0.5781
Epoch 23/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9456 - acc: 0.5755 - val_loss: 0.9424 - val_acc: 0.5912
Epoch 24/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9421 - acc: 0.5798 - val_loss: 0.9389 - val_acc: 0.5960
Epoch 25/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9387 - acc: 0.5833 - val_loss: 0.9353 - val_acc: 0.5919
Epoch 26/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9355 - acc: 0.5855 - val_loss: 0.9322 - val_acc: 0.5917
Epoch 27/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9325 - acc: 0.5852 - val_loss: 0.9294 - val_acc: 0.5988
Epoch 28/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9294 - acc: 0.5887 - val_loss: 0.9266 - val_acc: 0.5991
Epoch 29/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9266 - acc: 0.5888 - val_loss: 0.9240 - val_acc: 0.6029
Epoch 30/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9237 - acc: 0.5911 - val_loss: 0.9212 - val_acc: 0.6039
Epoch 31/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9212 - acc: 0.5941 - val_loss: 0.9185 - val_acc: 0.6044
Epoch 32/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9185 - acc: 0.5948 - val_loss: 0.9156 - val_acc: 0.6055
Epoch 33/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.9159 - acc: 0.5966 - val_loss: 0.9135 - val_acc: 0.6098
Epoch 34/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9133 - acc: 0.5978 - val_loss: 0.9113 - val_acc: 0.6073
Epoch 35/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9111 - acc: 0.5985 - val_loss: 0.9085 - val_acc: 0.6101
Epoch 36/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9089 - acc: 0.5999 - val_loss: 0.9062 - val_acc: 0.6103
Epoch 37/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9065 - acc: 0.6013 - val_loss: 0.9041 - val_acc: 0.6147
Epoch 38/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9043 - acc: 0.6049 - val_loss: 0.9018 - val_acc: 0.6106
Epoch 39/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9025 - acc: 0.6038 - val_loss: 0.8997 - val_acc: 0.6129
Epoch 40/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.9002 - acc: 0.6045 - val_loss: 0.8982 - val_acc: 0.6167
Epoch 41/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8983 - acc: 0.6079 - val_loss: 0.8958 - val_acc: 0.6164
Epoch 42/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8963 - acc: 0.6074 - val_loss: 0.8940 - val_acc: 0.6193
Epoch 43/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8944 - acc: 0.6089 - val_loss: 0.8921 - val_acc: 0.6170
Epoch 44/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8926 - acc: 0.6103 - val_loss: 0.8904 - val_acc: 0.6175
Epoch 45/150
15663/15663 [==============================] - 0s 15us/step - loss: 0.8908 - acc: 0.6124 - val_loss: 0.8889 - val_acc: 0.6180
Epoch 46/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8890 - acc: 0.6132 - val_loss: 0.8870 - val_acc: 0.6167
Epoch 47/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8875 - acc: 0.6131 - val_loss: 0.8851 - val_acc: 0.6210
Epoch 48/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8857 - acc: 0.6138 - val_loss: 0.8838 - val_acc: 0.6213
Epoch 49/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8841 - acc: 0.6155 - val_loss: 0.8819 - val_acc: 0.6221
Epoch 50/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8826 - acc: 0.6148 - val_loss: 0.8804 - val_acc: 0.6236
Epoch 51/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8811 - acc: 0.6169 - val_loss: 0.8790 - val_acc: 0.6254
Epoch 52/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8795 - acc: 0.6176 - val_loss: 0.8774 - val_acc: 0.6233
Epoch 53/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8780 - acc: 0.6180 - val_loss: 0.8757 - val_acc: 0.6246
Epoch 54/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8768 - acc: 0.6187 - val_loss: 0.8746 - val_acc: 0.6256
Epoch 55/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8753 - acc: 0.6187 - val_loss: 0.8731 - val_acc: 0.6254
Epoch 56/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8739 - acc: 0.6187 - val_loss: 0.8721 - val_acc: 0.6284
Epoch 57/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8726 - acc: 0.6197 - val_loss: 0.8703 - val_acc: 0.6256
Epoch 58/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8713 - acc: 0.6201 - val_loss: 0.8695 - val_acc: 0.6282
Epoch 59/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8701 - acc: 0.6212 - val_loss: 0.8678 - val_acc: 0.6284
Epoch 60/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8688 - acc: 0.6227 - val_loss: 0.8665 - val_acc: 0.6277
Epoch 61/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8676 - acc: 0.6211 - val_loss: 0.8652 - val_acc: 0.6305
Epoch 62/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8665 - acc: 0.6227 - val_loss: 0.8641 - val_acc: 0.6297
Epoch 63/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8652 - acc: 0.6225 - val_loss: 0.8629 - val_acc: 0.6305
Epoch 64/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8641 - acc: 0.6224 - val_loss: 0.8617 - val_acc: 0.6310
Epoch 65/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8631 - acc: 0.6234 - val_loss: 0.8612 - val_acc: 0.6310
Epoch 66/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8618 - acc: 0.6234 - val_loss: 0.8599 - val_acc: 0.6330
Epoch 67/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8609 - acc: 0.6254 - val_loss: 0.8584 - val_acc: 0.6325
Epoch 68/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8597 - acc: 0.6247 - val_loss: 0.8573 - val_acc: 0.6353
Epoch 69/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8588 - acc: 0.6257 - val_loss: 0.8565 - val_acc: 0.6336
Epoch 70/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8578 - acc: 0.6249 - val_loss: 0.8556 - val_acc: 0.6333
Epoch 71/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8569 - acc: 0.6263 - val_loss: 0.8545 - val_acc: 0.6336
Epoch 72/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8558 - acc: 0.6271 - val_loss: 0.8537 - val_acc: 0.6364
Epoch 73/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8548 - acc: 0.6272 - val_loss: 0.8529 - val_acc: 0.6351
Epoch 74/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8539 - acc: 0.6282 - val_loss: 0.8515 - val_acc: 0.6359
Epoch 75/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8531 - acc: 0.6275 - val_loss: 0.8505 - val_acc: 0.6359
Epoch 76/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8521 - acc: 0.6292 - val_loss: 0.8504 - val_acc: 0.6361
Epoch 77/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8514 - acc: 0.6281 - val_loss: 0.8486 - val_acc: 0.6382
Epoch 78/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8504 - acc: 0.6294 - val_loss: 0.8481 - val_acc: 0.6371
Epoch 79/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8496 - acc: 0.6289 - val_loss: 0.8468 - val_acc: 0.6389
Epoch 80/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8487 - acc: 0.6300 - val_loss: 0.8462 - val_acc: 0.6341
Epoch 81/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8480 - acc: 0.6298 - val_loss: 0.8452 - val_acc: 0.6394
Epoch 82/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8472 - acc: 0.6312 - val_loss: 0.8444 - val_acc: 0.6361
Epoch 83/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8466 - acc: 0.6302 - val_loss: 0.8439 - val_acc: 0.6376
Epoch 84/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8456 - acc: 0.6305 - val_loss: 0.8428 - val_acc: 0.6384
Epoch 85/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8449 - acc: 0.6313 - val_loss: 0.8421 - val_acc: 0.6376
Epoch 86/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8441 - acc: 0.6323 - val_loss: 0.8411 - val_acc: 0.6407
Epoch 87/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8434 - acc: 0.6314 - val_loss: 0.8403 - val_acc: 0.6394
Epoch 88/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8426 - acc: 0.6317 - val_loss: 0.8398 - val_acc: 0.6382
Epoch 89/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8419 - acc: 0.6321 - val_loss: 0.8395 - val_acc: 0.6382
Epoch 90/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8413 - acc: 0.6340 - val_loss: 0.8380 - val_acc: 0.6420
Epoch 91/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8406 - acc: 0.6331 - val_loss: 0.8377 - val_acc: 0.6387
Epoch 92/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8399 - acc: 0.6348 - val_loss: 0.8366 - val_acc: 0.6415
Epoch 93/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8392 - acc: 0.6334 - val_loss: 0.8363 - val_acc: 0.6410
Epoch 94/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8386 - acc: 0.6352 - val_loss: 0.8359 - val_acc: 0.6402
Epoch 95/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8379 - acc: 0.6339 - val_loss: 0.8351 - val_acc: 0.6387
Epoch 96/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8373 - acc: 0.6342 - val_loss: 0.8342 - val_acc: 0.6422
Epoch 97/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8367 - acc: 0.6354 - val_loss: 0.8337 - val_acc: 0.6399
Epoch 98/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8361 - acc: 0.6353 - val_loss: 0.8329 - val_acc: 0.6412
Epoch 99/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8355 - acc: 0.6362 - val_loss: 0.8324 - val_acc: 0.6412
Epoch 100/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8349 - acc: 0.6358 - val_loss: 0.8315 - val_acc: 0.6425
Epoch 101/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8344 - acc: 0.6354 - val_loss: 0.8311 - val_acc: 0.6420
Epoch 102/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8337 - acc: 0.6367 - val_loss: 0.8308 - val_acc: 0.6433
Epoch 103/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8334 - acc: 0.6358 - val_loss: 0.8299 - val_acc: 0.6422
Epoch 104/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8327 - acc: 0.6363 - val_loss: 0.8291 - val_acc: 0.6425
Epoch 105/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8321 - acc: 0.6374 - val_loss: 0.8285 - val_acc: 0.6425
Epoch 106/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8315 - acc: 0.6370 - val_loss: 0.8279 - val_acc: 0.6445
Epoch 107/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8311 - acc: 0.6383 - val_loss: 0.8277 - val_acc: 0.6443
Epoch 108/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8305 - acc: 0.6366 - val_loss: 0.8274 - val_acc: 0.6461
Epoch 109/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8300 - acc: 0.6372 - val_loss: 0.8264 - val_acc: 0.6430
Epoch 110/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8296 - acc: 0.6375 - val_loss: 0.8259 - val_acc: 0.6433
Epoch 111/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8290 - acc: 0.6390 - val_loss: 0.8257 - val_acc: 0.6448
Epoch 112/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8287 - acc: 0.6376 - val_loss: 0.8249 - val_acc: 0.6461
Epoch 113/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8281 - acc: 0.6381 - val_loss: 0.8244 - val_acc: 0.6448
Epoch 114/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8275 - acc: 0.6381 - val_loss: 0.8240 - val_acc: 0.6468
Epoch 115/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8271 - acc: 0.6380 - val_loss: 0.8235 - val_acc: 0.6471
Epoch 116/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8266 - acc: 0.6395 - val_loss: 0.8231 - val_acc: 0.6458
Epoch 117/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8262 - acc: 0.6393 - val_loss: 0.8222 - val_acc: 0.6471
Epoch 118/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8258 - acc: 0.6393 - val_loss: 0.8220 - val_acc: 0.6489
Epoch 119/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8252 - acc: 0.6406 - val_loss: 0.8216 - val_acc: 0.6458
Epoch 120/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8249 - acc: 0.6400 - val_loss: 0.8208 - val_acc: 0.6481
Epoch 121/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8245 - acc: 0.6399 - val_loss: 0.8204 - val_acc: 0.6481
Epoch 122/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8240 - acc: 0.6400 - val_loss: 0.8201 - val_acc: 0.6486
Epoch 123/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8236 - acc: 0.6399 - val_loss: 0.8196 - val_acc: 0.6486
Epoch 124/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8232 - acc: 0.6400 - val_loss: 0.8191 - val_acc: 0.6486
Epoch 125/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8226 - acc: 0.6409 - val_loss: 0.8192 - val_acc: 0.6507
Epoch 126/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8226 - acc: 0.6413 - val_loss: 0.8184 - val_acc: 0.6504
Epoch 127/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8219 - acc: 0.6407 - val_loss: 0.8176 - val_acc: 0.6491
Epoch 128/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8215 - acc: 0.6412 - val_loss: 0.8180 - val_acc: 0.6499
Epoch 129/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8212 - acc: 0.6412 - val_loss: 0.8170 - val_acc: 0.6458
Epoch 130/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8208 - acc: 0.6425 - val_loss: 0.8167 - val_acc: 0.6479
Epoch 131/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8204 - acc: 0.6431 - val_loss: 0.8159 - val_acc: 0.6496
Epoch 132/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8201 - acc: 0.6427 - val_loss: 0.8158 - val_acc: 0.6479
Epoch 133/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8196 - acc: 0.6424 - val_loss: 0.8159 - val_acc: 0.6519
Epoch 134/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8193 - acc: 0.6434 - val_loss: 0.8150 - val_acc: 0.6481
Epoch 135/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8190 - acc: 0.6425 - val_loss: 0.8148 - val_acc: 0.6514
Epoch 136/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8186 - acc: 0.6427 - val_loss: 0.8145 - val_acc: 0.6522
Epoch 137/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8182 - acc: 0.6439 - val_loss: 0.8137 - val_acc: 0.6504
Epoch 138/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8179 - acc: 0.6437 - val_loss: 0.8134 - val_acc: 0.6486
Epoch 139/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8174 - acc: 0.6437 - val_loss: 0.8130 - val_acc: 0.6537
Epoch 140/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8172 - acc: 0.6430 - val_loss: 0.8127 - val_acc: 0.6507
Epoch 141/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8168 - acc: 0.6448 - val_loss: 0.8124 - val_acc: 0.6486
Epoch 142/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8165 - acc: 0.6453 - val_loss: 0.8119 - val_acc: 0.6494
Epoch 143/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8162 - acc: 0.6450 - val_loss: 0.8116 - val_acc: 0.6504
Epoch 144/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8158 - acc: 0.6447 - val_loss: 0.8115 - val_acc: 0.6519
Epoch 145/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8156 - acc: 0.6457 - val_loss: 0.8111 - val_acc: 0.6519
Epoch 146/150
15663/15663 [==============================] - 0s 17us/step - loss: 0.8152 - acc: 0.6452 - val_loss: 0.8106 - val_acc: 0.6532
Epoch 147/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8149 - acc: 0.6442 - val_loss: 0.8105 - val_acc: 0.6527
Epoch 148/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8145 - acc: 0.6451 - val_loss: 0.8101 - val_acc: 0.6484
Epoch 149/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8143 - acc: 0.6456 - val_loss: 0.8095 - val_acc: 0.6507
Epoch 150/150
15663/15663 [==============================] - 0s 16us/step - loss: 0.8139 - acc: 0.6452 - val_loss: 0.8091 - val_acc: 0.6489
15663/15663 [==============================] - 0s 26us/step
==== Training loss, score are: 0.8136896675043862 0.6450233033529486 =======
3916/3916 [==============================] - 0s 27us/step
==== CV loss, score are: 0.8090695989631169 0.648876404494382 =======
Running fold 5
Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_48 (Embedding)     (None, 161, 100)          24734600  
_________________________________________________________________
global_average_pooling1d_48  (None, 100)               0         
_________________________________________________________________
dense_48 (Dense)             (None, 3)                 303       
=================================================================
Total params: 24,734,903
Trainable params: 303
Non-trainable params: 24,734,600
_________________________________________________________________
15664/15664 [==============================] - 1s 80us/step
Before training loss, score are: 1.107530347173377 0.32073544433094997
Train on 15664 samples, validate on 3915 samples
Epoch 1/150
15664/15664 [==============================] - 1s 70us/step - loss: 1.0897 - acc: 0.3879 - val_loss: 1.0776 - val_acc: 0.4215
Epoch 2/150
15664/15664 [==============================] - 0s 16us/step - loss: 1.0756 - acc: 0.4051 - val_loss: 1.0658 - val_acc: 0.4360
Epoch 3/150
15664/15664 [==============================] - 0s 16us/step - loss: 1.0645 - acc: 0.4273 - val_loss: 1.0548 - val_acc: 0.4524
Epoch 4/150
15664/15664 [==============================] - 0s 16us/step - loss: 1.0544 - acc: 0.4455 - val_loss: 1.0451 - val_acc: 0.4708
Epoch 5/150
15664/15664 [==============================] - 0s 16us/step - loss: 1.0449 - acc: 0.4602 - val_loss: 1.0365 - val_acc: 0.4912
Epoch 6/150
15664/15664 [==============================] - 0s 16us/step - loss: 1.0363 - acc: 0.4750 - val_loss: 1.0282 - val_acc: 0.5032
Epoch 7/150
15664/15664 [==============================] - 0s 16us/step - loss: 1.0283 - acc: 0.4899 - val_loss: 1.0194 - val_acc: 0.5073
Epoch 8/150
15664/15664 [==============================] - 0s 16us/step - loss: 1.0209 - acc: 0.4994 - val_loss: 1.0122 - val_acc: 0.5216
Epoch 9/150
15664/15664 [==============================] - 0s 16us/step - loss: 1.0137 - acc: 0.5109 - val_loss: 1.0047 - val_acc: 0.5098
Epoch 10/150
15664/15664 [==============================] - 0s 16us/step - loss: 1.0072 - acc: 0.5151 - val_loss: 0.9989 - val_acc: 0.5372
Epoch 11/150
15664/15664 [==============================] - 0s 16us/step - loss: 1.0010 - acc: 0.5210 - val_loss: 0.9929 - val_acc: 0.5453
Epoch 12/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9951 - acc: 0.5317 - val_loss: 0.9867 - val_acc: 0.5458
Epoch 13/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9896 - acc: 0.5353 - val_loss: 0.9817 - val_acc: 0.5571
Epoch 14/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9842 - acc: 0.5458 - val_loss: 0.9760 - val_acc: 0.5579
Epoch 15/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9793 - acc: 0.5474 - val_loss: 0.9712 - val_acc: 0.5642
Epoch 16/150
15664/15664 [==============================] - 0s 17us/step - loss: 0.9745 - acc: 0.5555 - val_loss: 0.9666 - val_acc: 0.5668
Epoch 17/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9699 - acc: 0.5568 - val_loss: 0.9627 - val_acc: 0.5745
Epoch 18/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9655 - acc: 0.5605 - val_loss: 0.9587 - val_acc: 0.5849
Epoch 19/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9613 - acc: 0.5672 - val_loss: 0.9537 - val_acc: 0.5727
Epoch 20/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9574 - acc: 0.5677 - val_loss: 0.9505 - val_acc: 0.5862
Epoch 21/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9534 - acc: 0.5718 - val_loss: 0.9458 - val_acc: 0.5821
Epoch 22/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9495 - acc: 0.5755 - val_loss: 0.9419 - val_acc: 0.5798
Epoch 23/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9462 - acc: 0.5745 - val_loss: 0.9391 - val_acc: 0.5916
Epoch 24/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9426 - acc: 0.5792 - val_loss: 0.9363 - val_acc: 0.5992
Epoch 25/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9394 - acc: 0.5809 - val_loss: 0.9324 - val_acc: 0.5941
Epoch 26/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9361 - acc: 0.5846 - val_loss: 0.9294 - val_acc: 0.6000
Epoch 27/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9331 - acc: 0.5861 - val_loss: 0.9266 - val_acc: 0.6038
Epoch 28/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9301 - acc: 0.5890 - val_loss: 0.9226 - val_acc: 0.5951
Epoch 29/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9272 - acc: 0.5891 - val_loss: 0.9208 - val_acc: 0.6023
Epoch 30/150
15664/15664 [==============================] - 0s 17us/step - loss: 0.9244 - acc: 0.5897 - val_loss: 0.9178 - val_acc: 0.6059
Epoch 31/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9217 - acc: 0.5937 - val_loss: 0.9147 - val_acc: 0.6074
Epoch 32/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9191 - acc: 0.5937 - val_loss: 0.9121 - val_acc: 0.6084
Epoch 33/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9165 - acc: 0.5957 - val_loss: 0.9099 - val_acc: 0.6123
Epoch 34/150
15664/15664 [==============================] - 0s 17us/step - loss: 0.9142 - acc: 0.5978 - val_loss: 0.9078 - val_acc: 0.6138
Epoch 35/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9116 - acc: 0.5975 - val_loss: 0.9051 - val_acc: 0.6140
Epoch 36/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9093 - acc: 0.5998 - val_loss: 0.9032 - val_acc: 0.6169
Epoch 37/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9071 - acc: 0.6015 - val_loss: 0.9007 - val_acc: 0.6158
Epoch 38/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9050 - acc: 0.6036 - val_loss: 0.8988 - val_acc: 0.6176
Epoch 39/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9028 - acc: 0.6050 - val_loss: 0.8962 - val_acc: 0.6156
Epoch 40/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.9009 - acc: 0.6044 - val_loss: 0.8949 - val_acc: 0.6199
Epoch 41/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8988 - acc: 0.6065 - val_loss: 0.8925 - val_acc: 0.6197
Epoch 42/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8969 - acc: 0.6075 - val_loss: 0.8907 - val_acc: 0.6202
Epoch 43/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8948 - acc: 0.6083 - val_loss: 0.8881 - val_acc: 0.6171
Epoch 44/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8932 - acc: 0.6095 - val_loss: 0.8866 - val_acc: 0.6215
Epoch 45/150
15664/15664 [==============================] - 0s 17us/step - loss: 0.8914 - acc: 0.6120 - val_loss: 0.8851 - val_acc: 0.6209
Epoch 46/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8896 - acc: 0.6108 - val_loss: 0.8837 - val_acc: 0.6220
Epoch 47/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8879 - acc: 0.6123 - val_loss: 0.8818 - val_acc: 0.6222
Epoch 48/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8861 - acc: 0.6127 - val_loss: 0.8805 - val_acc: 0.6250
Epoch 49/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8846 - acc: 0.6131 - val_loss: 0.8793 - val_acc: 0.6278
Epoch 50/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8831 - acc: 0.6150 - val_loss: 0.8770 - val_acc: 0.6253
Epoch 51/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8814 - acc: 0.6163 - val_loss: 0.8755 - val_acc: 0.6284
Epoch 52/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8799 - acc: 0.6159 - val_loss: 0.8751 - val_acc: 0.6296
Epoch 53/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8784 - acc: 0.6167 - val_loss: 0.8730 - val_acc: 0.6299
Epoch 54/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8770 - acc: 0.6173 - val_loss: 0.8712 - val_acc: 0.6299
Epoch 55/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8756 - acc: 0.6174 - val_loss: 0.8706 - val_acc: 0.6301
Epoch 56/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8743 - acc: 0.6188 - val_loss: 0.8686 - val_acc: 0.6319
Epoch 57/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8729 - acc: 0.6187 - val_loss: 0.8672 - val_acc: 0.6322
Epoch 58/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8716 - acc: 0.6194 - val_loss: 0.8658 - val_acc: 0.6319
Epoch 59/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8703 - acc: 0.6199 - val_loss: 0.8648 - val_acc: 0.6330
Epoch 60/150
15664/15664 [==============================] - 0s 17us/step - loss: 0.8690 - acc: 0.6201 - val_loss: 0.8636 - val_acc: 0.6332
Epoch 61/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8677 - acc: 0.6214 - val_loss: 0.8620 - val_acc: 0.6314
Epoch 62/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8667 - acc: 0.6222 - val_loss: 0.8609 - val_acc: 0.6345
Epoch 63/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8654 - acc: 0.6233 - val_loss: 0.8595 - val_acc: 0.6358
Epoch 64/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8641 - acc: 0.6224 - val_loss: 0.8583 - val_acc: 0.6360
Epoch 65/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8628 - acc: 0.6240 - val_loss: 0.8596 - val_acc: 0.6360
Epoch 66/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8620 - acc: 0.6251 - val_loss: 0.8563 - val_acc: 0.6358
Epoch 67/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8609 - acc: 0.6254 - val_loss: 0.8551 - val_acc: 0.6368
Epoch 68/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8598 - acc: 0.6253 - val_loss: 0.8541 - val_acc: 0.6340
Epoch 69/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8588 - acc: 0.6251 - val_loss: 0.8542 - val_acc: 0.6398
Epoch 70/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8578 - acc: 0.6260 - val_loss: 0.8526 - val_acc: 0.6391
Epoch 71/150
15664/15664 [==============================] - 0s 17us/step - loss: 0.8567 - acc: 0.6268 - val_loss: 0.8509 - val_acc: 0.6363
Epoch 72/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8559 - acc: 0.6259 - val_loss: 0.8502 - val_acc: 0.6373
Epoch 73/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8548 - acc: 0.6276 - val_loss: 0.8495 - val_acc: 0.6381
Epoch 74/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8539 - acc: 0.6278 - val_loss: 0.8484 - val_acc: 0.6381
Epoch 75/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8529 - acc: 0.6278 - val_loss: 0.8482 - val_acc: 0.6429
Epoch 76/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8520 - acc: 0.6299 - val_loss: 0.8473 - val_acc: 0.6416
Epoch 77/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8511 - acc: 0.6304 - val_loss: 0.8456 - val_acc: 0.6386
Epoch 78/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8502 - acc: 0.6300 - val_loss: 0.8448 - val_acc: 0.6355
Epoch 79/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8494 - acc: 0.6300 - val_loss: 0.8444 - val_acc: 0.6429
Epoch 80/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8486 - acc: 0.6306 - val_loss: 0.8438 - val_acc: 0.6437
Epoch 81/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8478 - acc: 0.6314 - val_loss: 0.8425 - val_acc: 0.6414
Epoch 82/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8469 - acc: 0.6310 - val_loss: 0.8423 - val_acc: 0.6439
Epoch 83/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8461 - acc: 0.6300 - val_loss: 0.8415 - val_acc: 0.6437
Epoch 84/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8454 - acc: 0.6314 - val_loss: 0.8405 - val_acc: 0.6429
Epoch 85/150
15664/15664 [==============================] - 0s 17us/step - loss: 0.8446 - acc: 0.6318 - val_loss: 0.8395 - val_acc: 0.6432
Epoch 86/150
15664/15664 [==============================] - 0s 20us/step - loss: 0.8438 - acc: 0.6316 - val_loss: 0.8384 - val_acc: 0.6437
Epoch 87/150
15664/15664 [==============================] - 0s 17us/step - loss: 0.8431 - acc: 0.6323 - val_loss: 0.8381 - val_acc: 0.6447
Epoch 88/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8423 - acc: 0.6326 - val_loss: 0.8384 - val_acc: 0.6465
Epoch 89/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8416 - acc: 0.6324 - val_loss: 0.8365 - val_acc: 0.6442
Epoch 90/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8409 - acc: 0.6330 - val_loss: 0.8361 - val_acc: 0.6457
Epoch 91/150
15664/15664 [==============================] - ETA: 0s - loss: 0.8391 - acc: 0.634 - 0s 16us/step - loss: 0.8402 - acc: 0.6336 - val_loss: 0.8356 - val_acc: 0.6460
Epoch 92/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8395 - acc: 0.6338 - val_loss: 0.8347 - val_acc: 0.6460
Epoch 93/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8388 - acc: 0.6344 - val_loss: 0.8344 - val_acc: 0.6460
Epoch 94/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8382 - acc: 0.6345 - val_loss: 0.8333 - val_acc: 0.6457
Epoch 95/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8376 - acc: 0.6329 - val_loss: 0.8328 - val_acc: 0.6470
Epoch 96/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8369 - acc: 0.6337 - val_loss: 0.8322 - val_acc: 0.6473
Epoch 97/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8363 - acc: 0.6356 - val_loss: 0.8321 - val_acc: 0.6467
Epoch 98/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8357 - acc: 0.6343 - val_loss: 0.8315 - val_acc: 0.6485
Epoch 99/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8350 - acc: 0.6355 - val_loss: 0.8305 - val_acc: 0.6508
Epoch 100/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8343 - acc: 0.6348 - val_loss: 0.8296 - val_acc: 0.6485
Epoch 101/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8338 - acc: 0.6359 - val_loss: 0.8293 - val_acc: 0.6506
Epoch 102/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8332 - acc: 0.6359 - val_loss: 0.8287 - val_acc: 0.6524
Epoch 103/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8327 - acc: 0.6362 - val_loss: 0.8282 - val_acc: 0.6488
Epoch 104/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8320 - acc: 0.6360 - val_loss: 0.8275 - val_acc: 0.6473
Epoch 105/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8315 - acc: 0.6366 - val_loss: 0.8271 - val_acc: 0.6475
Epoch 106/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8308 - acc: 0.6369 - val_loss: 0.8257 - val_acc: 0.6470
Epoch 107/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8306 - acc: 0.6363 - val_loss: 0.8257 - val_acc: 0.6519
Epoch 108/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8299 - acc: 0.6365 - val_loss: 0.8252 - val_acc: 0.6498
Epoch 109/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8294 - acc: 0.6370 - val_loss: 0.8248 - val_acc: 0.6501
Epoch 110/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8287 - acc: 0.6387 - val_loss: 0.8248 - val_acc: 0.6521
Epoch 111/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8285 - acc: 0.6385 - val_loss: 0.8239 - val_acc: 0.6496
Epoch 112/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8278 - acc: 0.6387 - val_loss: 0.8235 - val_acc: 0.6496
Epoch 113/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8274 - acc: 0.6378 - val_loss: 0.8234 - val_acc: 0.6503
Epoch 114/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8269 - acc: 0.6384 - val_loss: 0.8225 - val_acc: 0.6529
Epoch 115/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8264 - acc: 0.6383 - val_loss: 0.8218 - val_acc: 0.6531
Epoch 116/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8259 - acc: 0.6402 - val_loss: 0.8219 - val_acc: 0.6488
Epoch 117/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8255 - acc: 0.6396 - val_loss: 0.8214 - val_acc: 0.6534
Epoch 118/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8250 - acc: 0.6399 - val_loss: 0.8210 - val_acc: 0.6542
Epoch 119/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8246 - acc: 0.6406 - val_loss: 0.8198 - val_acc: 0.6521
Epoch 120/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8241 - acc: 0.6393 - val_loss: 0.8203 - val_acc: 0.6542
Epoch 121/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8236 - acc: 0.6409 - val_loss: 0.8196 - val_acc: 0.6552
Epoch 122/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8232 - acc: 0.6415 - val_loss: 0.8193 - val_acc: 0.6542
Epoch 123/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8227 - acc: 0.6410 - val_loss: 0.8183 - val_acc: 0.6552
Epoch 124/150
15664/15664 [==============================] - 0s 17us/step - loss: 0.8223 - acc: 0.6404 - val_loss: 0.8187 - val_acc: 0.6547
Epoch 125/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8218 - acc: 0.6420 - val_loss: 0.8175 - val_acc: 0.6559
Epoch 126/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8214 - acc: 0.6422 - val_loss: 0.8177 - val_acc: 0.6549
Epoch 127/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8210 - acc: 0.6432 - val_loss: 0.8166 - val_acc: 0.6552
Epoch 128/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8208 - acc: 0.6414 - val_loss: 0.8171 - val_acc: 0.6542
Epoch 129/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8203 - acc: 0.6430 - val_loss: 0.8160 - val_acc: 0.6552
Epoch 130/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8199 - acc: 0.6431 - val_loss: 0.8157 - val_acc: 0.6549
Epoch 131/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8195 - acc: 0.6434 - val_loss: 0.8155 - val_acc: 0.6547
Epoch 132/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8192 - acc: 0.6433 - val_loss: 0.8155 - val_acc: 0.6549
Epoch 133/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8188 - acc: 0.6438 - val_loss: 0.8149 - val_acc: 0.6562
Epoch 134/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8184 - acc: 0.6443 - val_loss: 0.8145 - val_acc: 0.6547
Epoch 135/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8181 - acc: 0.6431 - val_loss: 0.8144 - val_acc: 0.6549
Epoch 136/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8177 - acc: 0.6434 - val_loss: 0.8137 - val_acc: 0.6554
Epoch 137/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8173 - acc: 0.6443 - val_loss: 0.8137 - val_acc: 0.6554
Epoch 138/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8168 - acc: 0.6442 - val_loss: 0.8131 - val_acc: 0.6552
Epoch 139/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8167 - acc: 0.6454 - val_loss: 0.8125 - val_acc: 0.6564
Epoch 140/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8161 - acc: 0.6457 - val_loss: 0.8118 - val_acc: 0.6549
Epoch 141/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8159 - acc: 0.6438 - val_loss: 0.8120 - val_acc: 0.6564
Epoch 142/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8155 - acc: 0.6446 - val_loss: 0.8121 - val_acc: 0.6580
Epoch 143/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8152 - acc: 0.6454 - val_loss: 0.8116 - val_acc: 0.6557
Epoch 144/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8149 - acc: 0.6455 - val_loss: 0.8108 - val_acc: 0.6567
Epoch 145/150
15664/15664 [==============================] - 0s 17us/step - loss: 0.8147 - acc: 0.6445 - val_loss: 0.8107 - val_acc: 0.6564
Epoch 146/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8143 - acc: 0.6456 - val_loss: 0.8108 - val_acc: 0.6572
Epoch 147/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8139 - acc: 0.6454 - val_loss: 0.8107 - val_acc: 0.6559
Epoch 148/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8136 - acc: 0.6456 - val_loss: 0.8106 - val_acc: 0.6567
Epoch 149/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8132 - acc: 0.6461 - val_loss: 0.8098 - val_acc: 0.6582
Epoch 150/150
15664/15664 [==============================] - 0s 16us/step - loss: 0.8130 - acc: 0.6463 - val_loss: 0.8087 - val_acc: 0.6554
15664/15664 [==============================] - 1s 32us/step
==== Training loss, score are: 0.8128567321551345 0.6447906026557712 =======
3915/3915 [==============================] - 0s 27us/step
==== CV loss, score are: 0.8087320398096838 0.6554278416195135 =======


===== Model: fast_text_char_glove  ========:
 Cross-val log losses are: [0.81595307091933333, 0.82309050842853493, 0.82084592173363069, 0.80906959211613616, 0.80873203528611248]
====== Mean cross-val log loss is: 0.8155382256967496 =========


</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Final-stacking-model---Run-XGBoost">Final stacking model - Run XGBoost<a class="anchor-link" href="#Final-stacking-model---Run-XGBoost">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[54]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">## Display the columns in the training and test data at this point</span>
<span class="n">display</span><span class="p">(</span><span class="n">train_raw</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">display</span><span class="p">(</span><span class="n">test_raw</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="c1"># Display all the features</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_raw</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>author</th>
      <th>num_punctuations</th>
      <th>num_stopwords</th>
      <th>num_words</th>
      <th>mnb_tfidf_eap</th>
      <th>mnb_tfidf_hpl</th>
      <th>mnb_tfidf_mws</th>
      <th>mnb_count_eap</th>
      <th>...</th>
      <th>cnn_glove_mws</th>
      <th>fast_text_char_none_eap</th>
      <th>fast_text_char_none_hpl</th>
      <th>fast_text_char_none_mws</th>
      <th>fast_text_char_gensim_eap</th>
      <th>fast_text_char_gensim_hpl</th>
      <th>fast_text_char_gensim_mws</th>
      <th>fast_text_char_glove_eap</th>
      <th>fast_text_char_glove_hpl</th>
      <th>fast_text_char_glove_mws</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id26305</td>
      <td>This process, however, afforded me no means of...</td>
      <td>EAP</td>
      <td>7</td>
      <td>19</td>
      <td>41</td>
      <td>0.942005</td>
      <td>0.01129</td>
      <td>0.046705</td>
      <td>0.999989</td>
      <td>...</td>
      <td>0.013262</td>
      <td>0.998048</td>
      <td>0.000732</td>
      <td>0.001221</td>
      <td>0.999284</td>
      <td>0.000205</td>
      <td>0.000511</td>
      <td>0.839104</td>
      <td>0.099007</td>
      <td>0.061889</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 45 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>num_punctuations</th>
      <th>num_stopwords</th>
      <th>num_words</th>
      <th>mnb_tfidf_eap</th>
      <th>mnb_tfidf_hpl</th>
      <th>mnb_tfidf_mws</th>
      <th>mnb_count_eap</th>
      <th>mnb_count_hpl</th>
      <th>...</th>
      <th>cnn_glove_mws</th>
      <th>fast_text_char_none_eap</th>
      <th>fast_text_char_none_hpl</th>
      <th>fast_text_char_none_mws</th>
      <th>fast_text_char_gensim_eap</th>
      <th>fast_text_char_gensim_hpl</th>
      <th>fast_text_char_gensim_mws</th>
      <th>fast_text_char_glove_eap</th>
      <th>fast_text_char_glove_hpl</th>
      <th>fast_text_char_glove_mws</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id02310</td>
      <td>Still, as I urged our leaving Ireland with suc...</td>
      <td>3</td>
      <td>9</td>
      <td>19</td>
      <td>0.07408</td>
      <td>0.004957</td>
      <td>0.920963</td>
      <td>0.038465</td>
      <td>0.000992</td>
      <td>...</td>
      <td>0.577344</td>
      <td>0.029432</td>
      <td>0.019272</td>
      <td>0.951296</td>
      <td>0.011391</td>
      <td>0.011352</td>
      <td>0.977257</td>
      <td>0.314749</td>
      <td>0.133217</td>
      <td>0.552034</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 44 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Index([&#39;id&#39;, &#39;text&#39;, &#39;author&#39;, &#39;num_punctuations&#39;, &#39;num_stopwords&#39;,
       &#39;num_words&#39;, &#39;mnb_tfidf_eap&#39;, &#39;mnb_tfidf_hpl&#39;, &#39;mnb_tfidf_mws&#39;,
       &#39;mnb_count_eap&#39;, &#39;mnb_count_hpl&#39;, &#39;mnb_count_mws&#39;, &#39;mnb_tfidf_char_eap&#39;,
       &#39;mnb_tfidf_char_hpl&#39;, &#39;mnb_tfidf_char_mws&#39;, &#39;lr_count_eap&#39;,
       &#39;lr_count_hpl&#39;, &#39;lr_count_mws&#39;, &#39;fast_text_none_eap&#39;,
       &#39;fast_text_none_hpl&#39;, &#39;fast_text_none_mws&#39;, &#39;fast_text_gensim_eap&#39;,
       &#39;fast_text_gensim_hpl&#39;, &#39;fast_text_gensim_mws&#39;, &#39;fast_text_glove_eap&#39;,
       &#39;fast_text_glove_hpl&#39;, &#39;fast_text_glove_mws&#39;, &#39;cnn_none_eap&#39;,
       &#39;cnn_none_hpl&#39;, &#39;cnn_none_mws&#39;, &#39;cnn_gensim_eap&#39;, &#39;cnn_gensim_hpl&#39;,
       &#39;cnn_gensim_mws&#39;, &#39;cnn_glove_eap&#39;, &#39;cnn_glove_hpl&#39;, &#39;cnn_glove_mws&#39;,
       &#39;fast_text_char_none_eap&#39;, &#39;fast_text_char_none_hpl&#39;,
       &#39;fast_text_char_none_mws&#39;, &#39;fast_text_char_gensim_eap&#39;,
       &#39;fast_text_char_gensim_hpl&#39;, &#39;fast_text_char_gensim_mws&#39;,
       &#39;fast_text_char_glove_eap&#39;, &#39;fast_text_char_glove_hpl&#39;,
       &#39;fast_text_char_glove_mws&#39;],
      dtype=&#39;object&#39;)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[55]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Run XGboost:</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;xgboost_stacking_final&quot;</span>
<span class="n">model_preprocess_function</span> <span class="o">=</span> <span class="n">run_xgb_preprocess</span>
<span class="n">model_function</span> <span class="o">=</span> <span class="n">run_xgb</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;seed_val&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;child&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;colsample&#39;</span><span class="p">:</span> <span class="mf">0.3</span> <span class="p">}</span>
<span class="c1">#features_drop = [&#39;num_punctuations&#39;,&#39;num_stopwords&#39;,&#39;num_words&#39;]</span>
<span class="c1">#features_drop = [&#39;svm_tfidf_eap&#39;, &#39;svm_tfidf_hpl&#39;,&#39;svm_tfidf_mws&#39;,</span>
<span class="c1">#                &#39;rf_tfidf_eap&#39;, &#39;rf_tfidf_hpl&#39;, &#39;rf_tfidf_mws&#39;]</span>
<span class="c1">#features_drop = [&#39;fast_text_char_glove_eap&#39;, &#39;fast_text_char_glove_hpl&#39;, &#39;fast_text_char_glove_mws&#39;, </span>
<span class="c1">#                 &#39;fast_text_glove_eap&#39;, &#39;fast_text_glove_hpl&#39;, &#39;fast_text_glove_mws&#39;, </span>
<span class="c1">#                 &#39;cnn_glove_eap&#39;, &#39;cnn_glove_hpl&#39;, &#39;cnn_glove_mws&#39;]</span>
<span class="n">features_drop</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">model_preprocess_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;features_drop&#39;</span><span class="p">:</span> <span class="n">features_drop</span> <span class="p">}</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">run_kfold_training</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                           <span class="n">model_preprocess_function</span><span class="p">,</span> 
                                           <span class="n">model_preprocess_params</span><span class="p">,</span>
                                           <span class="n">model_function</span><span class="p">,</span>
                                           <span class="n">model_params</span><span class="p">,</span>
                                           <span class="s2">&quot;VECT&quot;</span><span class="p">,</span> 
                                           <span class="n">train_raw</span><span class="p">,</span> <span class="n">y_train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> 
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="n">xgb</span><span class="o">.</span><span class="n">plot_importance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Running kfold training with model xgboost_stacking_final
Shapes: x_train_raw.shape (19579, 45), y_train_raw.shape (19579,) , x_test_raw.shape (8392, 44)
Running preprocess: &lt;function run_xgb_preprocess at 0x7f64a2b12158&gt;
Shapes: x_train.shape (19579, 42), y_train.shape (19579,) , x_test.shape (8392, 42)
Running fold 1
[0]	train-mlogloss:0.99489	cross-valid-mlogloss:0.996059
Multiple eval metrics have been passed: &#39;cross-valid-mlogloss&#39; will be used for early stopping.

Will train until cross-valid-mlogloss hasn&#39;t improved in 50 rounds.
[20]	train-mlogloss:0.357634	cross-valid-mlogloss:0.370399
[40]	train-mlogloss:0.285885	cross-valid-mlogloss:0.305027
[60]	train-mlogloss:0.266458	cross-valid-mlogloss:0.29167
[80]	train-mlogloss:0.255798	cross-valid-mlogloss:0.286635
[100]	train-mlogloss:0.247489	cross-valid-mlogloss:0.284522
[120]	train-mlogloss:0.240158	cross-valid-mlogloss:0.283111
[140]	train-mlogloss:0.233112	cross-valid-mlogloss:0.28258
[160]	train-mlogloss:0.22696	cross-valid-mlogloss:0.282749
[180]	train-mlogloss:0.220761	cross-valid-mlogloss:0.281765
[200]	train-mlogloss:0.2151	cross-valid-mlogloss:0.281388
[220]	train-mlogloss:0.209764	cross-valid-mlogloss:0.28103
[240]	train-mlogloss:0.20482	cross-valid-mlogloss:0.280823
[260]	train-mlogloss:0.199924	cross-valid-mlogloss:0.28144
[280]	train-mlogloss:0.195522	cross-valid-mlogloss:0.282084
Stopping. Best iteration:
[244]	train-mlogloss:0.203734	cross-valid-mlogloss:0.280771

Running fold 2
[0]	train-mlogloss:0.994957	cross-valid-mlogloss:0.996138
Multiple eval metrics have been passed: &#39;cross-valid-mlogloss&#39; will be used for early stopping.

Will train until cross-valid-mlogloss hasn&#39;t improved in 50 rounds.
[20]	train-mlogloss:0.357542	cross-valid-mlogloss:0.371007
[40]	train-mlogloss:0.284863	cross-valid-mlogloss:0.306904
[60]	train-mlogloss:0.2658	cross-valid-mlogloss:0.293866
[80]	train-mlogloss:0.25492	cross-valid-mlogloss:0.289668
[100]	train-mlogloss:0.246663	cross-valid-mlogloss:0.287402
[120]	train-mlogloss:0.239143	cross-valid-mlogloss:0.285772
[140]	train-mlogloss:0.232208	cross-valid-mlogloss:0.284951
[160]	train-mlogloss:0.226052	cross-valid-mlogloss:0.284242
[180]	train-mlogloss:0.220049	cross-valid-mlogloss:0.284223
[200]	train-mlogloss:0.214272	cross-valid-mlogloss:0.284181
[220]	train-mlogloss:0.209069	cross-valid-mlogloss:0.284559
[240]	train-mlogloss:0.204124	cross-valid-mlogloss:0.284661
Stopping. Best iteration:
[209]	train-mlogloss:0.211855	cross-valid-mlogloss:0.284009

Running fold 3
[0]	train-mlogloss:0.995459	cross-valid-mlogloss:0.995887
Multiple eval metrics have been passed: &#39;cross-valid-mlogloss&#39; will be used for early stopping.

Will train until cross-valid-mlogloss hasn&#39;t improved in 50 rounds.
[20]	train-mlogloss:0.359933	cross-valid-mlogloss:0.364394
[40]	train-mlogloss:0.28762	cross-valid-mlogloss:0.29664
[60]	train-mlogloss:0.268303	cross-valid-mlogloss:0.282868
[80]	train-mlogloss:0.25711	cross-valid-mlogloss:0.278698
[100]	train-mlogloss:0.248783	cross-valid-mlogloss:0.276603
[120]	train-mlogloss:0.241371	cross-valid-mlogloss:0.275995
[140]	train-mlogloss:0.234912	cross-valid-mlogloss:0.275541
[160]	train-mlogloss:0.228865	cross-valid-mlogloss:0.275494
[180]	train-mlogloss:0.222747	cross-valid-mlogloss:0.27561
[200]	train-mlogloss:0.216999	cross-valid-mlogloss:0.275862
Stopping. Best iteration:
[154]	train-mlogloss:0.230487	cross-valid-mlogloss:0.275379

Running fold 4
[0]	train-mlogloss:0.99508	cross-valid-mlogloss:0.995546
Multiple eval metrics have been passed: &#39;cross-valid-mlogloss&#39; will be used for early stopping.

Will train until cross-valid-mlogloss hasn&#39;t improved in 50 rounds.
[20]	train-mlogloss:0.359066	cross-valid-mlogloss:0.366009
[40]	train-mlogloss:0.286764	cross-valid-mlogloss:0.299644
[60]	train-mlogloss:0.267457	cross-valid-mlogloss:0.286236
[80]	train-mlogloss:0.256943	cross-valid-mlogloss:0.281646
[100]	train-mlogloss:0.248592	cross-valid-mlogloss:0.278934
[120]	train-mlogloss:0.241352	cross-valid-mlogloss:0.278324
[140]	train-mlogloss:0.234646	cross-valid-mlogloss:0.277808
[160]	train-mlogloss:0.228471	cross-valid-mlogloss:0.277539
[180]	train-mlogloss:0.222246	cross-valid-mlogloss:0.277275
[200]	train-mlogloss:0.21659	cross-valid-mlogloss:0.27683
[220]	train-mlogloss:0.211481	cross-valid-mlogloss:0.27662
[240]	train-mlogloss:0.206477	cross-valid-mlogloss:0.276423
[260]	train-mlogloss:0.201601	cross-valid-mlogloss:0.276185
[280]	train-mlogloss:0.196648	cross-valid-mlogloss:0.276687
Stopping. Best iteration:
[231]	train-mlogloss:0.208626	cross-valid-mlogloss:0.276124

Running fold 5
[0]	train-mlogloss:0.994172	cross-valid-mlogloss:0.994354
Multiple eval metrics have been passed: &#39;cross-valid-mlogloss&#39; will be used for early stopping.

Will train until cross-valid-mlogloss hasn&#39;t improved in 50 rounds.
[20]	train-mlogloss:0.362262	cross-valid-mlogloss:0.370273
[40]	train-mlogloss:0.287382	cross-valid-mlogloss:0.300461
[60]	train-mlogloss:0.268155	cross-valid-mlogloss:0.287233
[80]	train-mlogloss:0.256901	cross-valid-mlogloss:0.281848
[100]	train-mlogloss:0.24793	cross-valid-mlogloss:0.279642
[120]	train-mlogloss:0.240459	cross-valid-mlogloss:0.278149
[140]	train-mlogloss:0.23362	cross-valid-mlogloss:0.277311
[160]	train-mlogloss:0.227271	cross-valid-mlogloss:0.276909
[180]	train-mlogloss:0.221323	cross-valid-mlogloss:0.276729
[200]	train-mlogloss:0.215852	cross-valid-mlogloss:0.276869
[220]	train-mlogloss:0.210448	cross-valid-mlogloss:0.277246
[240]	train-mlogloss:0.205172	cross-valid-mlogloss:0.277749
Stopping. Best iteration:
[192]	train-mlogloss:0.218064	cross-valid-mlogloss:0.276426



===== Model: xgboost_stacking_final  ========:
 Cross-val log losses are: [0.28077129560865283, 0.2840085180417305, 0.27537939469448852, 0.27612407379749465, 0.27642561327629245]
====== Mean cross-val log loss is: 0.2785417790837318 =========


</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA1AAAALJCAYAAAC6BcIOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[56]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">create_submission_final</span><span class="p">(</span><span class="s2">&quot;final_submission&quot;</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Creating submission file: final_submission_submission.csv
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Find-number-of-misclassifications-and-plot-confusion-matrix">Find number of misclassifications and plot confusion matrix<a class="anchor-link" href="#Find-number-of-misclassifications-and-plot-confusion-matrix">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[66]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">find_misclassifications</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">num_misclass</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">num_correctclass</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">y_true</span><span class="p">:</span>
        <span class="n">true_val</span> <span class="o">=</span> <span class="n">i</span>
        <span class="n">pred_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">true_val</span> <span class="o">!=</span> <span class="n">pred_val</span><span class="p">:</span>
            <span class="c1">#print(&quot;idx {}, true_val {}, pred_val {}&quot;.format(idx, true_val, pred_val))</span>
            <span class="c1">#display(train_raw.loc[idx,&#39;text&#39;])</span>
            <span class="c1">#display(train_raw.loc[idx,&#39;author&#39;])</span>
            <span class="n">num_misclass</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1">#break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">num_correctclass</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">idx</span><span class="o">+=</span><span class="mi">1</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;num_misclass </span><span class="si">{}</span><span class="s2">, num_correctclass </span><span class="si">{}</span><span class="s2">, accuracy </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                                            <span class="n">num_misclass</span><span class="p">,</span> <span class="n">num_correctclass</span><span class="p">,</span>
                                                            <span class="p">(</span><span class="n">num_correctclass</span><span class="o">*</span><span class="mf">100.</span><span class="o">/</span><span class="n">idx</span><span class="p">)))</span>
            
<span class="n">find_misclassifications</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">pred_train</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>num_misclass 2158, num_correctclass 17421, accuracy 88.97798661831554
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[63]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Plot confusion matrix:</span>
<span class="c1">#http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py</span>
    
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">confusion_matrix</span>

<span class="k">def</span> <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span>
                          <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Confusion matrix&#39;</span><span class="p">,</span>
                          <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function prints and plots the confusion matrix.</span>
<span class="sd">    Normalization can be applied by setting `normalize=True`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">cm</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Normalized confusion matrix&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Confusion matrix, without normalization&#39;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">tick_marks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>

    <span class="n">fmt</span> <span class="o">=</span> <span class="s1">&#39;.2f&#39;</span> <span class="k">if</span> <span class="n">normalize</span> <span class="k">else</span> <span class="s1">&#39;d&#39;</span>
    <span class="n">thresh</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">/</span> <span class="mf">2.</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">fmt</span><span class="p">),</span>
                 <span class="n">horizontalalignment</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span>
                 <span class="n">color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span> <span class="k">if</span> <span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span> <span class="k">else</span> <span class="s2">&quot;black&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True label&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted label&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[64]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cmatr</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cmatr</span><span class="p">))</span>

<span class="c1"># Plot non-normalized confusion matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cmatr</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;EAP&#39;</span><span class="p">,</span> <span class="s1">&#39;HPL&#39;</span><span class="p">,</span> <span class="s1">&#39;MWS&#39;</span><span class="p">],</span>
                      <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Confusion matrix without normalization&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>19579
Confusion matrix, without normalization
[[7109  315  476]
 [ 428 5006  201]
 [ 557  181 5306]]
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjMAAAI4CAYAAACflWgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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=
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[131]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Check correlation between predictions from different models:</span>

<span class="c1"># Get predictions with max probability</span>
<span class="n">new_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;mnb_tf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;mnb_tfidf_eap&#39;</span><span class="p">,</span> <span class="s1">&#39;mnb_tfidf_hpl&#39;</span><span class="p">,</span> <span class="s1">&#39;mnb_tfidf_mws&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;mnb_cnt&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;mnb_count_eap&#39;</span><span class="p">,</span> <span class="s1">&#39;mnb_count_hpl&#39;</span><span class="p">,</span> <span class="s1">&#39;mnb_count_mws&#39;</span><span class="p">]</span> <span class="p">]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;mnb_tfc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;mnb_tfidf_char_eap&#39;</span><span class="p">,</span><span class="s1">&#39;mnb_tfidf_char_hpl&#39;</span><span class="p">,</span> <span class="s1">&#39;mnb_tfidf_char_mws&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;lr_cnt&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;lr_count_eap&#39;</span><span class="p">,</span> <span class="s1">&#39;lr_count_hpl&#39;</span><span class="p">,</span> <span class="s1">&#39;lr_count_mws&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;ft_n&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;fast_text_none_eap&#39;</span><span class="p">,</span> <span class="s1">&#39;fast_text_none_hpl&#39;</span><span class="p">,</span> <span class="s1">&#39;fast_text_none_mws&#39;</span><span class="p">]</span> <span class="p">]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;ft_gs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;fast_text_gensim_eap&#39;</span><span class="p">,</span> <span class="s1">&#39;fast_text_gensim_hpl&#39;</span><span class="p">,</span> <span class="s1">&#39;fast_text_gensim_mws&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;ft_gl&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;fast_text_glove_eap&#39;</span><span class="p">,</span> <span class="s1">&#39;fast_text_glove_hpl&#39;</span><span class="p">,</span> <span class="s1">&#39;fast_text_glove_mws&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;cnn_n&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;cnn_none_eap&#39;</span><span class="p">,</span> <span class="s1">&#39;cnn_none_hpl&#39;</span><span class="p">,</span> <span class="s1">&#39;cnn_none_mws&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;cnn_gs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;cnn_gensim_eap&#39;</span><span class="p">,</span> <span class="s1">&#39;cnn_gensim_hpl&#39;</span><span class="p">,</span> <span class="s1">&#39;cnn_gensim_mws&#39;</span><span class="p">]</span> <span class="p">]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;cnn_gl&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;cnn_glove_eap&#39;</span><span class="p">,</span> <span class="s1">&#39;cnn_glove_hpl&#39;</span><span class="p">,</span> <span class="s1">&#39;cnn_glove_mws&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;ftc_n&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;fast_text_char_none_eap&#39;</span><span class="p">,</span><span class="s1">&#39;fast_text_char_none_hpl&#39;</span><span class="p">,</span><span class="s1">&#39;fast_text_char_none_mws&#39;</span><span class="p">]</span> <span class="p">]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;ftc_gs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;fast_text_char_gensim_eap&#39;</span><span class="p">,</span><span class="s1">&#39;fast_text_char_gensim_hpl&#39;</span><span class="p">,</span><span class="s1">&#39;fast_text_char_gensim_mws&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;ftc_gl&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_raw</span><span class="p">[[</span><span class="s1">&#39;fast_text_char_glove_eap&#39;</span><span class="p">,</span> <span class="s1">&#39;fast_text_char_glove_hpl&#39;</span><span class="p">,</span><span class="s1">&#39;fast_text_char_glove_mws&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Convert it to numbers 0,1,2 for authors</span>
<span class="n">display</span><span class="p">(</span><span class="n">new_df</span><span class="p">)</span>
<span class="n">new_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_df</span><span class="o">.</span><span class="n">applymap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;.*_eap$&#39;</span><span class="p">,</span><span class="s1">&#39;0&#39;</span><span class="p">,</span><span class="n">x</span><span class="p">)))</span>
<span class="n">new_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_df</span><span class="o">.</span><span class="n">applymap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;.*_hpl$&#39;</span><span class="p">,</span><span class="s1">&#39;1&#39;</span><span class="p">,</span><span class="n">x</span><span class="p">)))</span>
<span class="n">new_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_df</span><span class="o">.</span><span class="n">applymap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;.*_mws$&#39;</span><span class="p">,</span><span class="s1">&#39;2&#39;</span><span class="p">,</span><span class="n">x</span><span class="p">)))</span>
<span class="n">new_df</span> <span class="o">=</span> <span class="n">new_df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">new_df</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mnb_tf</th>
      <th>mnb_cnt</th>
      <th>mnb_tfc</th>
      <th>lr_cnt</th>
      <th>ft_n</th>
      <th>ft_gs</th>
      <th>ft_gl</th>
      <th>cnn_n</th>
      <th>cnn_gs</th>
      <th>cnn_gl</th>
      <th>ftc_n</th>
      <th>ftc_gs</th>
      <th>ftc_gl</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>1</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>2</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>3</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_mws</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_mws</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>4</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>5</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_mws</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_mws</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_mws</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_mws</td>
    </tr>
    <tr>
      <th>6</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>7</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>8</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>9</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_mws</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>10</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_mws</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_mws</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_mws</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_mws</td>
    </tr>
    <tr>
      <th>11</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>12</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>13</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>14</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>15</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_mws</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_mws</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_mws</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_mws</td>
    </tr>
    <tr>
      <th>16</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_mws</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_mws</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_mws</td>
    </tr>
    <tr>
      <th>17</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_mws</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_mws</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_mws</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_mws</td>
    </tr>
    <tr>
      <th>18</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>20</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>21</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>22</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_mws</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>23</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>24</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>25</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>26</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>27</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_mws</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_mws</td>
    </tr>
    <tr>
      <th>28</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>29</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>19549</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_mws</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_mws</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_mws</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_mws</td>
    </tr>
    <tr>
      <th>19550</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19551</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19552</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19553</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19554</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>19555</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19556</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>19557</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19558</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19559</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>19560</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19561</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19562</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19563</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>19564</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>19565</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19566</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_mws</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_mws</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19567</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19568</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>19569</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_mws</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_mws</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_mws</td>
    </tr>
    <tr>
      <th>19570</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_mws</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_mws</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_mws</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_mws</td>
    </tr>
    <tr>
      <th>19571</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_mws</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19572</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_mws</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_mws</td>
    </tr>
    <tr>
      <th>19573</th>
      <td>mnb_tfidf_mws</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_mws</td>
      <td>fast_text_none_mws</td>
      <td>fast_text_gensim_mws</td>
      <td>fast_text_glove_mws</td>
      <td>cnn_none_mws</td>
      <td>cnn_gensim_mws</td>
      <td>cnn_glove_mws</td>
      <td>fast_text_char_none_mws</td>
      <td>fast_text_char_gensim_mws</td>
      <td>fast_text_char_glove_mws</td>
    </tr>
    <tr>
      <th>19574</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_hpl</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_hpl</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_hpl</td>
    </tr>
    <tr>
      <th>19575</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_mws</td>
      <td>mnb_tfidf_char_mws</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19576</th>
      <td>mnb_tfidf_eap</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_eap</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19577</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_eap</td>
      <td>mnb_tfidf_char_eap</td>
      <td>lr_count_eap</td>
      <td>fast_text_none_eap</td>
      <td>fast_text_gensim_eap</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_eap</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_eap</td>
      <td>fast_text_char_gensim_eap</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
    <tr>
      <th>19578</th>
      <td>mnb_tfidf_hpl</td>
      <td>mnb_count_hpl</td>
      <td>mnb_tfidf_char_hpl</td>
      <td>lr_count_hpl</td>
      <td>fast_text_none_hpl</td>
      <td>fast_text_gensim_hpl</td>
      <td>fast_text_glove_eap</td>
      <td>cnn_none_hpl</td>
      <td>cnn_gensim_hpl</td>
      <td>cnn_glove_eap</td>
      <td>fast_text_char_none_hpl</td>
      <td>fast_text_char_gensim_hpl</td>
      <td>fast_text_char_glove_eap</td>
    </tr>
  </tbody>
</table>
<p>19579 rows × 13 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mnb_tf</th>
      <th>mnb_cnt</th>
      <th>mnb_tfc</th>
      <th>lr_cnt</th>
      <th>ft_n</th>
      <th>ft_gs</th>
      <th>ft_gl</th>
      <th>cnn_n</th>
      <th>cnn_gs</th>
      <th>cnn_gl</th>
      <th>ftc_n</th>
      <th>ftc_gs</th>
      <th>ftc_gl</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>17</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>25</th>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>28</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>29</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>19549</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>19550</th>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19551</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19552</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19553</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19554</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>19555</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19556</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>19557</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19558</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19559</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>19560</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19561</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19562</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19563</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>19564</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>19565</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19566</th>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19567</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19568</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>19569</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>19570</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>19571</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19572</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>19573</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>19574</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>19575</th>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19576</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19577</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19578</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>19579 rows × 13 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[134]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Plot heatmap of different predictions:</span>
<span class="n">corr</span> <span class="o">=</span> <span class="n">new_df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Heatmap of correlation between different predictions&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">corr</span><span class="p">,</span> 
            <span class="n">xticklabels</span><span class="o">=</span><span class="n">corr</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
            <span class="n">yticklabels</span><span class="o">=</span><span class="n">corr</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">);</span>
<span class="c1">#pd.scatter_matrix(new_df, alpha = 0.3, figsize = (14,8), diagonal = &#39;kde&#39;);</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAegAAAIDCAYAAADPO43QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
