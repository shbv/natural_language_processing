{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Features, and other misc.:\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Models:\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.layers import Dense, Flatten, Embedding, Dropout, SpatialDropout1D \n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D \n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.models import Sequential \n",
    "import xgboost as xgb\n",
    "\n",
    "# Metrics:\n",
    "from sklearn.metrics import log_loss, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import make_scorer\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test.csv', 'train.csv']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(\"../input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) (8392, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_raw = pd.read_csv(\"../input/train.csv\")\n",
    "test_raw = pd.read_csv(\"../input/test.csv\")\n",
    "print(train_raw.shape, test_raw.shape)\n",
    "display(train_raw.head())\n",
    "display(test_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis and cleanup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19579 entries, 0 to 19578\n",
      "Data columns (total 3 columns):\n",
      "id        19579 non-null object\n",
      "text      19579 non-null object\n",
      "author    19579 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 459.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8392 entries, 0 to 8391\n",
      "Data columns (total 2 columns):\n",
      "id      8392 non-null object\n",
      "text    8392 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 131.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Is data clean and mem usage\n",
    "train_raw.info()\n",
    "test_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EAP    7900\n",
       "MWS    6044\n",
       "HPL    5635\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmaFODjMzKwQB4eZmRXi4DAzs0L+P3+iLrBVQ4I1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f649b4871d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Is number of training examples per class balanced?\n",
    "plt.figure()\n",
    "sns.countplot(x='author',data=train_raw)\n",
    "#sns.countplot(train.author)\n",
    "plt.title(\"Num of training examples per class (i.e author)\");\n",
    "train_raw['author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===EAP===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===MWS===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===HPL===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It never once occurred to me that the fumbling might be a mere mistake.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Herbert West needed fresh bodies because his life work was the reanimation of the dead.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Peak at the sentences:\n",
    "for author in [\"EAP\", \"MWS\", \"HPL\"]:\n",
    "    print(\"===\" + str(author) + \"===\")\n",
    "    for i in range(3):\n",
    "        display(train_raw[train_raw['author'] == author]['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   num_punctuations  num_stopwords  num_words  \n",
       "0                 7             19         41  \n",
       "1                 1              8         14  \n",
       "2                 5             16         36  \n",
       "3                 4             13         34  \n",
       "4                 4             11         27  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "3  id27757  While I was thinking how I should possibly man...   \n",
       "4  id04081  I am not sure to what limit his knowledge may ...   \n",
       "\n",
       "   num_punctuations  num_stopwords  num_words  \n",
       "0                 3              9         19  \n",
       "1                 7             33         62  \n",
       "2                 3             15         33  \n",
       "3                 5             19         41  \n",
       "4                 1              6         11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "num_punctuations ---> 7\n",
      "num_stopwords ---> 19\n",
      "num_words ---> 41\n",
      "It never once occurred to me that the fumbling might be a mere mistake.\n",
      "num_punctuations ---> 1\n",
      "num_stopwords ---> 8\n",
      "num_words ---> 14\n",
      "In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.\n",
      "num_punctuations ---> 5\n",
      "num_stopwords ---> 16\n",
      "num_words ---> 36\n",
      "How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.\n",
      "num_punctuations ---> 4\n",
      "num_stopwords ---> 13\n",
      "num_words ---> 34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f649b18dc50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmcXFWd///Xp/fupDu9pLOns5CE\nfY8BER1UmEFBZBTGBTEoin4dBRQQUBxWBZdRkPk5yqJGEBCQfcdIBhEkhoSQBULI0gnpTtJJutP7\nVvX5/XFvJ5Wml6pOV1V38n4+HvWoe2/de+pTdavqU+ece881d0dERPZvGekOQERE0k/JQERElAxE\nRETJQEREUDIQERGUDEREBCWDATOzX5vZDwaprAozazSzzHB+gZl9ZTDKDst72szmDlZ5CTzvDWa2\nzcw2p/q594aZ/buZbQz3ydHpjidVwtc7Pd1xDFWD/b0capQMemBm682sxcwazKzOzF42s6+b2a73\ny92/7u7Xx1nWyX2t4+4b3H2ku0cGIfZrzOzubuV/zN3n7W3ZCcYxGbgEOMTdx6XyuQfBz4Bvhvtk\nSbqDATCz35vZDYNY3nt+2MLXu3awnmM46+l7tK9TMujdJ9y9EJgC3ARcDtw52E9iZlmDXeYQMQXY\n7u5b0x3IAEwBVqQ7CInfcPoeDdlY3V23bjdgPXByt2VzgChwWDj/e+CGcHo08ARQB+wA/kaQaO8K\nt2kBGoHvAlMBB84HNgAvxizLCstbANwILAR2Ao8CpeFjJwHv9hQvcCrQDnSEz7c0pryvhNMZwFVA\nJbAV+AMwKnysK465YWzbgO/38T6NCrevCcu7Kiz/5PA1R8M4ft/DticB7xLUHrYC1cCXYh7fFXM4\nfx7wUsy8A98AVgMNwPXAAcArQD1wP5DTS9w9vgdAbhivA03Aml62d+BCYG34Hv0UyAgfuwa4O2bd\nnvbt9cDfw7ifA0bHrH8i8DLBZ2lj+LovCPdpexjf4zFxzIjZ9vfs/kyWEHwma4DacHpS+NgPgQjQ\nGpb3P93L623fxu4LghpULbAO+Fi3fbU2fH3rgHN6eR+vAR4E/hSuuxg4MubxCcCfwxjWARf2sO3d\n4f7+Sg/lnwYsCR/fCFzT/fM3gO9RX/vuDII/EXXhugd3K/ty4A2grevzMJRuaQ9gKN7oIRmEyzcA\n/y+cjv3i3Qj8GsgObx8ErKey2P3j8AdgBJBPzz8Ym4DDwnX+TPgD09eHOJy+hpgfo5jyupLBl4F3\ngOnASOAh4K5usd0exnVk+ME9uJf36Q8Eiaow3PZt4Pze4uy27UlAJ3Bd+J59HGgGSrrHHM6fx3uT\nwWNAEXBoGOf88HWNAlYCc3t57l7fg5iyZ/QRuwMvAKVARfi6u97fPd7/XvbtGmBW+B4vAG4KH6sg\n+JH5XPielAFHdf+89RYne34my4BPAwXh/nkAeKSnz0RP5fWzb88j+KH8KpAJ/D+gCjCCz2s9cGC4\n7njg0F7ex2vCcs4KX++lBD/62QQJ+zXgv4CccF+tBf6t27Znhuvm9/IZOzx8/AhgC3DmXn6Pett3\nswj+QJwSxv9dgs9YTkzZrwOTe4p1KNzUTJSYKoIfgO46CD70U9y9w93/5uEnoA/XuHuTu7f08vhd\n7r7c3ZuAHwD/0dXBvJfOAX7u7mvdvRG4Evhst6rrte7e4u5LgaUESWEPYSyfAa509wZ3Xw/8N3Bu\nArF0ANeF79lTBP/CDkxg+x+7e727rwCWA8+Fr2sn8DTQW+dvPO9BPM+9w903ADcT/IDH63fu/na4\n7+8HjoqJ6y/ufm/4nmx399cTKHeXcNs/u3uzuzcQ1Ab+JZ5t49y3le5+uwf9XPMIPv9jw8eiwGFm\nlu/u1eH+6c1r7v6gu3cAPwfygOOB9wHl7n6du7d70JdxO/DZmG1fcfdH3D3a0/fI3Re4+7Lw8TeA\ne+N9D/rQ2777DPCkuz8fvpafESSME2K2/aW7b+zjO59WSgaJmUjQDNTdTwn+BTxnZmvN7Io4ytqY\nwOOVBP82RscVZd8mhOXFlp3F7i8yQOzRP80E/567G03wj617WRMTiGW7u3fG8Vy92RIz3dLDfG9l\nxfMe9Kf7/pmQwLa9vb+TCf557jUzKzCz35hZpZnVEzRHFsf5hyKefbvrNbh7czg5Mvzz8hng60C1\nmT1pZgf18Vy73kd3jxI0HU4g6LeZEB7AUWdmdcD32HMf9fkdMrPjzOwFM6sxs51hTHv7Hept3+3x\nmQpfy0b2fM/6+86nlZJBnMzsfQQ79qXuj4X/ni5x9+nAJ4DvmNlHux7upcj+ag6TY6YrCP5FbyOo\nihbExJUJlCdQbhXBFy227E72/CGNx7Ywpu5lbUqwnN7s8TqBwTwiaTDeg+77pyqc3pu4NxL0e/Sk\np/3a3MdzXUJQyzrO3YuAD4XLrY/yuuzVvnX3Z939FILawlsE/+h7s+t9DI/Wm0TwXm4E1rl7ccyt\n0N0/HvtU/YRyD0FT4mR3H0XQlNv1+vf2e9TdHp8pM7PwtcW+Z0N6iGglg36YWZGZnQ7cR9CGuKyH\ndU43sxnhB6CeoHOu6zDRLQTtnYn6gpkdYmYFBO3qD4ZV8reBPDM7zcyyCTr2cmO22wJMjT0Mtpt7\ngW+b2TQzGwn8CPhTt3/o/QpjuR/4oZkVmtkU4DsEHXqD4XXgU+E/3BkEHe6DZTDeg8vMrCQ8hPYi\ngk7Qrrg/FJ47MoqgCSpefwRONrP/MLMsMyszs65miJ4+R68DnzezTDM7lT2bQAoJakd1ZlYKXN1t\n214/l3uzb81srJmdYWYjCPpxGtn9XejJsWb2qbCJ7uJwm38QHDxRb2aXm1l++BoPC/+UxasQ2OHu\nrWY2B/h8zGN7+z3q7n7gNDP7aFjeJeFreTmBeNNKyaB3j5tZA8E/lO8TtGd+qZd1ZwJ/IfjgvwL8\nyt0XhI/dCFwVVnUvTeD57yLoENxM0I56IUDYHv4N4A6Cfx1NBFXrLg+E99vNbHEP5f42LPtFgs66\nVuBbCcQV61vh868lqDHdE5Y/GH5BcETHFoI26T8OUrkwOO/BowQdnK8DTxIeduzuzxMkhjfCx5+I\nt8Cw/+HjBD8kO8Kyu/pr7gQOCT9Hj4TLLiKoidYR9Dc8ElPczQRt1tsIflyf6fZ0twBnmVmtmf2y\nh3AGum8zwvirwtfwLwSf1948StCsVEvQJ/GpsL8kEr62owj20TaCz/yoOGLo8g3guvB7/F8EP9jA\noHyP9uDuq4AvALeGsX6C4PD09gTiTauuI15EJE5m5sBMd38n3bEMZ2Z2DcHRS19IdyyimoGIiKBk\nICIiqJlIRERQzUBERAhOtBkWRo8e7VOnTk13GCIiw8prr722zd3L+1tv2CSDqVOnsmjRonSHISIy\nrJhZZf9rqZlIRERQMhAREZQMREQEJQMREUHJQEREUDIQERGUDEREBCWDIUPDgohIOikZDAE/+9nP\nuPbaa9Mdhojsx5QMhoDHHnuMv/71r+kOQ0T2Y0oGIiKS3GRgZgea2esxt3ozu9jMSs3seTNbHd6X\nJDMOERHpW1KTgbuvcvej3P0o4FigGXgYuAKY7+4zgfnhvIiIpEkqm4k+Cqxx90rgkwQXOSe8PzOF\ncYiISDepTAafBe4Np8e6ezVAeD+mpw3M7AIzW2Rmi2pqalIUpojI/iclycDMcoAzgAcS2c7db3P3\n2e4+u7y832sziIjIAKWqZvAxYLG7bwnnt5jZeIDwfmuK4hARkR6kKhl8jt1NRACPAXPD6bnAoymK\nQ0REepD0ZGBmBcApwEMxi28CTjGz1eFjNyU7DhER6V3Sr4Hs7s1AWbdl2wmOLhIRkSFAZyCLiIiS\ngYiIKBmIiAhKBiIigpKBiIigZCAiIigZiIgISgYiIoKSgYiIoGQgIiIoGYiICEoGIiKCkoGIiKBk\nICIiKBmIiAhKBiIigpKBiIigZCAiIqTmGsjFZvagmb1lZm+a2fvNrNTMnjez1eF9SbLjEBGR3qWi\nZnAL8Iy7HwQcCbwJXAHMd/eZwPxwXkRE0iSpycDMioAPAXcCuHu7u9cBnwTmhavNA85MZhwiItK3\nZNcMpgM1wO/MbImZ3WFmI4Cx7l4NEN6PSXIcIiLvsWLFCm677bZ0hzEkJDsZZAHHAP/r7kcDTSTQ\nJGRmF5jZIjNbVFNTk6wYRWQ/dfXVV3P33XenO4whIdnJ4F3gXXd/NZx/kCA5bDGz8QDh/daeNnb3\n29x9trvPLi8vT3KoIrK/2bq1x5+e/VJSk4G7bwY2mtmB4aKPAiuBx4C54bK5wKPJjENERPqWlYLn\n+BbwRzPLAdYCXyJIQveb2fnABuDsFMQhIiK9SHoycPfXgdk9PPTRZD+3iIjER2cgi4iIkoGIiCgZ\niIgISgYiIoKSgYiIoGQgIiIoGYiICEoGIiKCkoGIiKBkICIiKBmIiAhKBiIigpKBiIigZCAiIigZ\niIgISgYiIsIAk4GZZZhZ0WAHIyIi6RF3MjCze8ysyMxGEFzHeJWZXZa80EREJFUSqRkc4u71wJnA\nU0AFcG5SohIRkZRKJBlkm1k2QTJ41N07AO9vIzNbb2bLzOx1M1sULis1s+fNbHV4XzKw8EVEZDAk\nkgx+A6wHRgAvmtkUoD7ObT/s7ke5++xw/gpgvrvPBOaH8yIikiZxJwN3/6W7T3T3j3ugEvjwAJ/3\nk8C8cHoeQW1DRETSJCveFc0sF/g0MLXbdtf1s6kDz5mZA79x99uAse5eDeDu1WY2ppfnvAC4AKCi\noiLeUEVEJEFxJwPgUWAn8BrQlsB2H3D3qvAH/3kzeyveDcPEcRvA7Nmz++2fEBGRgUkkGUxy91MT\nfQJ3rwrvt5rZw8AcYIuZjQ9rBeOBrYmWKyIigyeRDuSXzezwRAo3sxFmVtg1DfwrsBx4DJgbrjaX\noNYhIiJpkkjN4ETgPDNbR9BMZIC7+xF9bDMWeNjMup7rHnd/xsz+CdxvZucDG4CzBxS9iIgMikSS\nwccSLdzd1wJH9rB8O/DRRMsTEZHkSOTQ0kqgGPhEeCsOl4mIyDCXyNhEFwF/BMaEt7vN7FvJCkxE\nRFInkWai84Hj3L0JwMx+DLwC3JqMwEREJHUSOZrIgEjMfCRcJiIiw1wiNYPfAa+G5wpAMITEnYMf\nkoiIpFrcycDdf25mCwgOMTXgS+6+JFmBiYhI6vSbDMysyN3rzayUYNTS9TGPlbr7juSFJyIiqRBP\nzeAe4HSCMYlixweycH56EuISEZEU6jcZuPvp4f205IcjIiLpkMh5BvPjWSYiIsNPPH0GeUABMDq8\nPGXX4aRFwIQkxiYiIikST5/B14CLCX74X2N3MqgH/r8kxSUiIikUT5/BLcAtZvYtd9fZxiIi+6BE\nzjO41cwOAw4B8mKW/yEZgYmISOokcg3kq4GTCJLBUwRDWr8EKBmIiAxziYxNdBbBNQg2u/uXCK5T\nkJuUqEREJKUSSQYt7h4FOs2siOC6xTrhTERkH5DIQHWLzKwYuJ3gqKJGYGFSohIRkZRKpAP5G+Hk\nr83sGaDI3d+IZ1szywQWAZvc/XQzmwbcB5QCi4Fz3b09sdBFRGSwJHIG8oe6bkAFUBxOx+Mi4M2Y\n+R8Dv3D3mUAtwYVzREQkTRJpJrosZjoPmEPQXPSRvjYys0nAacAPge+YmYXbfD5cZR5wDfC/CcQi\nIiKDKJFmok/EzpvZZOAncWx6M/BdoDCcLwPq3L0znH8XmNjThmZ2AXABQEVFRbyhiohIghI5mqi7\nd4HD+lrBzE4Htrr7a7GLe1jVe1iGu9/m7rPdfXZ5efnAIxURkT4lctLZrez+0c4AjgKW9rPZB4Az\nzOzjBE1LRQQ1hWIzywprB5OAqkQDFxGRwZPQoaUx053Ave7+9742cPcrgSsBzOwk4FJ3P8fMHiA4\nie0+YC7waCJBi4jI4EokGRSHg9btYmYXdV8Wp8uB+8zsBmAJcOcAyhARkUGSSJ/B3B6WnRfvxu6+\nIOaqaWvdfY67z3D3s929LYE4RERkkMVzcZvPERwGOs3MHot5qBDYnqzAREQkdeJpJnoZqAZGA/8d\ns7wBiOsMZBERGdriubhNJVBpZucAVe7eCmBm+QRHAq1PaoQiIpJ0ifQZ3A9EY+YjwAODG46IiKRD\nIskgK3YwuXA6Z/BDEhGRVEskGdSY2RldM2b2SWDb4IckIiKplsh5Bl8H/mhm/0MwpMRG4ItJiUpE\nRFIqkYHq1gDHm9lIwNy9IXlhiYhIKiUyNlEu8GlgKpAVjEQN7n5dUiITEZGUSaSZ6FFgJ8E1DHTG\nsIjIPiSRZDDJ3U9NWiQiIpI2iRxN9LKZHZ60SEREJG0SqRmcCJxnZusImokMcHc/IimRiYhIyiSS\nDD6WtChERCStEkkGPV6aUkREhr9EksGTBAnBCC5hOQ1YBRyahLhERCSFEjnpbI/OYzM7BvjaoEck\nIiIpl8jRRHtw98XA+wYxFhERSZNEzkD+TsxsBnAMUNPPNnnAi0Bu+FwPuvvVZjYNuA8oBRYD58aO\niCoiIqmVSM2gMOaWS9CH8Ml+tmkDPuLuRwJHAaea2fHAj4FfuPtMoBY4P9HARURk8CTSZ3AtgJkV\nBbP9D1Tn7g40hrPZ4c2BjxBcVxlgHnAN8L9xRy0iIoMq7pqBmc02s2UE1z1eZmZLzezYOLbLNLPX\nga3A88AaoM7dO8NV3gUm9rLtBWa2yMwW1dT02SIlIiJ7IZFmot8C33D3qe4+FfhP4Hf9beTuEXc/\niuB6yXOAg3tarZdtb3P32e4+u7y8PIFQRUQkEYkkgwZ3/1vXjLu/BMR9TQN3rwMWAMcDxWbW1UQ1\nCahKIA4RERlkiSSDhWb2GzM7ycz+xcx+BSwws2PCcw7ew8zKzaw4nM4HTgbeBF4AzgpXm0swPLbI\nsLR48WJaW1vTHYbIXknkDOSjwvuruy0/gd2dwt2NB+aZWSZB4rnf3Z8ws5XAfWZ2A7AEuDOxsEWG\nhvXr13PxxRdz7rnn8tWvfjXd4YgMWCJHE324r8fNbK67z+u2zRvA0T2UtZag/0BkWKurqwNg6dKl\naY5EZO8M+AzkHlw0iGWJiEgKDWYysEEsS0REUmgwk4GGuBYRGaZUMxDZC2bBxz442V5k+BrMZPD3\nQSxLZFjoSgYiw10io5YWA18EpsZu5+4XhvffHOzgRIY61QhkX5HIeQZPAf8AlgHR5IQjMrx0JQPV\nEGS4SyQZ5Ln7d/pfTUREhptE+gzuMrOvmtl4MyvtuiUtMhERSZlEagbtwE+B77P7MFIHpg92UCLD\njfoOZLhLJBl8B5jh7tuSFYzIcKO+AtlXJNJMtAJoTlYgIsOROpBlX5FIzSACvG5mLxBc2xjYfWip\nyP5IJ53JviKRZPBIeBORkGoGsq9IZAjref2vJSIiw1EiZyCvo4fB6NxdRxOJiAxziTQTzY6ZzgPO\nBnSegYjIPiDuo4ncfXvMbZO730zPl7oUEZFhJpFmotiL3mcQ1BQKBz0ikWFERxPtGyKRCJmZmekO\nI60SaSb6b3b3GXQC6wmainplZpOBPwDjCAa3u83dbwmHsfgTwQio64H/cPfaRAIXGQp0NNG+Qck8\nsZPOPgbcCcwnuHbBJuCz/WzTCVzi7gcDxwP/aWaHAFcA8919ZljeFYkGLiIyWCKRSLpDSLtEksEj\nwCeADqAxvDX1tYG7V7v74nC6AXgTmAh8Eug6VHUecGZiYYsMDWom2jdo/yXWTDTJ3U8d6BOZ2VTg\naOBVYKy7V0OQMMxsTC/bXABcAFBRUTHQpxZJGjUT7RuiUV2iJZGawctmdvhAnsTMRgJ/Bi529/p4\nt3P329x9trvPLi8vH8hTi4j0S81EidUMTgTOC08+awMMcHc/oq+NzCybIBH80d0fChdvMbPxYa1g\nPLB1ALGLiMggSSQZfCzRwi2oO98JvOnuP4956DFgLnBTeP9oomWLiAwW9RkkNjZR5QDK/wBwLrDM\nzF4Pl32PIAncb2bnAxvo5xBVEZFk2t/PMYDEagYJc/eXCJqTevLRZD63SCroaKJ9g/ZfYh3IkmT6\nQA4/2mf7htbW1nSHkHZKBkOIjmgYfnRI6b5ByUDJYEhRMhh+dJ7BvqG5WVf0VTJIs9hmhs7OzjRG\nIntDzUXDW3Z2drpDSDslgzRrb2/fNa2qqkh6ZGUl9ViaYUHJIM1iE4CSwfClZiIZ7pQM0kzJYN+g\nZqLhTX0GSgZpF5sA2tra0hiJ7A3VDIa3+vq4h0zbZykZpFlsn0HstIikjg7eUDJIu9jagGoGw4+G\nPt436LBuJYO0a2ho2DWtqurw05XA1WcwvCmpKxmkXV1d3a7pnTt3pjESGQj9o9w3dHR0pDuEtFMy\nSLPt27f3OC3DQ1fNTv8shzcdyZfkUUulf7W1tVhmNmTlUltbm+5wJEGNjY2AhkAejmITuJpoVTNI\nu7a2NsjKhswsdSAPQxrGYPiKrYlv3aqLLSoZpFlzczNkZBG1LJqamtIdjiSopaUFUDPRcLRp06Ye\np/dXaiZKs6qqajqzR+CZOWyqqk53OJKgro5HnXQ2/HT193iO73FU3/5KNYM0cncqN2wgmltENK+I\n6uoqHdUwzKiZaPjq6u8hH+ob1GeQ1GRgZr81s61mtjxmWamZPW9mq8P7kmTGMJRt2LCBpsYGoiPH\nEB0xhs6ODlavXp3usCQBXT8onUriw86WLVsA8DJn69at+/25IsmuGfweOLXbsiuA+e4+E5gfzu+X\n3njjDQAihWOJFo4FYOnSpekMSRLU1Vewv/+QDEcL/7kQG2VQBm2tbaxcuTLdIaVVUpOBu78I7Oi2\n+JPAvHB6HnBmMmMYypYsWYLlFuB5o/CcAigoZsmSJekOSxLQNZ5UR4eOBBtOqqurWb5sOZHJEXyi\nY1nGc889l+6w0iodfQZj3b0aILwf09uKZnaBmS0ys0U1NTUpCzAVotEoC/+5iI6R4yHsfOwYOZ4l\nS17XgHXDSFdTw+bNW9IciSSi60+XT3TIhujoKP9c9M80R5VeQ7oD2d1vc/fZ7j67vLw83eEMqpUr\nV1K/s47O4opdyyIlFbS1tap2MIxs2bIZgOaWVh2RMowsW7YMyzUoDOa93Hl347v79ZAw6UgGW8xs\nPEB4v1+e7TF//nzIyCBSPImcylfIqXyFSNF4LDM7eEyGvNWrV7Nq1dvMGdOOu/PUU0+lOySJ01ur\n3iJaHMWWGva64SVBn8/+fABHOpLBY8DccHou8GgaYkir5uZmnnrqaTpLpkFWLhlN28lo2g4ZWbSX\nzeAv8+fvMYCdDE2rVq0C4LMzWigvgLfeeivNEUk8qqurWfPOGny0Y3WG1RmUgmUaL730UrrDS5tk\nH1p6L/AKcKCZvWtm5wM3AaeY2WrglHB+v/L444/T0tJMx9hD3/NYx9hD6Ozo4KGHHkpDZJKI7oOb\nabCz4aHru+VTY44Ay4bIhAjPPPvMfttUlOyjiT7n7uPdPdvdJ7n7ne6+3d0/6u4zw/vuRxvt05qb\nm/nDXXcRGTWRaOF7+869oITO0mnce999qh0MYe7OIw8/REVhlPL8KMeObuWVV17Z1aEsQ9OaNWt4\n4MEHiE6NQsGej/nBTnNLM7/61a/SE1yaDekO5H3RPffcQ0N9Pe2Tju11nfZJx9DW1sa8efN6XUfS\nq7W1lQ0b3+XY0W2YwezyDqLR6H7d5jzUtba2cs211+DZjh/Rw3khoyA6K8rTTz/NggULUh5fuikZ\npFBVVRX33HsvnWUHEB3Z6xG1eH4JHeUH8fDDD7N27doURijxaG5u5uqr/wuATU2Z3LUqnymFnYzI\ngf+59ZesX78+vQHKe7g7P/3pT6lcX0nn+zoht5f1DnUogx/d+CMqKytTG2SaKRmkSEdHB9ffcAOR\nKLRPntPv+u2TjsUzs7nu+ht2jYwpQ8N9993Hq/94lfMOaqK+3ahsyCQ/Cy45op7GHZv56U9/ku4Q\npZu7776b559/nuihURjXx4oZEDk+Qru3c9l3L9uvmmqVDFLA3fn5z3/OiuXLaZl2Ip47ov+NsvNo\nmX4Sa9es4cYbb9RwB0NEe3s7zz77LKX5cPKkPU8OnFUc4cjSNpYtW862bdvSFKHEcnfuuusubr/9\ndqIVUfzgOL5HBdBxQgdbtm7h4m9fTHX1/jGasJJBkrk7d9xxB08++STtE44kUnZA3NtGiifTPnk2\nCxYs4Oabb1ZCSLMVK1bwlS9/ierqauaU93zk0NGjO8jJNL547hd03kGatbS08IMf/CBIBJOj+GyH\neEcaL4POEzpZt3Ed53/lfBYvXpzUWIcCJYMk6koEd911Fx3lB9IxaXbCZXSMP4L28Yfz8MMPc/PN\nN+siKmn07W9/m/UbNnLZUQ2cM6vnprs5Yzv44Zw6ciKN3HTTTaxZsybFUQoEncXfvfy7vPi3F4ke\nEcWPc0j0yqTjoPMjnTRZE5dedikLFy5MSqxDhZJBkjQ3N3PttdfuSgTt007cNQZRrJzKV8ho3k5G\n83byVj5BTuUre65gRsfkObsSwpXf+56GPUih+vp6XnjhBa655hpaW1s5ZVIrR47u7HOb8SOifHFW\nMwDXX3cdjz76KFVVVakIV4CamhouvfRSli5dSnROFD+w5xqBvW5QB9RBxoKMYL67Qug8qZPIyAhX\nXHkFzz333D5bQ7fh8sJmz57tixYtSncYcamsrOSqq35A5YZK2iceS8eEI3tMBAB5K58gs2HzrvlI\n4ThaDzn9vSu6k7VlJbkbXmXcuHH88IbrmTlzZrJewn6vra2Nn/z4x8yfP5+oOwXZxkcnNvPp6a1k\nxfyFumHRSACumt34njJe3pzN/WtGsi2sRMyaOYMbfvgjxo3rqwdTBsrdeeaZZ7j5lptpbW8lcmwE\nr+j99y1jQQZWs/t76eVO9KReat5tkPlyJmyDD37wg1xyySWUlpYO9ktICjN7zd37bZZQMhhEjY2N\nzJs3jwcffJBoZg7N008iOmpin9vEnQxCGQ2bKVjzAtbRwhlnnMGXv/xliouLB+sl7Pc6Ozupqqri\n9ttv5//+7//4WEUr7xvTzgFFETK71aPvWpXPi1U5AEwpjDClMMK5B+7ZfOQO1c0ZvLE9m4fWjaB4\n9Fiuve56KioqyM/PT9XL2uc8XzrfAAASKUlEQVQtXbqU39z2G5YvWw6jIfK+CIzse5uEkgGAg71t\nZK7IJC83j3M+fw5nnXUWBQUFvW8zBCgZpFAkEuGJJ57g9tvvoL6+no7ymXRMmh1co6AfiSYDADpa\nydm0mOytb1JQUMCXv/QlPvWpT5GVpUtax8vdqa6uZvXq1bzzzjusX7+e9evWsmlTFZ2RCABnTW/h\nzOm9DzFxw6KRvFW3+7KXBxV39FhD6PJWbRY/WlxINPzKjR1TzpSp05g6dSozZsxg5syZTJkyRfsx\nAW+//Ta33XYbCxcuxPKNyMERfHp8HcUJJ4Mu9ZCxLAOrMoqKizjvi+dxxhlnkJOTsxevJHmUDFJk\n8eLF3PLLX7Ju7VqiReNoqzie6IjRcW8/oGQQsuZacjf8g8ydm5g4aTIXfuubvP/970/4NexPWlpa\n+P73v8eiRa/tWmYGYwtgQn47E0ZEmTAiwuSREaYWRnpr3QMSTwYANS0ZrKvPZFNTJlVNmVS1ZFHV\nlEFHZPc6hYUjufnmW9QM2IdNmzZxxx13MH/+fCzXiBwYwQ9wSCCPDjgZdNkOmcszYSuMGTuGC756\nASeffDIZGUOrKzbeZKC/IAO0efNmbr31Vv72t79BXiGtMz5CpHRar30DyeAFJbQeeCqZdRvZtPFV\nLr/8cubMmcNFF13E5MmTUxbHUNfS0kJtbS3r16/n1Vdf3ZUI/mNGM4eWdDJpZITcRI80GaDy/GAs\nI9h9zeRIFDa3ZFDZkMmvlo+koaGRn/zkJ5x99tnMmjWLsrIyRo4ciaXwszUUuTtvvvkmjz76KM89\n9xxRixI9KIofFFygJuXKIPKhCGyBmuU13HDDDdz9x7s569NncfLJJw/55qPuVDMYgDfffJNLL7uM\nxqYW2sYfQcf4wyFjYHk1b9lDjPQWTjvtNJ588kkaLZ/Wwz+VeEHRCFlbVpJXtYS8nGx+fNONHHXU\nUQOKabhpamrir3/9KzU1NdTW1rJjxw5qd+xgx45t7NhRR2vb7ktSZmZAeV6U949t48xpre/pB0jE\n9/9RyNZI4a59NyazgR8ev3dHer1UncOzG3PZ3JJNS8fu72Z2ViYlxcWUlJVRWlpGSUkJpaWllJSU\ncMIJJzBxYt99U8NZY2Mjzz77LI89/hjr1q7DsozIlEhwAtledLtkPJ9BQVvBrv3XnNtM9JQBHrrt\nYBuNzLcy8Z1Obl4u/3rKv/KJT3yCgw46aOBBDgLVDJJk4cKFfP/7V9Fm2TQfeiaeP2qvyrPOdk47\n4zQuvPBCAO5/7JmBFZSRSef4w2kqnYa//QzfueQSrrn6aj70oQ/tVXxD3Y4dOzjzzN2X0S7MMYpy\noozK7qQixzl8TJRROc6o3Chj8qNML+octFpAc6dx2um7993/PfGnvS7zxPHtnDi+najDxsZM3m3M\nZGe7sbM9g53tzezctpmq6kzeas9kZ5sTdbj11lu5+eabOeaYY/b6+Yea7du387Wvf42tW7ZipUb0\n2Cg+eZBqAh1w2mm7998DTz8w8LIMvMLpnNwJO6BlbQtPPP0Ejz/+OBdeeCFnnXXWIAScXEoGCair\nq+OqH/yA1qwRtMz6t7g6iPvjWTk8+eSTADz55JN41t4dYeK5I2k66HQKVj/HNddey7333MPYsWP3\nOs50c3caGxvZsWPHrtv27dvpXlu86pg6Jo5MzYl5BVm+x74bkzV4tewM232EUk86onD+C8V09ZRe\neeWVzJ07l9LSUkpLSykrK6O0tJRRo0YNuTbseLW1tXH5FZdTs72GyL9E+rha+gBls8f+623wuoQY\nUAZe5nQe1UnGwgxuvfVWxo8fzwc+8IFBeILkUTJIwN13301raysth398UBIBAJk5tDTs4MEHHwzm\nC/eupgEE4xod8BHsjQf43e9+xxVXXLH3ZaZIfX099913H42NjTQ3N9PQ0MDm6io2b95MS2vbe9bP\nyoDCHGhoh5HZUXIH8Qe5P/lZTktjy659l1+cuufONDiyrIMl23LIywLvaOXXv/71e9bLzspkzJhy\nxo2fQElJKfn5+RQUFHDKKacwa9aslMU7EE8++SRvr3qbyAlJSAQA2dBSt3v/9Xco6kDKjx4XxV4w\nbr7lZo4//ngyM1PUOTUASgZxamxs5OGHH6GjbAaeX5LucPrluSPpKD+IZ555hq9+9auUlZWlO6S4\n3HLLLTz//PN9rlOeF2HiyAiTRkQpz48wItsZkeUUZDmdUaOhPfjXvjf9AUNV1KE1Ak0dGZx1QCsf\nq2ijqdNo7jB2tGXwblMmmxozebcp+NHp6IywqWozm6o271HOn/70J1588cV0vIS4Pfb4Y5ADDML/\no7TJgGhZlC1rtvDaa68xZ07/Ixani5JBnBYsWEBHRzud5cPncL/O8plkb1nBX/7yFz7zmc+kO5y4\nXHzxxUyfPp2cnBzcnZ07d1JfX099fT11dXXU76xl586drNzZyOvbOvosKz/bGJENBZlRCjI7Kcx2\nZhZ3ckx5B+MKhu4YTw3txuvbslm+I4u69gyaOzNpimTQ1GE0dzh9HfORkZFBUeFIpkwuoqi4hOLi\nYoqKiigqKmLUqFHk5ubS0tIyLA5BPv6446lcXwnPQHRSFJ/lUEL8g82lUxtYpZH5Tibe5EyZOoXx\n48enO6o+pS0ZmNmpwC0Ew0fd4e5D+lrIXQOO5b/1NF5QSueIcqIjxxAZOQbPGzXgQ0qjI8rIaN4e\nTBeUER0xwH/w7lhbPRmNW8ls2EpWUw0WlvvOO+8MrMw0KCws5Jxzzolr3dbWVhoaGmhsbNzjvudl\n9VRvq+Gfq6u5ZzWMG+EcOKqdA4s7mVXcyei86B7DTMRjSmGEyobMXdO9te/3Jeqws91YszOLt+uy\nWLUzm3X1mUQdSopHMXHiJMYVFTFy5EgKCwt33cdOx96PGDFinzkE9etf/zqf/vSnefDBB3n4kYdp\n3diKZRk+yomOikIxeIlDEQP6JfNiD8YmgqCsgTTzOdAM1IHVGVZrZNZnEm0K/mwcfuThnPP5czj+\n+OOH/H5Jy6GlZpYJvA2cArwL/BP4nLuv7G2bdB9a2tbWxpIlS1i5ciXLly9nxYqVtLQEg5FZdh4d\nBaOJjhxDdGQ50fzSoE8hzp3fNThd+5Q4/625Yx0tZDTvIKNpGxmNW8hu3oa3B0Mh5OblccjBh3DY\nYYdy6KGHcvTRR2vog1B1dTUvv/wyCxcuZPmyN2hobNr1WFGuUZwbpTi7k5LcaDCd45TmBucGjMmP\nkNftR+euVcH72n0YCoDOKGxrzWBrSwY7WjOobcugrt2oa8ugti2Tuo4sdrY6kfArmJ2dxcEHHcwx\nxx7LCSecwKxZs4Zt5+9ga2xs5KWXXmL16tWsXr2aVW+voqU5fM8NrMiIjIrAKPBCh0KCPoB+3r6u\nwen8qH5+Bx1oAxrAGgzqgx//jJ0ZeHuwrZkxafIkDpx1IDNnzuToo49O+2GlYVxD9wxkM3s/cI27\n/1s4fyWAu9/Y2zbpTgbdRaNRNmzYwPLly1m5ciXLli1nw4bKXSMaWnYukbwSIvklRAtKiOaXEi0o\ngawED1nobCejpTb44W+pJaO5lqzWWrxj9zAJkyZP5vDDDuPQQ4Mf/6lTpw7pjqqhIhqNUllZyYoV\nK6ipqWHbtm1s376dbdtq2F5TQ23dTqLdvh+jco3yvE7G5HdSkuvvabFo7DC2tmSwtS2b7S28p0mn\nqHAkZWVljC4fw+jRoykrK6OsrIxZs2Zx4IEHDtkhDYYad2fz5s27hhN5++23WfX2KrZv2757JQMr\nNKIjokGCKIpJFDn03NwUAZoIfvTrDRogozEDa7BdP/oA2TnZHHDAAcyaOYuZM2cyc+ZMpk+fTl5e\nXnJf+AAM9WRwFnCqu38lnD8XOM7dv9ltvQuACwAqKiqOHerXJG1sbOSdd95h7dq1rF27ljVr17J2\nzdpdNQgAyx1JR14x0YISPL8E73aymkUjWEstGS21ZLfW4a27T2LKzctj+rTpzJhxANOmTWP69OnM\nmDGDoqKilL3G/UkkEqG2tpaamhqqqqqorq6mqqqKTZs2UbXpXXbU1r5nmxH5+UyYNImJEycxYcIE\nJkyYwPjx4xk7diylpaXk5g7G8YvSm8bGRjZu3MiGDRvYsGEDGzduZN36dWzatInOjt1Dj2fkZgQJ\nIZYTNO/E/CSWlJYwdepUplRMYfLkyUyZEtyPHTt22NTahnoyOBv4t27JYI67f6u3bYZazSBe7s7W\nrVtZu3Yt69atY+3atbyzZg0bKivp7Ox5XPzMzEwmV1Qw44ADmD59OtOnT2fatGmMGzduyLc7igxF\nkUiELVu2sHHjRiorK9m4cSNNTU17rJOZmcm4ceOoqKigoqKCyZMnD7shJXoy1M9AfheIHTxnErBP\nXv3DzBg7dixjx47d4wiOzs5ONm/eTEfHnkfEZGVlMX78eI1cKTKIMjMzd9XUjjvuuHSHMySl6xfn\nn8BMM5sGbAI+C3w+TbGkRVZWFpMmTUp3GCIiQJqSgbt3mtk3gWcJDi39rbuvSEcsIiKSxvMM3P0p\n4Kl0Pb+IiOw2PLrDRUQkqZQMREREyUBERJQMREQEJQMREWEYXQPZzGqAoT0exd4ZDWxLdxAyINp3\nw9u+vv+muHt5fysNm2SwrzOzRfGcMi5Dj/bd8Kb9F1AzkYiIKBmIiIiSwVByW7oDkAHTvhvetP9Q\nn4GIiKCagYiIoGQgIiIoGaSEmUXM7PWY2xUxj5WbWYeZfa3bNuvNbJmZLTWz58xsXOojFwAza+w2\nf56Z/U84fY2ZbQr363IzOyNm+aXpiHd/Z2ZuZnfFzGeZWY2ZPWGBbWZWEj42Plz/xJj1a8yszMwO\nNLMF4b5908z26b4FJYPUaHH3o2JuN8U8djbwD+BzPWz3YXc/ElgEfC8VgcqA/MLdjyLYl781M32v\n0qsJOMzM8sP5UwguooUHnaSvAl2XHTwBWBLeY2YHAtvcfTvwS8J96+4HA7em7iWknj606fc54BJg\nkplN7GWdF4EZqQtJBsLd3wQ6Cc5olfR6GjgtnP4ccG/MY38n/PEP73/Onsnh5XB6PMElegFw92XJ\nCnYoUDJIjfxuzUSfATCzycA4d18I3A98ppftTwf26Q/iELfH/gOu62klMzsOiAI1KY1OenIf8Fkz\nywOOIKgNdHmZ3clgDvAIu6/JfgJBsgD4BfBXM3vazL5tZsXJDzt9dNX11GgJmxG6+yxBEoDgw3sn\nwb+ULi+YWQR4A7gquSFKH/bYf2Z2HhA7fMG3zewLQAPwGXd3M0txiBLL3d8ws6kEtYLuV1RcCBxt\nZiOAbHdvNLO1ZjaDIBn8d1jG78zsWeBU4JPA18zsSHdvS9XrSCUlg/T6HDDWzM4J5yeY2Ux3Xx3O\nf9jd9+UBtPYVv3D3n6U7CHmPx4CfAScBZV0L3b3ZzN4BvgwsDhf/A/g4MAZYFbNuFfBbgr6g5cBh\nwGupCD7V1EyUJmFH1Qh3n+juU919KnAjQW1BRPbeb4Hremnr/ztwMfBKOP8KcBHwj7CTGTM71cyy\nw+lxBAllU9KjThMlg9To3mdwE0Gt4OFu6/2Zno8qkuHpKjN7t+uW7mD2N+7+rrvf0svDfwemszsZ\nLAYmsbvzGOBfgeVmthR4FrjM3TcnK95003AUIiKimoGIiCgZiIgISgYiIoKSgYiIoGQgIiIoGYgk\nzMzONLNDYuYXmNl+f0F1Gd6UDEQSdyZwSL9rxcHMNAqADAlKBiKAmT1iZq+Z2QozuyBc1hjz+Flm\n9nszOwE4A/hpeALhAeEqZ5vZQjN728w+GG6TZ2a/C69LscTMPhwuP8/MHjCzx4HnUvtKRXqmfyUi\ngS+7+45wDPx/mtmfe1rJ3V82s8eAJ9z9QYBwULosd59jZh8HrgZOBv4z3OZwMzsIeM7MZoVFvR84\nwt13JPdlicRHyUAkcKGZ/Xs4PRmYmeD2D4X3rwFTw+kTCS+I4u5vmVkl0JUMnlcikKFEyUD2e2Z2\nEsE/+feHI1ouAPKA2LFa8voppmtY4wi7v1d9jWPdlHikIsmjPgMRGAXUhongIOD4cPkWMzs4vIzl\nv8es3wAUxlHui8A5AGHzUAUxwyOLDCVKBiLwDJBlZm8A1xOMbQ9wBfAE8FegOmb9+4DLwk7hA+jd\nr4BMM1sG/Ak4b1+9MIoMfxq1VEREVDMQERElAxERQclARERQMhAREZQMREQEJQMREUHJQEREgP8f\n74El6nFdtRoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f649b478ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEWCAYAAACT7WsrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wuw4eB5NIKJugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f649b1df048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEWCAYAAACT7WsrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f649b1afb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check stop words and punctuation per author:\n",
    "# Add them as features:\n",
    "\n",
    "# Punctuations:\n",
    "train_raw['num_punctuations'] = train_raw['text'].apply(lambda x: len([char for char in str(x) if char in string.punctuation]))\n",
    "test_raw['num_punctuations'] = test_raw['text'].apply(lambda x: len([char for char in str(x) if char in string.punctuation]))\n",
    "\n",
    "# Stopwords:\n",
    "train_raw['num_stopwords'] = train_raw['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords.words(\"english\")]))\n",
    "test_raw['num_stopwords'] = test_raw['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords.words(\"english\")]))\n",
    "\n",
    "# Num of words:\n",
    "train_raw['num_words'] = train_raw['text'].apply(lambda x: len([word for word in str(x).lower().split()]))\n",
    "test_raw['num_words'] = test_raw['text'].apply(lambda x: len([word for word in str(x).lower().split()]))\n",
    "\n",
    "#print(train_raw.loc[0:4]['text'])\n",
    "#print(train_raw['num_punctuations'][0:4])\n",
    "display(train_raw.head())\n",
    "display(test_raw.head())\n",
    "for idx in range(4):\n",
    "    print(train_raw.loc[idx]['text'])\n",
    "    print(\"num_punctuations ---> \" + str(train_raw['num_punctuations'][idx]))\n",
    "    print(\"num_stopwords ---> \" + str(train_raw['num_stopwords'][idx]))\n",
    "    print(\"num_words ---> \" + str(train_raw['num_words'][idx]))\n",
    "\n",
    "## Plot\n",
    "plt.figure()\n",
    "plt.title(\"Distribution of num of punctuations per author\");\n",
    "sns.violinplot(x='author',y='num_punctuations',data = train_raw)\n",
    "plt.figure()\n",
    "plt.title(\"Distribution of num of stopwords per author\");\n",
    "sns.violinplot(x='author',y='num_stopwords',data = train_raw)\n",
    "plt.figure()\n",
    "plt.title(\"Distribution of num of words per author\");\n",
    "sns.violinplot(x='author',y='num_words',data = train_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the output as numerical data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'MWS']\n",
      "[0 1 2]\n",
      "['EAP' 'HPL' 'MWS']\n",
      "['EAP']\n"
     ]
    }
   ],
   "source": [
    "# Encode the output labels as 3 classes (from string to int)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_raw['author'])\n",
    "y_train_raw = label_encoder.fit_transform(train_raw['author'])\n",
    "print(label_encoder.classes_)\n",
    "print(label_encoder.transform(['EAP','HPL','MWS']))\n",
    "\n",
    "# Store inverse transform for submissions csv\n",
    "print(label_encoder.inverse_transform([0,1,2]))\n",
    "label_inv_transform = label_encoder.inverse_transform([0,1,2])\n",
    "print(label_encoder.inverse_transform([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Submission file function\n",
    "\n",
    "# Given prob, print it to file\n",
    "def create_submission_final(model_name, predictions_prob):\n",
    "\n",
    "    subm_file = model_name+'_submission.csv'\n",
    "    print(\"Creating submission file: \" + str(subm_file))\n",
    "    \n",
    "    # Get probabilities\n",
    "    test_id_values = test_raw['id'].values\n",
    "    #print(predictions[::100])\n",
    "    #print(predictions_prob[::100])\n",
    "\n",
    "    # Print to file\n",
    "    op_file = pd.DataFrame()\n",
    "    op_file['id'] = test_id_values\n",
    "    op_file[label_inv_transform[0]] = predictions_prob[:,0] ; # EAP\n",
    "    op_file[label_inv_transform[1]] = predictions_prob[:,1] ; # HPL\n",
    "    op_file[label_inv_transform[2]] = predictions_prob[:,2] ; # MWS\n",
    "    op_file.to_csv(subm_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file: baseline_1_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Baseline accuracy:\n",
    "baseline_prob = np.tile(np.array([1,0,0]), (8392,1))\n",
    "create_submission_final(\"baseline_1\", baseline_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold training function: It will be used throughout this notebook\n",
    "#### Also, includes function to add predictions as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# K-fold CV training function:\n",
    "#############################\n",
    "# Inputs: x_train (includes y labels), y_train (only y labels), x_test\n",
    "# Preprocess train_raw, test_raw to give x_train, y_train, x_test\n",
    "def run_kfold_training(model_name, \n",
    "                       run_model_preprocess, model_preprocess_params,\n",
    "                       run_model, model_params,\n",
    "                       algorithm_type,\n",
    "                       x_train_raw, y_train_raw, x_test_raw,\n",
    "                       kfold_idx_not_on_df):\n",
    "\n",
    "    # Print some messages\n",
    "    print(\"Running kfold training with model\", model_name)\n",
    "    print(\"Shapes: x_train_raw.shape {}, y_train_raw.shape {} , x_test_raw.shape {}\".format(\n",
    "        x_train_raw.shape, y_train_raw.shape, x_test_raw.shape))\n",
    "    \n",
    "    # Preprocess the data to input to the model\n",
    "    print(\"Running preprocess:\", run_model_preprocess)\n",
    "    (x_train, x_test, \n",
    "     extra_params) = run_model_preprocess(x_train_raw, x_test_raw, model_preprocess_params)\n",
    "    y_train = y_train_raw\n",
    "    print(\"Shapes: x_train.shape {}, y_train.shape {} , x_test.shape {}\".format(\n",
    "        x_train.shape, y_train.shape, x_test.shape))\n",
    "    \n",
    "    \n",
    "    # Add extra params to model_params dictionary to pass to running the model\n",
    "    if extra_params is not None:\n",
    "        for k,v in extra_params.items():\n",
    "            model_params[k] = v\n",
    "    \n",
    "    # Type of split\n",
    "    num_splits = 5\n",
    "    kf = KFold(n_splits = num_splits, shuffle=True, random_state = 1)\n",
    "    \n",
    "    # Num of classes in the output\n",
    "    num_classes = 3\n",
    "    \n",
    "    # train and test predictions (could be added as stacking features later)\n",
    "    pred_test = 0\n",
    "    pred_train = np.zeros((x_train_raw.shape[0], num_classes))\n",
    "    \n",
    "    # val_scores\n",
    "    val_scores = []\n",
    "    \n",
    "   \n",
    "    # Kfold training and prediction\n",
    "    fold_num = 0\n",
    "    for train_folds_idx, val_fold_idx in kf.split(x_train_raw):\n",
    "\n",
    "        fold_num+=1\n",
    "        print(\"Running fold\", fold_num)\n",
    "        \n",
    "        # Get the train and validation folds (Dataframe idx needs .loc)\n",
    "        if kfold_idx_not_on_df == 1:\n",
    "            x_train_folds, x_val_fold = x_train[train_folds_idx], x_train[val_fold_idx]\n",
    "        else:\n",
    "            x_train_folds, x_val_fold = x_train.loc[train_folds_idx], x_train.loc[val_fold_idx]\n",
    "        y_train_folds, y_val_fold = y_train[train_folds_idx], y_train[val_fold_idx]\n",
    "        \n",
    "        # Fit \n",
    "        (pred_val_fold, pred_test_per_fold_training, \n",
    "         model) = run_model(x_train_folds, y_train_folds, \n",
    "                            x_val_fold, y_val_fold,\n",
    "                            x_test,\n",
    "                            model_params)\n",
    "        \n",
    "        # Compile predictions on validation fold (i.e. unseen data)\n",
    "        pred_train[val_fold_idx,:] = pred_val_fold\n",
    "        # Compile predictions on test set using current training folds model\n",
    "        pred_test += pred_test_per_fold_training\n",
    "        val_scores.append(log_loss(y_val_fold, pred_val_fold))\n",
    "\n",
    "    pred_test = pred_test * (1. / num_splits) \n",
    "    print(\"\\n\\n===== Model: {}  ========:\".format(model_name))\n",
    "    print(\" Cross-val log losses are:\", val_scores)\n",
    "    print(\"====== Mean cross-val log loss is: {} =========\\n\\n\".format(np.mean(val_scores)))\n",
    "\n",
    "    return pred_train, pred_test, model\n",
    "\n",
    "#########################################\n",
    "# Function to add predictions as features\n",
    "#########################################\n",
    "def add_pred_features(modelname, x_train, x_test, pred_train, pred_test):\n",
    "    \n",
    "    feature_eap = modelname + '_eap'\n",
    "    feature_hpl = modelname + '_hpl'\n",
    "    feature_mws = modelname + '_mws'\n",
    "    x_train[feature_eap] = pred_train[:,0]\n",
    "    x_train[feature_hpl] = pred_train[:,1]\n",
    "    x_train[feature_mws] = pred_train[:,2]\n",
    "    x_test[feature_eap] = pred_test[:,0]\n",
    "    x_test[feature_hpl] = pred_test[:,1]\n",
    "    x_test[feature_mws] = pred_test[:,2]\n",
    "    \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing and Supervised Machine learning algorithms:\n",
    "- CountVectorizer, TfidfVectorizer\n",
    "- Multinominal Naive Bayes (MNB), Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running kfold training with model mnb_tfidf\n",
      "Shapes: x_train_raw.shape (19579, 6), y_train_raw.shape (19579,) , x_test_raw.shape (8392, 5)\n",
      "Running preprocess: <function run_mnb_tfidf at 0x7f64f5860e18>\n",
      "Shapes: x_train.shape (19579, 249062), y_train.shape (19579,) , x_test.shape (8392, 249062)\n",
      "Running fold 1\n",
      "Running fold 2\n",
      "Running fold 3\n",
      "Running fold 4\n",
      "Running fold 5\n",
      "\n",
      "\n",
      "===== Model: mnb_tfidf  ========:\n",
      " Cross-val log losses are: [0.37908346999014125, 0.37106918023241087, 0.37236379382643092, 0.37617292752863873, 0.36991758602498825]\n",
      "====== Mean cross-val log loss is: 0.373721391520522 =========\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tfidf vectorizer with MNB:\n",
    "def gen_tfidf(x_train_text, x_test_text):\n",
    "    ngram = 2\n",
    "    #vect = TfidfVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(x_train)\n",
    "    vect = TfidfVectorizer(ngram_range = (1,ngram)).fit(x_train_text)\n",
    "    x_train_doc_term = vect.transform(x_train_text)\n",
    "    x_test_doc_term = vect.transform(x_test_text)\n",
    "    return x_train_doc_term, x_test_doc_term\n",
    "\n",
    "# Run mnb on tfidf\n",
    "def run_mnb_tfidf(x_train, x_test, params = None):\n",
    "    # Separate out text\n",
    "    x_train_text = x_train['text']\n",
    "    x_test_text = x_test['text']\n",
    "    # Get tfidf doc term matrix\n",
    "    x_train_doc_term, x_test_doc_term = gen_tfidf(x_train_text, x_test_text) \n",
    "    return x_train_doc_term, x_test_doc_term, None\n",
    "\n",
    "# Multinomial Naive bayes algorithm\n",
    "def run_mnb(x_train, y_train, x_val, y_val, x_test, params):\n",
    "    alpha = params['alpha']\n",
    "    model = MultinomialNB(alpha = alpha)\n",
    "    model.fit(x_train, y_train)\n",
    "    pred_val = model.predict_proba(x_val)\n",
    "    pred_test = model.predict_proba(x_test)\n",
    "    return pred_val, pred_test, model   \n",
    "\n",
    "# Run MNB tfidf and add predictions as features:\n",
    "model_name = \"mnb_tfidf\"\n",
    "model_preprocess_function = run_mnb_tfidf\n",
    "model_function = run_mnb\n",
    "model_params = {'alpha': 0.01 }\n",
    "model_preprocess_params = None\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"VECT\", \n",
    "                                           train_raw, y_train_raw, test_raw, 1)                                           \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n",
    "#display(train_raw.head(1))\n",
    "#display(test_raw.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running kfold training with model mnb_count\n",
      "Shapes: x_train_raw.shape (19579, 9), y_train_raw.shape (19579,) , x_test_raw.shape (8392, 8)\n",
      "Running preprocess: <function run_mnb_count at 0x7f64a2b12510>\n",
      "Shapes: x_train.shape (19579, 212575), y_train.shape (19579,) , x_test.shape (8392, 212575)\n",
      "Running fold 1\n",
      "Running fold 2\n",
      "Running fold 3\n",
      "Running fold 4\n",
      "Running fold 5\n",
      "\n",
      "\n",
      "===== Model: mnb_count  ========:\n",
      " Cross-val log losses are: [0.46875532098312317, 0.45116954365731066, 0.43660145956478208, 0.46107245572976219, 0.44702080768041885]\n",
      "====== Mean cross-val log loss is: 0.45292391752307937 =========\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words</th>\n",
       "      <th>mnb_tfidf_eap</th>\n",
       "      <th>mnb_tfidf_hpl</th>\n",
       "      <th>mnb_tfidf_mws</th>\n",
       "      <th>mnb_count_eap</th>\n",
       "      <th>mnb_count_hpl</th>\n",
       "      <th>mnb_count_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>0.942005</td>\n",
       "      <td>0.01129</td>\n",
       "      <td>0.046705</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "\n",
       "   num_punctuations  num_stopwords  num_words  mnb_tfidf_eap  mnb_tfidf_hpl  \\\n",
       "0                 7             19         41       0.942005        0.01129   \n",
       "\n",
       "   mnb_tfidf_mws  mnb_count_eap  mnb_count_hpl  mnb_count_mws  \n",
       "0       0.046705       0.999989       0.000007       0.000004  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words</th>\n",
       "      <th>mnb_tfidf_eap</th>\n",
       "      <th>mnb_tfidf_hpl</th>\n",
       "      <th>mnb_tfidf_mws</th>\n",
       "      <th>mnb_count_eap</th>\n",
       "      <th>mnb_count_hpl</th>\n",
       "      <th>mnb_count_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.07408</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.920963</td>\n",
       "      <td>0.038465</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.960543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "\n",
       "   num_punctuations  num_stopwords  num_words  mnb_tfidf_eap  mnb_tfidf_hpl  \\\n",
       "0                 3              9         19        0.07408       0.004957   \n",
       "\n",
       "   mnb_tfidf_mws  mnb_count_eap  mnb_count_hpl  mnb_count_mws  \n",
       "0       0.920963       0.038465       0.000992       0.960543  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count vectorizer with MNB:\n",
    "def gen_count(x_train_text, x_test_text):\n",
    "    ngram = 2\n",
    "    vect = CountVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(x_train_text)\n",
    "    x_train_doc_term = vect.transform(x_train_text)\n",
    "    x_test_doc_term = vect.transform(x_test_text)\n",
    "    return x_train_doc_term, x_test_doc_term\n",
    "\n",
    "# Run mnb on tfidf\n",
    "def run_mnb_count(x_train, x_test, params = None):\n",
    "    # Separate out text\n",
    "    x_train_text = x_train['text']\n",
    "    x_test_text = x_test['text']\n",
    "    # Get tfidf doc term matrix\n",
    "    x_train_doc_term, x_test_doc_term = gen_count(x_train_text, x_test_text) \n",
    "    return x_train_doc_term, x_test_doc_term, None\n",
    "\n",
    "# Multinomial Naive bayes algorithm\n",
    "def run_mnb(x_train, y_train, x_val, y_val, x_test, params):\n",
    "    alpha = params['alpha']\n",
    "    model = MultinomialNB(alpha = alpha)\n",
    "    model.fit(x_train, y_train)\n",
    "    pred_val = model.predict_proba(x_val)\n",
    "    pred_test = model.predict_proba(x_test)\n",
    "    return pred_val, pred_test, model \n",
    "\n",
    "# Run MNB count and add predictions as features:\n",
    "model_name = \"mnb_count\"\n",
    "model_preprocess_function = run_mnb_count\n",
    "model_function = run_mnb\n",
    "model_params = {'alpha': 1.0 }\n",
    "model_preprocess_params = None\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"VECT\", \n",
    "                                           train_raw, y_train_raw, test_raw, 1)                                           \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n",
    "display(train_raw.head(1))\n",
    "display(test_raw.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running kfold training with model mnb_tfidf_char\n",
      "Shapes: x_train_raw.shape (19579, 12), y_train_raw.shape (19579,) , x_test_raw.shape (8392, 11)\n",
      "Running preprocess: <function run_mnb_tfidf_char at 0x7f64a2b7f6a8>\n",
      "Shapes: x_train.shape (19579, 1151052), y_train.shape (19579,) , x_test.shape (8392, 1151052)\n",
      "Running fold 1\n",
      "Running fold 2\n",
      "Running fold 3\n",
      "Running fold 4\n",
      "Running fold 5\n",
      "\n",
      "\n",
      "===== Model: mnb_tfidf_char  ========:\n",
      " Cross-val log losses are: [0.38824197963183649, 0.3890223276436871, 0.39628563464291233, 0.3847979523816813, 0.37640460557993577]\n",
      "====== Mean cross-val log loss is: 0.3869504999760106 =========\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words</th>\n",
       "      <th>mnb_tfidf_eap</th>\n",
       "      <th>mnb_tfidf_hpl</th>\n",
       "      <th>mnb_tfidf_mws</th>\n",
       "      <th>mnb_count_eap</th>\n",
       "      <th>mnb_count_hpl</th>\n",
       "      <th>mnb_count_mws</th>\n",
       "      <th>mnb_tfidf_char_eap</th>\n",
       "      <th>mnb_tfidf_char_hpl</th>\n",
       "      <th>mnb_tfidf_char_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>0.942005</td>\n",
       "      <td>0.01129</td>\n",
       "      <td>0.046705</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.00049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "\n",
       "   num_punctuations  num_stopwords  num_words  mnb_tfidf_eap  mnb_tfidf_hpl  \\\n",
       "0                 7             19         41       0.942005        0.01129   \n",
       "\n",
       "   mnb_tfidf_mws  mnb_count_eap  mnb_count_hpl  mnb_count_mws  \\\n",
       "0       0.046705       0.999989       0.000007       0.000004   \n",
       "\n",
       "   mnb_tfidf_char_eap  mnb_tfidf_char_hpl  mnb_tfidf_char_mws  \n",
       "0            0.999451            0.000059             0.00049  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words</th>\n",
       "      <th>mnb_tfidf_eap</th>\n",
       "      <th>mnb_tfidf_hpl</th>\n",
       "      <th>mnb_tfidf_mws</th>\n",
       "      <th>mnb_count_eap</th>\n",
       "      <th>mnb_count_hpl</th>\n",
       "      <th>mnb_count_mws</th>\n",
       "      <th>mnb_tfidf_char_eap</th>\n",
       "      <th>mnb_tfidf_char_hpl</th>\n",
       "      <th>mnb_tfidf_char_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.07408</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.920963</td>\n",
       "      <td>0.038465</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.960543</td>\n",
       "      <td>0.00035</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.999614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "\n",
       "   num_punctuations  num_stopwords  num_words  mnb_tfidf_eap  mnb_tfidf_hpl  \\\n",
       "0                 3              9         19        0.07408       0.004957   \n",
       "\n",
       "   mnb_tfidf_mws  mnb_count_eap  mnb_count_hpl  mnb_count_mws  \\\n",
       "0       0.920963       0.038465       0.000992       0.960543   \n",
       "\n",
       "   mnb_tfidf_char_eap  mnb_tfidf_char_hpl  mnb_tfidf_char_mws  \n",
       "0             0.00035            0.000036            0.999614  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tfidf vectorizer with MNB at char level:\n",
    "def gen_tfidf_char(x_train_text, x_test_text):\n",
    "    ngram = 7\n",
    "    #vect = TfidfVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(x_train)\n",
    "    vect = TfidfVectorizer(ngram_range = (1,ngram), analyzer='char').fit(x_train_text)\n",
    "    x_train_doc_term = vect.transform(x_train_text)\n",
    "    x_test_doc_term = vect.transform(x_test_text)\n",
    "    return x_train_doc_term, x_test_doc_term\n",
    "\n",
    "# Run mnb on tfidf\n",
    "def run_mnb_tfidf_char(x_train, x_test, params = None):\n",
    "    # Separate out text\n",
    "    x_train_text = x_train['text']\n",
    "    x_test_text = x_test['text']\n",
    "    # Get tfidf doc term matrix\n",
    "    x_train_doc_term, x_test_doc_term = gen_tfidf_char(x_train_text, x_test_text) \n",
    "    return x_train_doc_term, x_test_doc_term, None\n",
    "\n",
    "# Multinomial Naive bayes algorithm\n",
    "def run_mnb(x_train, y_train, x_val, y_val, x_test, params):\n",
    "    alpha = params['alpha']\n",
    "    model = MultinomialNB(alpha = alpha)\n",
    "    model.fit(x_train, y_train)\n",
    "    pred_val = model.predict_proba(x_val)\n",
    "    pred_test = model.predict_proba(x_test)\n",
    "    return pred_val, pred_test, model  \n",
    "\n",
    "# Run MNB tfidf chars and add predictions as features:\n",
    "model_name = \"mnb_tfidf_char\"\n",
    "model_preprocess_function = run_mnb_tfidf_char\n",
    "model_function = run_mnb\n",
    "model_params = {'alpha': 0.03 }\n",
    "model_preprocess_params = None\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"VECT\", \n",
    "                                           train_raw, y_train_raw, test_raw, 1)                                           \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n",
    "\n",
    "\n",
    "display(train_raw.head(1))\n",
    "display(test_raw.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running kfold training with model lr_count\n",
      "Shapes: x_train_raw.shape (19579, 15), y_train_raw.shape (19579,) , x_test_raw.shape (8392, 14)\n",
      "Running preprocess: <function run_lr_count at 0x7f649b474158>\n",
      "Shapes: x_train.shape (19579, 249062), y_train.shape (19579,) , x_test.shape (8392, 249062)\n",
      "Running fold 1\n",
      "Running fold 2\n",
      "Running fold 3\n",
      "Running fold 4\n",
      "Running fold 5\n",
      "\n",
      "\n",
      "===== Model: lr_count  ========:\n",
      " Cross-val log losses are: [0.4636133436069198, 0.46855246411426921, 0.4412857462490028, 0.45436025990786655, 0.44846678129836448]\n",
      "====== Mean cross-val log loss is: 0.45525571903528456 =========\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count vectorizer with Logistic regression:\n",
    "def gen_count(x_train_text, x_test_text):\n",
    "    ngram = 2\n",
    "    vect = CountVectorizer(ngram_range = (1,ngram)).fit(x_train_text)\n",
    "    x_train_doc_term = vect.transform(x_train_text)\n",
    "    x_test_doc_term = vect.transform(x_test_text)\n",
    "    return x_train_doc_term, x_test_doc_term\n",
    "\n",
    "# Run mnb on tfidf\n",
    "def run_lr_count(x_train, x_test, params = None):\n",
    "    # Separate out text\n",
    "    x_train_text = x_train['text']\n",
    "    x_test_text = x_test['text']\n",
    "    # Get tfidf doc term matrix\n",
    "    x_train_doc_term, x_test_doc_term = gen_count(x_train_text, x_test_text) \n",
    "    return x_train_doc_term, x_test_doc_term, None\n",
    "\n",
    "# LR algorithm\n",
    "def run_lr(x_train, y_train, x_val, y_val, x_test, params):\n",
    "    C = params['C']\n",
    "    model = LogisticRegression(C = C)\n",
    "    model.fit(x_train, y_train)\n",
    "    pred_val = model.predict_proba(x_val)\n",
    "    pred_test = model.predict_proba(x_test)\n",
    "    return pred_val, pred_test, model  \n",
    "\n",
    "# Run LR count and add predictions as features:\n",
    "model_name = \"lr_count\"\n",
    "model_preprocess_function = run_lr_count\n",
    "model_function = run_lr\n",
    "model_params = {'C': 1.0 }\n",
    "model_preprocess_params = None\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"VECT\", \n",
    "                                           train_raw, y_train_raw, test_raw, 1)                                           \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n",
    "\n",
    "#display(train_raw.head(1))\n",
    "#display(test_raw.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost model definition and Run Stacking model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words</th>\n",
       "      <th>mnb_tfidf_eap</th>\n",
       "      <th>mnb_tfidf_hpl</th>\n",
       "      <th>mnb_tfidf_mws</th>\n",
       "      <th>mnb_count_eap</th>\n",
       "      <th>mnb_count_hpl</th>\n",
       "      <th>mnb_count_mws</th>\n",
       "      <th>mnb_tfidf_char_eap</th>\n",
       "      <th>mnb_tfidf_char_hpl</th>\n",
       "      <th>mnb_tfidf_char_mws</th>\n",
       "      <th>lr_count_eap</th>\n",
       "      <th>lr_count_hpl</th>\n",
       "      <th>lr_count_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>0.942005</td>\n",
       "      <td>0.01129</td>\n",
       "      <td>0.046705</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.996793</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "\n",
       "   num_punctuations  num_stopwords  num_words  mnb_tfidf_eap  mnb_tfidf_hpl  \\\n",
       "0                 7             19         41       0.942005        0.01129   \n",
       "\n",
       "   mnb_tfidf_mws  mnb_count_eap  mnb_count_hpl  mnb_count_mws  \\\n",
       "0       0.046705       0.999989       0.000007       0.000004   \n",
       "\n",
       "   mnb_tfidf_char_eap  mnb_tfidf_char_hpl  mnb_tfidf_char_mws  lr_count_eap  \\\n",
       "0            0.999451            0.000059             0.00049      0.996793   \n",
       "\n",
       "   lr_count_hpl  lr_count_mws  \n",
       "0      0.001718      0.001489  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words</th>\n",
       "      <th>mnb_tfidf_eap</th>\n",
       "      <th>mnb_tfidf_hpl</th>\n",
       "      <th>mnb_tfidf_mws</th>\n",
       "      <th>mnb_count_eap</th>\n",
       "      <th>mnb_count_hpl</th>\n",
       "      <th>mnb_count_mws</th>\n",
       "      <th>mnb_tfidf_char_eap</th>\n",
       "      <th>mnb_tfidf_char_hpl</th>\n",
       "      <th>mnb_tfidf_char_mws</th>\n",
       "      <th>lr_count_eap</th>\n",
       "      <th>lr_count_hpl</th>\n",
       "      <th>lr_count_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.07408</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.920963</td>\n",
       "      <td>0.038465</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.960543</td>\n",
       "      <td>0.00035</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.999614</td>\n",
       "      <td>0.173632</td>\n",
       "      <td>0.008389</td>\n",
       "      <td>0.817979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "\n",
       "   num_punctuations  num_stopwords  num_words  mnb_tfidf_eap  mnb_tfidf_hpl  \\\n",
       "0                 3              9         19        0.07408       0.004957   \n",
       "\n",
       "   mnb_tfidf_mws  mnb_count_eap  mnb_count_hpl  mnb_count_mws  \\\n",
       "0       0.920963       0.038465       0.000992       0.960543   \n",
       "\n",
       "   mnb_tfidf_char_eap  mnb_tfidf_char_hpl  mnb_tfidf_char_mws  lr_count_eap  \\\n",
       "0             0.00035            0.000036            0.999614      0.173632   \n",
       "\n",
       "   lr_count_hpl  lr_count_mws  \n",
       "0      0.008389      0.817979  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Display the columns in the training and test data at this point\n",
    "display(train_raw.head(1))\n",
    "display(test_raw.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running kfold training with model xgboost_stacking_1\n",
      "Shapes: x_train_raw.shape (19579, 18), y_train_raw.shape (19579,) , x_test_raw.shape (8392, 17)\n",
      "Running preprocess: <function run_xgb_preprocess at 0x7f64a2b12158>\n",
      "Shapes: x_train.shape (19579, 15), y_train.shape (19579,) , x_test.shape (8392, 15)\n",
      "Running fold 1\n",
      "[0]\ttrain-mlogloss:0.999664\tcross-valid-mlogloss:1.00074\n",
      "Multiple eval metrics have been passed: 'cross-valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until cross-valid-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.383147\tcross-valid-mlogloss:0.395538\n",
      "[40]\ttrain-mlogloss:0.308226\tcross-valid-mlogloss:0.325671\n",
      "[60]\ttrain-mlogloss:0.290174\tcross-valid-mlogloss:0.31099\n",
      "[80]\ttrain-mlogloss:0.281545\tcross-valid-mlogloss:0.306877\n",
      "[100]\ttrain-mlogloss:0.275509\tcross-valid-mlogloss:0.305253\n",
      "[120]\ttrain-mlogloss:0.269675\tcross-valid-mlogloss:0.304423\n",
      "[140]\ttrain-mlogloss:0.264935\tcross-valid-mlogloss:0.304101\n",
      "[160]\ttrain-mlogloss:0.260306\tcross-valid-mlogloss:0.303982\n",
      "[180]\ttrain-mlogloss:0.256018\tcross-valid-mlogloss:0.304124\n",
      "[200]\ttrain-mlogloss:0.251718\tcross-valid-mlogloss:0.303537\n",
      "[220]\ttrain-mlogloss:0.247239\tcross-valid-mlogloss:0.303689\n",
      "[240]\ttrain-mlogloss:0.243359\tcross-valid-mlogloss:0.303824\n",
      "Stopping. Best iteration:\n",
      "[203]\ttrain-mlogloss:0.251067\tcross-valid-mlogloss:0.303406\n",
      "\n",
      "Running fold 2\n",
      "[0]\ttrain-mlogloss:0.999933\tcross-valid-mlogloss:1.00036\n",
      "Multiple eval metrics have been passed: 'cross-valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until cross-valid-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.383909\tcross-valid-mlogloss:0.392427\n",
      "[40]\ttrain-mlogloss:0.308679\tcross-valid-mlogloss:0.323086\n",
      "[60]\ttrain-mlogloss:0.290174\tcross-valid-mlogloss:0.309287\n",
      "[80]\ttrain-mlogloss:0.281121\tcross-valid-mlogloss:0.305519\n",
      "[100]\ttrain-mlogloss:0.275056\tcross-valid-mlogloss:0.304575\n",
      "[120]\ttrain-mlogloss:0.269389\tcross-valid-mlogloss:0.304092\n",
      "[140]\ttrain-mlogloss:0.264492\tcross-valid-mlogloss:0.304471\n",
      "[160]\ttrain-mlogloss:0.259924\tcross-valid-mlogloss:0.304657\n",
      "Stopping. Best iteration:\n",
      "[111]\ttrain-mlogloss:0.27183\tcross-valid-mlogloss:0.303924\n",
      "\n",
      "Running fold 3\n",
      "[0]\ttrain-mlogloss:1.00009\tcross-valid-mlogloss:1.00079\n",
      "Multiple eval metrics have been passed: 'cross-valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until cross-valid-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.384732\tcross-valid-mlogloss:0.390998\n",
      "[40]\ttrain-mlogloss:0.309888\tcross-valid-mlogloss:0.317788\n",
      "[60]\ttrain-mlogloss:0.291875\tcross-valid-mlogloss:0.30292\n",
      "[80]\ttrain-mlogloss:0.282877\tcross-valid-mlogloss:0.298574\n",
      "[100]\ttrain-mlogloss:0.276846\tcross-valid-mlogloss:0.297413\n",
      "[120]\ttrain-mlogloss:0.271184\tcross-valid-mlogloss:0.297027\n",
      "[140]\ttrain-mlogloss:0.266093\tcross-valid-mlogloss:0.296948\n",
      "[160]\ttrain-mlogloss:0.261045\tcross-valid-mlogloss:0.297053\n",
      "[180]\ttrain-mlogloss:0.25676\tcross-valid-mlogloss:0.296568\n",
      "[200]\ttrain-mlogloss:0.252349\tcross-valid-mlogloss:0.29664\n",
      "[220]\ttrain-mlogloss:0.248352\tcross-valid-mlogloss:0.296514\n",
      "Stopping. Best iteration:\n",
      "[184]\ttrain-mlogloss:0.256062\tcross-valid-mlogloss:0.296448\n",
      "\n",
      "Running fold 4\n",
      "[0]\ttrain-mlogloss:1.00058\tcross-valid-mlogloss:1.00069\n",
      "Multiple eval metrics have been passed: 'cross-valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until cross-valid-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.384244\tcross-valid-mlogloss:0.390903\n",
      "[40]\ttrain-mlogloss:0.308641\tcross-valid-mlogloss:0.32172\n",
      "[60]\ttrain-mlogloss:0.290232\tcross-valid-mlogloss:0.308447\n",
      "[80]\ttrain-mlogloss:0.281686\tcross-valid-mlogloss:0.305206\n",
      "[100]\ttrain-mlogloss:0.275316\tcross-valid-mlogloss:0.303804\n",
      "[120]\ttrain-mlogloss:0.26974\tcross-valid-mlogloss:0.303346\n",
      "[140]\ttrain-mlogloss:0.264704\tcross-valid-mlogloss:0.303374\n",
      "[160]\ttrain-mlogloss:0.260258\tcross-valid-mlogloss:0.303342\n",
      "[180]\ttrain-mlogloss:0.25579\tcross-valid-mlogloss:0.303336\n",
      "[200]\ttrain-mlogloss:0.251712\tcross-valid-mlogloss:0.30332\n",
      "Stopping. Best iteration:\n",
      "[164]\ttrain-mlogloss:0.259251\tcross-valid-mlogloss:0.303142\n",
      "\n",
      "Running fold 5\n",
      "[0]\ttrain-mlogloss:0.99835\tcross-valid-mlogloss:0.998813\n",
      "Multiple eval metrics have been passed: 'cross-valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until cross-valid-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.382279\tcross-valid-mlogloss:0.386316\n",
      "[40]\ttrain-mlogloss:0.308401\tcross-valid-mlogloss:0.316768\n",
      "[60]\ttrain-mlogloss:0.291033\tcross-valid-mlogloss:0.304198\n",
      "[80]\ttrain-mlogloss:0.282189\tcross-valid-mlogloss:0.30023\n",
      "[100]\ttrain-mlogloss:0.275981\tcross-valid-mlogloss:0.299054\n",
      "[120]\ttrain-mlogloss:0.270792\tcross-valid-mlogloss:0.298777\n",
      "[140]\ttrain-mlogloss:0.265665\tcross-valid-mlogloss:0.298761\n",
      "[160]\ttrain-mlogloss:0.260892\tcross-valid-mlogloss:0.299008\n",
      "Stopping. Best iteration:\n",
      "[123]\ttrain-mlogloss:0.270081\tcross-valid-mlogloss:0.298649\n",
      "\n",
      "\n",
      "\n",
      "===== Model: xgboost_stacking_1  ========:\n",
      " Cross-val log losses are: [0.30340605295119594, 0.30392434009540742, 0.29644798686117763, 0.30314216150289319, 0.29864875083094305]\n",
      "====== Mean cross-val log loss is: 0.30111385844832345 =========\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAysAAALJCAYAAAC0rLxhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNRQhekcq0noAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f649b135710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Run XGboost on features so far ###\n",
    "\n",
    "# Drop the necessary columns\n",
    "def run_xgb_preprocess(x_train, x_test, params):\n",
    "    \n",
    "    features_drop = params['features_drop']\n",
    "    \n",
    "    # train data\n",
    "    drop_columns_train = ['id','text','author']\n",
    "    if features_drop is not None:\n",
    "        drop_columns_train = drop_columns_train + features_drop\n",
    "    x_train_features = x_train.drop(drop_columns_train, axis=1)\n",
    "        \n",
    "    # test data\n",
    "    drop_columns_test = ['id','text']\n",
    "    if features_drop is not None:\n",
    "        drop_columns_test = drop_columns_test + features_drop\n",
    "    x_test_features = x_test.drop(drop_columns_test, axis = 1)    \n",
    "    \n",
    "    return x_train_features, x_test_features, None\n",
    "\n",
    "#####################\n",
    "## XGboost:\n",
    "#####################\n",
    "def run_xgb(train_X, train_y, \n",
    "                      val_X, val_y, \n",
    "                      test_X, \n",
    "                      model_params):\n",
    "    \n",
    "    # Extra params\n",
    "    seed_val = model_params['seed_val']\n",
    "    child = model_params['child']\n",
    "    colsample = model_params['colsample']\n",
    "    \n",
    "    # Params list\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 3\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    early_stopping_rounds = 50\n",
    "    num_rounds = 2000\n",
    "    param_list = list(param.items())\n",
    "   \n",
    "    # Train \n",
    "    d_train = xgb.DMatrix(train_X, label= train_y)\n",
    "    d_val = xgb.DMatrix(val_X, label= val_y)\n",
    "    watchlist = [ (d_train,'train'), (d_val, 'cross-valid') ]\n",
    "    model = xgb.train(param_list, d_train, num_rounds, \n",
    "                      watchlist, early_stopping_rounds= early_stopping_rounds, \n",
    "                      verbose_eval=20)\n",
    "    \n",
    "    # Get validation predictions \n",
    "    #pred_train = model.predict(d_train, \n",
    "    #                            ntree_limit = model.best_ntree_limit)\n",
    "    pred_val = model.predict(d_val, \n",
    "                                ntree_limit = model.best_ntree_limit)\n",
    "    \n",
    "    # Get test set predictions.\n",
    "    d_test = xgb.DMatrix(test_X)\n",
    "    pred_test = model.predict(d_test, \n",
    "                                 ntree_limit = model.best_ntree_limit)\n",
    "\n",
    "    return pred_val, pred_test, model\n",
    "\n",
    "# Run XGboost and add predictions as features:\n",
    "model_name = \"xgboost_stacking_1\"\n",
    "model_preprocess_function = run_xgb_preprocess\n",
    "model_function = run_xgb\n",
    "model_params = {'seed_val': 0, 'child': 1, 'colsample': 0.3 }\n",
    "#features_drop = ['num_punctuations','num_stopwords','num_words']\n",
    "features_drop = None\n",
    "model_preprocess_params = {'features_drop': features_drop }\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"VECT\", \n",
    "                                           train_raw, y_train_raw, test_raw, 0) \n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "xgb.plot_importance(model, height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell used for initial training to search for good models and hyperparameters: - Disabled now\n",
    "\n",
    " - Split the data into training and test set\n",
    " - Tokenize the input and try Count vectorizer, Tf-idf vectorizer\n",
    " - Try MNB, LR and GridSearchCV for hyperparameters.\n",
    " - Try Stemming + Lemmatize, with / without stopwords, various n-grams for Tf-idf etc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Disable this cell:\n",
    "if False:\n",
    "\n",
    "    ########################################################################################\n",
    "    # Create inputs for training and Split into train and test sets \n",
    "    ########################################################################################\n",
    "    X_train = train_raw['text']\n",
    "    #y_tr = train_raw['author']\n",
    "\n",
    "    # Split into train and test data (0.05 -> 0.2 -> .02)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tr,\n",
    "                                                        y_tr,\n",
    "                                                        test_size=0.1,\n",
    "                                                        stratify=y_tr,\n",
    "                                                        random_state = 2)\n",
    "\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    print(X_train.head())\n",
    "    print(y_train[0:20])\n",
    "    display(train_raw.loc[[17794,13109,16951,11267,11267]])\n",
    "\n",
    "    ## Exclude stopwords\n",
    "\n",
    "    ngram = 1\n",
    "\n",
    "    # Count vectorizer\n",
    "    vect = CountVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)\n",
    "    #print(len(vect.get_feature_names()))\n",
    "    #print(vect.get_feature_names()[::100])\n",
    "    # Transform reg vectorizer\n",
    "    X_train_doc_term = vect.transform(X_train)\n",
    "    print(\"Count doc_term_matrix shape with {}-gram features is {}\\n\".format(ngram, X_train_doc_term.shape))\n",
    "\n",
    "    # Tf idf vectorizer\n",
    "    tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)\n",
    "    #print(len(tfidf_vect.get_feature_names()))\n",
    "    print(tfidf_vect.get_feature_names()[::200])\n",
    "    print(tfidf_vect.get_feature_names()[:30])\n",
    "    # Transform tfidf vectorizer\n",
    "    X_train_doc_term_tfidf = tfidf_vect.transform(X_train)\n",
    "    print(\"Tf-idf count doc_term_matrix shape with {}-gram features is {}\\n\".format(ngram, X_train_doc_term_tfidf.shape))\n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "\n",
    "    ## Include stop words\n",
    "\n",
    "    ngram = 1\n",
    "\n",
    "    # Count vectorizer\n",
    "    vect = CountVectorizer(ngram_range = (1,ngram)).fit(X_train)\n",
    "    #print(len(vect.get_feature_names()))\n",
    "    #print(vect.get_feature_names()[::500])\n",
    "    # Transform reg vectorizer\n",
    "    X_train_doc_term = vect.transform(X_train)\n",
    "    print(\"Count doc_term_matrix shape with {}-gram features is {}\\n\".format(ngram, X_train_doc_term.shape))\n",
    "\n",
    "    # Tf idf vectorizer\n",
    "    tfidf_vect = TfidfVectorizer(ngram_range = (1,ngram)).fit(X_train)\n",
    "    #print(len(tfidf_vect.get_feature_names()))\n",
    "    print(tfidf_vect.get_feature_names()[::200])\n",
    "    print(tfidf_vect.get_feature_names()[:30])\n",
    "    # Transform tfidf vectorizer\n",
    "    X_train_doc_term_tfidf = tfidf_vect.transform(X_train)\n",
    "    print(\"Tf-idf count doc_term_matrix shape with {}-gram features is {}\\n\".format(ngram, X_train_doc_term_tfidf.shape))\n",
    "\n",
    "\n",
    "    ########################################################################################\n",
    "    Add Stemming and Lemmatization\n",
    "    ########################################################################################\n",
    "\n",
    "    # Stem and Lemmatize the tokens since sklearn doesnt support it.\n",
    "    # Idea borrowed from https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.stem import PorterStemmer\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    class LCountVectorizer(CountVectorizer):\n",
    "        def build_analyzer(self):\n",
    "            analyzer= super(LCountVectorizer, self).build_analyzer()\n",
    "            return lambda document: (lemmatizer.lemmatize(stemmer.stem(token)) for token in analyzer(document))\n",
    "\n",
    "    class LTfidfVectorizer(TfidfVectorizer):\n",
    "        def build_analyzer(self):\n",
    "            analyzer= super(LTfidfVectorizer, self).build_analyzer()\n",
    "            return lambda document: (lemmatizer.lemmatize(stemmer.stem(token)) for token in analyzer(document))\n",
    "\n",
    "    ngram = 1\n",
    "\n",
    "    if False:\n",
    "        # Count vectorizer\n",
    "        vect = LCountVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)\n",
    "        #print(len(vect.get_feature_names()))\n",
    "        #print(vect.get_feature_names()[::100])\n",
    "        # Transform reg vectorizer\n",
    "        X_train_doc_term = vect.transform(X_train)\n",
    "        print(\"Count doc_term_matrix shape with {}-gram features is {}\\n\".format(ngram, X_train_doc_term.shape))\n",
    "\n",
    "        # Tf idf vectorizer\n",
    "        tfidf_vect = LTfidfVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)\n",
    "        #print(len(tfidf_vect.get_feature_names()))\n",
    "        print(tfidf_vect.get_feature_names()[::200])\n",
    "        print(tfidf_vect.get_feature_names()[:30])\n",
    "        # Transform tfidf vectorizer\n",
    "        X_train_doc_term_tfidf = tfidf_vect.transform(X_train)\n",
    "        print(\"Tf-idf count doc_term_matrix shape with {}-gram features is {}\\n\".format(ngram, X_train_doc_term_tfidf.shape))\n",
    "\n",
    "    ###############################################################################################\n",
    "    # Grid search Count vect / Tfidf vect \n",
    "    # with/without stop words\n",
    "    # with/without stemlemmatize\n",
    "    # Logistic regression and Multinomial NB\n",
    "    # For various ngrams\n",
    "    ###############################################################################################\n",
    "\n",
    "    #ngrams = [1,2,3,4]\n",
    "    ngrams = [7]\n",
    "\n",
    "    count_based = 1\n",
    "    tf_idf_based = 1\n",
    "    LR = 0\n",
    "    MNB = 1\n",
    "    char_based = 1\n",
    "\n",
    "    for ngram in ngrams:\n",
    "\n",
    "        print(\"\\n\\n\\n\\n ======= ngram used is {} ========= \\n\\n\\n\\n\".format(ngram))\n",
    "\n",
    "        if count_based:\n",
    "            ##---------------------------------------\n",
    "            ## -- Count vectorizer without stopwords --\n",
    "            ##---------------------------------------\n",
    "            # Stem+Lemmatize version\n",
    "            #vect = LCountVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)\n",
    "            # No Stem+Lemmatize i.e. default version\n",
    "            if char_based:\n",
    "                vect = CountVectorizer(stop_words='english', analyzer = 'char', ngram_range = (1,ngram)).fit(X_train)\n",
    "            else:\n",
    "                vect = CountVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)\n",
    "\n",
    "            ##---------------------------------------\n",
    "            ## -- Count vectorizer with stopwords --\n",
    "            ##---------------------------------------\n",
    "            # Stem+Lemmatize version\n",
    "            #vect = LCountVectorizer(ngram_range = (1,ngram)).fit(X_train)\n",
    "            # No Stem+Lemmatize i.e. default version\n",
    "            #vect = CountVectorizer(ngram_range = (1,ngram)).fit(X_train)\n",
    "            ##---------------------------------------\n",
    "            print(len(vect.get_feature_names()))\n",
    "            print(vect.get_feature_names()[::2000])\n",
    "            # Transform reg vectorizer\n",
    "            X_train_doc_term = vect.transform(X_train)\n",
    "            print(\"Count doc_term_matrix shape with {}-gram features is {}\\n\".format(ngram, X_train_doc_term.shape))\n",
    "\n",
    "        if tf_idf_based:\n",
    "            ##---------------------------------------\n",
    "            ## Tf idf vectorizer without stopwords --\n",
    "            ##---------------------------------------\n",
    "            # Stem+Lemmatize version\n",
    "            #tfidf_vect = LTfidfVectorizer(stop_words='english', ngram_range =(1,ngram)).fit(X_train)\n",
    "            # No Stem+Lemmatize i.e. default version\n",
    "            if char_based:\n",
    "                tfidf_vect = TfidfVectorizer(stop_words='english', analyzer = 'char', ngram_range = (1,ngram)).fit(X_train)\n",
    "            else:\n",
    "                tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range = (1,ngram)).fit(X_train)\n",
    "            #---------------------------------------\n",
    "            ##---------------------------------------\n",
    "            ## Tf idf vectorizer with stopwords --\n",
    "            ##--------------------------------------\n",
    "            # Stem+Lemmatize version\n",
    "            #tfidf_vect = LTfidfVectorizer(ngram_range =(1,ngram)).fit(X_train)\n",
    "            # No Stem+Lemmatize i.e. default version\n",
    "            #tfidf_vect = TfidfVectorizer(ngram_range = (1,ngram)).fit(X_train)\n",
    "            ##---------------------------------------\n",
    "            print(len(tfidf_vect.get_feature_names()))\n",
    "            print(tfidf_vect.get_feature_names()[::2000])\n",
    "            # Transform tfidf vectorizer\n",
    "            X_train_doc_term_tfidf = tfidf_vect.transform(X_train)\n",
    "            print(\"Tf-idf count doc_term_matrix shape with {}-gram features is {}\\n\".format(ngram, X_train_doc_term_tfidf.shape))\n",
    "\n",
    "\n",
    "        if LR:\n",
    "            ##########################\n",
    "            ## Logistic regression\n",
    "            ##########################\n",
    "            if count_based:\n",
    "                # Count vectorizer\n",
    "                param_grid = {'C': [0.1, 1.0, 10.0, 100.0]}\n",
    "                model = LogisticRegression()\n",
    "                print(model)\n",
    "                skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "                grid_obj = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'neg_log_loss', cv = skfold, verbose = 1)\n",
    "                grid_obj_fit = grid_obj.fit(X_train_doc_term,y_train)\n",
    "                best_model = grid_obj_fit.best_estimator_\n",
    "                cv_results = grid_obj_fit.cv_results_\n",
    "                for mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):\n",
    "                    print(params, mean_score)\n",
    "                predictions_prob = best_model.predict_proba(tfidf_vect.transform(X_test))\n",
    "                logloss = log_loss(y_test,predictions_prob)\n",
    "                print(\"best params is \" + str(grid_obj_fit.best_params_))\n",
    "                print(\"\\n=== logloss on trainset for logistic regression & count vectorizer: {:.5f}=== \\n\".format(grid_obj_fit.best_score_))\n",
    "                print(\"\\n=== logloss on testset for logistic regression & count vectorizer: {:.5f}=== \\n\".format(logloss))\n",
    "\n",
    "            if tf_idf_based:\n",
    "                # Tf-idf vectorizer\n",
    "                param_grid = {'C': [0.1, 1.0, 10.0, 100.0]}\n",
    "                model = LogisticRegression()\n",
    "                skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "                grid_obj = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'neg_log_loss', cv = skfold, verbose = 1)\n",
    "                grid_obj_fit = grid_obj.fit(X_train_doc_term_tfidf,y_train)\n",
    "                best_model = grid_obj_fit.best_estimator_\n",
    "                cv_results = grid_obj_fit.cv_results_\n",
    "                for mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):\n",
    "                    print(params, mean_score)\n",
    "                predictions_prob = best_model.predict_proba(tfidf_vect.transform(X_test))\n",
    "                logloss = log_loss(y_test,predictions_prob)\n",
    "                print(\"best params is \" + str(grid_obj_fit.best_params_))\n",
    "                print(\"\\n=== logloss on trainset for logistic regression & tf-idf vectorizer: {:.5f}=== \\n\".format(grid_obj_fit.best_score_))\n",
    "                print(\"\\n=== logloss on testset for logistic regression & tf-idf vectorizer: {:.5f}=== \\n\".format(logloss))\n",
    "\n",
    "\n",
    "        if MNB:\n",
    "            ####################################################\n",
    "            ## Multinomial Naive Bayes \n",
    "            ####################################################\n",
    "\n",
    "            if count_based:\n",
    "                # Count vectorizer\n",
    "                param_grid = {'alpha': [0.01, 0.03, 0.1, 0.3, 0.6, 1.0]}\n",
    "                #scoring = make_scorer(log_loss, labels = np.array([0,1,2]))\n",
    "                model = MultinomialNB()\n",
    "                skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "                #grid_obj = GridSearchCV(estimator = model, param_grid = param_grid, scoring = scoring)\n",
    "                grid_obj = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'neg_log_loss', cv = skfold, verbose = 1)\n",
    "                grid_obj_fit = grid_obj.fit(X_train_doc_term,y_train)\n",
    "                best_model = grid_obj_fit.best_estimator_\n",
    "                cv_results = grid_obj_fit.cv_results_\n",
    "                for mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):\n",
    "                    print(params, mean_score)\n",
    "                #print(pd.DataFrame(grid_obj_fit.cv_results_))\n",
    "                predictions_prob = best_model.predict_proba(vect.transform(X_test))\n",
    "                logloss = log_loss(y_test,predictions_prob)\n",
    "                print(\"best params is \" + str(grid_obj_fit.best_params_))\n",
    "                print(\"\\n=== logloss on trainset for MNB & count vectorizer: {:.5f}=== \\n\".format(grid_obj_fit.best_score_))\n",
    "                print(\"\\n=== logloss on testset for MNB & count vectorizer: {:.5f}=== \\n\".format(logloss))\n",
    "\n",
    "            if tf_idf_based:\n",
    "                # Tf-idf vectorizer\n",
    "                param_grid = {'alpha': [0.01, 0.03, 0.1, 0.3, 0.6, 1.0]}\n",
    "                model = MultinomialNB()\n",
    "                skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "                grid_obj = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'neg_log_loss', cv = skfold, verbose = 1)\n",
    "                grid_obj_fit = grid_obj.fit(X_train_doc_term_tfidf,y_train)\n",
    "                best_model = grid_obj_fit.best_estimator_\n",
    "                cv_results = grid_obj_fit.cv_results_\n",
    "                for mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):\n",
    "                    print(params, mean_score)\n",
    "                predictions_prob = best_model.predict_proba(tfidf_vect.transform(X_test))\n",
    "                logloss = log_loss(y_test,predictions_prob)\n",
    "                print(\"best params is \" + str(grid_obj_fit.best_params_))\n",
    "                print(\"\\n=== logloss on trainset for MNB & tf idf vectorizer: {:.5f}=== \\n\".format(grid_obj_fit.best_score_))\n",
    "                print(\"\\n=== logloss on testset for MNB & tf idf vectorizer: {:.5f}=== \\n\".format(logloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings + Neural networks / Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3)\n",
      "0 [ 1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to one hot to train NN\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "print(y_train_one_hot.shape)\n",
    "print(y_train[0], y_train_one_hot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cells are used for pre-processing text for NN networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# Function to create word2vec from \n",
    "# given texts using gensim\n",
    "###################################\n",
    "\n",
    "def create_gensim_wordvec(x_train, wordvecsize):\n",
    "    \n",
    "    from gensim.models import Word2Vec\n",
    "    from gensim.utils import simple_preprocess\n",
    "    \n",
    "    ngrams_max = 2\n",
    "    sentences = []\n",
    "    for sent in x_train:\n",
    "        simple = 0\n",
    "        if simple:\n",
    "            # only tokenize\n",
    "            #print(sent)\n",
    "            sent_temp = simple_preprocess(sent)\n",
    "            sentences.append(sent_temp)\n",
    "            #print(sent_temp)\n",
    "            #break\n",
    "        else:\n",
    "            # separate punctuation\n",
    "            sent = sent.replace(\"' \", \" ' \")\n",
    "            punct = set('!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "            prods = set(sent) & punct\n",
    "            #print(\"sent is {}\".format(sent))\n",
    "            ##print(\"punct is {}\".format(punct))\n",
    "            #print(\"prods is {}\".format(prods))\n",
    "            if not prods:\n",
    "                continue\n",
    "            for sign in prods:\n",
    "                sent = sent.replace(sign, ' {} '.format(sign))\n",
    "            #sent = sent.split()\n",
    "            \n",
    "            # Add n-grams\n",
    "            sent_words = sent.split()\n",
    "            ngrams = []\n",
    "            for nidx in range(2, ngrams_max+1):\n",
    "                for widx in range(len(sent_words) - nidx + 1):                    \n",
    "                    ngrams.append('--'.join(sent_words[widx:widx+nidx]))\n",
    "                    #break\n",
    "            ngrams_string = ' '.join(ngrams)\n",
    "            sent = sent + ' ' + ngrams_string\n",
    "            sent = sent.split()\n",
    "            sentences.append(sent)\n",
    "            #print(sent)\n",
    "            #break\n",
    "            \n",
    "    print(sentences[0:2])\n",
    "    wordvecsize = wordvecsize\n",
    "    gensim_model = Word2Vec(sentences, \n",
    "                            window=5, iter=25, size=wordvecsize, min_count=2)\n",
    "\n",
    "    # Get some details\n",
    "    print(gensim_model)\n",
    "    # Get the most and least common words\n",
    "    vocab_size_gensim = len(gensim_model.wv.vocab)\n",
    "    print(vocab_size_gensim)\n",
    "    print(gensim_model.wv.index2word[0], \n",
    "          gensim_model.wv.index2word[1], \n",
    "          gensim_model.wv.index2word[2])\n",
    "    print(gensim_model.wv.index2word[vocab_size_gensim - 1], \n",
    "          gensim_model.wv.index2word[vocab_size_gensim - 2], \n",
    "          gensim_model.wv.index2word[vocab_size_gensim - 3])\n",
    "    print('Index of \"of\" is: {}'.format(gensim_model.wv.vocab['of'].index))\n",
    "    print(gensim_model.wv['the'])\n",
    "\n",
    "    # Write out vectors file\n",
    "    #weights = gensim_model.syn0 \n",
    "    gensim_embeddings = '../scratch/gensim.'+ str(wordvecsize) +'.txt'\n",
    "    word_vectors = gensim_model.wv\n",
    "    word_vectors.save(gensim_embeddings)\n",
    "    print(len(gensim_model.wv.vocab.keys()))\n",
    "    f = open(gensim_embeddings, 'w')\n",
    "    for w in gensim_model.wv.vocab.keys():\n",
    "        v = gensim_model.wv[w]\n",
    "        #print(\"{} {}\".format(w,v))\n",
    "        f.write(\"{} \".format(w))\n",
    "        for vi in v:\n",
    "            #print(\"{} \".format(vi))\n",
    "            f.write(\"{} \".format(vi))\n",
    "        #break\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "# Disabled here\n",
    "if False:\n",
    "    ###################################\n",
    "    # Create gensim wordvectors from given text \n",
    "    ###################################\n",
    "    create_gensim_wordvectors = 1\n",
    "    if create_gensim_wordvectors:\n",
    "        wordvecsize = 20\n",
    "        create_gensim_wordvec(X_train, wordvecsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Function to Preprocess to separate out punctuation and n-grams \n",
    "# since Tokenizer isnt doing good.\n",
    "#######################################################\n",
    "def preprocess(x_train):\n",
    "    \n",
    "    idx = 0\n",
    "    separate_punctuation = True\n",
    "    # Fast text: ngrams_max = 2\n",
    "    ngrams_max = 2\n",
    "    for sent in x_train:\n",
    "        #print(type(sent))\n",
    "\n",
    "        if separate_punctuation:\n",
    "            # Separate punctuation\n",
    "            sent = sent.replace(\"' \", \" ' \")\n",
    "            punct = set('!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "            prods = set(sent) & punct\n",
    "            #print(\"sent is {}\".format(sent))\n",
    "            ##print(\"punct is {}\".format(punct))\n",
    "            #print(\"prods is {}\".format(prods))\n",
    "            if not prods:\n",
    "                continue\n",
    "            for sign in prods:\n",
    "                sent = sent.replace(sign, ' {} '.format(sign))\n",
    "            #print(\"new sent is {}\".format(sent))\n",
    "            x_train.iloc[idx] = sent\n",
    "\n",
    "        if False:\n",
    "            # Remove punctuation before ngrams\n",
    "            sent = sent.replace(\"' \", \" ' \")\n",
    "            punct = set('!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "            prods = set(sent) & punct\n",
    "            #print(\"sent is {}\".format(sent))\n",
    "            ##print(\"punct is {}\".format(punct))\n",
    "            #print(\"prods is {}\".format(prods))\n",
    "            if not prods:\n",
    "                continue\n",
    "            for sign in prods:\n",
    "                sent = sent.replace(sign,''.format(sign))\n",
    "            #print(\"new sent is {}\".format(sent))\n",
    "            x_train.iloc[idx] = sent\n",
    "            \n",
    "        if ngrams_max > 1:\n",
    "            # Add n-grams\n",
    "            sent_words = sent.split()\n",
    "            ngrams = []\n",
    "            for nidx in range(2, ngrams_max+1):\n",
    "                for widx in range(len(sent_words) - nidx + 1):                    \n",
    "                    ngrams.append('--'.join(sent_words[widx:widx+nidx]))\n",
    "                    #break\n",
    "            ngrams_string = ' '.join(ngrams)\n",
    "            sent = sent + ' ' + ngrams_string\n",
    "            #print(\"new sent is {}\".format(sent))\n",
    "            x_train.iloc[idx] = sent\n",
    "\n",
    "        idx += 1\n",
    "        #break\n",
    "        \n",
    "    return x_train\n",
    "\n",
    "# Disabled code\n",
    "if False:\n",
    "    ###################################\n",
    "    # Preprocess X_train and X_test data:\n",
    "    # Separate punctuation, add ngrams\n",
    "    ###################################\n",
    "    x_train = preprocess(X_train)\n",
    "    print(x_train.head())\n",
    "    print(x_train.iloc[0])\n",
    "    x_test = preprocess(X_test)\n",
    "    print(x_test.head())\n",
    "    print(x_test.iloc[0])\n",
    "    #print(X_train_orig.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convert text to integers that can be input to Embedding layer\n",
    "\n",
    "def create_tokenizer(X_train, char_level):\n",
    "    \n",
    "    # This is for word vectors to be learnt by embedding layer\n",
    "    if char_level:\n",
    "        tokenizer = Tokenizer(filters='', char_level = True)\n",
    "        #tokenizer = Tokenizer()\n",
    "    else:\n",
    "        tokenizer = Tokenizer(filters='')\n",
    "\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    sentences_encoded = tokenizer.texts_to_sequences(X_train) \n",
    "    print(\"No of unique words found is \" + str(vocab_size))\n",
    "\n",
    "    min_num_occ = 2\n",
    "    num_words_atleastocc = sum([1 for w,c in tokenizer.word_counts.items() if c >= min_num_occ])\n",
    "    print(\"No of words occuring more than %d times is %d\" % (min_num_occ, num_words_atleastocc))\n",
    "    # hardcode to 10000\n",
    "    #num_words_atleastocc = 15000\n",
    "    tokenizer = Tokenizer(num_words=num_words_atleastocc,filters='')\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    sentences_encoded = tokenizer.texts_to_sequences(X_train) \n",
    "    print(\"No of unique words found is after capping is \" + str(vocab_size))\n",
    "\n",
    "    # Cap the length of sentence, and pad with 0 if sentences less than that length\n",
    "    sent_lengths = [len(s) for s in sentences_encoded]\n",
    "    sent_lengths_arr = np.array(sent_lengths)\n",
    "    print(\"max sentence length is {}\".format(len(max(sentences_encoded, key = len))))\n",
    "    print(\"mean, median, 99th perc length of sentence is %f %f %f\" % (\n",
    "        np.mean(sent_lengths_arr),np.median(sent_lengths_arr),\n",
    "        np.percentile(sent_lengths_arr,99)))\n",
    "    #plt.figure()\n",
    "    #sns.violinplot(y=np.array(sent_lengths))\n",
    "    # Fasttext: sentence_maxlength_cap = 256\n",
    "    sentence_maxlength_cap = int(np.percentile(sent_lengths_arr,99))\n",
    "    #sentence_maxlength_cap = 140\n",
    "    sentences_encoded = pad_sequences(sentences_encoded, maxlen=sentence_maxlength_cap)\n",
    "    print(\"After padding: max sentence size is {}\".format(len(max(sentences_encoded, key = len))))\n",
    "    sent_lengths = [len(s) for s in sentences_encoded]\n",
    "    print(\"After padding: mean, median length of sentence is %f %f\" % (np.mean(np.array(sent_lengths)),np.median(np.array(sent_lengths))))\n",
    "\n",
    "    # dimension of word vector output from Embedding layer\n",
    "    # Fasttext: wordvecdim = 20\n",
    "    wordvecdim = 20\n",
    "    \n",
    "    return  wordvecdim, sentence_maxlength_cap, sentences_encoded, tokenizer, vocab_size\n",
    "\n",
    "# Disabled code\n",
    "if False:\n",
    "    # Create tokenizer and encoded sentences X_train:\n",
    "    #wordvecdim, sentence_maxlength_cap, sentences_encoded, tokenizer, vocab_size = create_tokenizer(x_train, 0)\n",
    "    (wordvecdim, sentence_maxlength_cap, \n",
    "     sentences_encoded, tokenizer, vocab_size) = create_tokenizer(x_train, 0)\n",
    "\n",
    "    ### debug ###\n",
    "    if True:\n",
    "        print(X_train.head())\n",
    "        #print(tokenizer.word_index)\n",
    "        firstfewwords = {k: tokenizer.word_index[k] for k in list(tokenizer.word_index)[0:5]}\n",
    "        print(\"first few words are\", firstfewwords)\n",
    "        print(sentences_encoded[0])\n",
    "        print(tokenizer.document_count)\n",
    "        print(len(sentences_encoded))\n",
    "        print(y_train.shape)\n",
    "        print(len(tokenizer.word_counts.keys()))  \n",
    "        print(list(tokenizer.word_index)[:100])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create word embedding matrix for word vectors:\n",
    "def create_word_embedding_matrix(word_vector_type, vocab_size, tokenizer):\n",
    "    \n",
    "    if word_vector_type == \"None\":\n",
    "        print(\"\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Creating word embedding matrix for {} vectors\".format(word_vector_type))\n",
    "    \n",
    "    if word_vector_type == \"glove\": \n",
    "        # Load pre-trained Glove vectors. \n",
    "        # Convert them as word to word vector mapping\n",
    "        # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "        wordvecdim = 100\n",
    "        f = open('../scratch/glove.6B.100d.txt')\n",
    "\n",
    "    if word_vector_type == \"gensim\":\n",
    "        wordvecdim = 20\n",
    "        gensim_embeddings = '../scratch/gensim.'+ str(wordvecdim) +'.txt'\n",
    "        f = open(gensim_embeddings)\n",
    "\n",
    "\n",
    "    embeddings_index = {}\n",
    "    for line in tqdm(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "    # Create pretrained weight matrix to be used as Embedded layer input \n",
    "    # Each word (in input data) to its pretrained word vector mapping\n",
    "    embedding_matrix = np.zeros((vocab_size,wordvecdim))\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.zeros(wordvecdim)\n",
    "            #break\n",
    "    print(word,idx)\n",
    "    print(embedding_vector)\n",
    "    print(embedding_matrix.shape)\n",
    "    \n",
    "    return wordvecdim, embedding_matrix\n",
    "\n",
    "# Disabled code:\n",
    "if False:\n",
    "    gs_wordvecdim, gs_embedding_matrix = create_word_embedding_matrix(\"gensim\", vocab_size)\n",
    "    gl_wordvecdim, gl_embedding_matrix = create_word_embedding_matrix(\"glove\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell used for tuning hyper-parameters - Disabled now"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Disable the cell\n",
    "if False:\n",
    "    ## FastText model\n",
    "\n",
    "    def create_Fasttext(vocab_size, wordvecdim, sentence_maxlength_cap, embedding_matrix, word_vector_type):  \n",
    "\n",
    "        print(\"Creating Fasttext model vocab_size: {}, wordvecdim {}, sentence_maxlength_cap {}, word_vector_type {} \".format( \n",
    "              vocab_size, wordvecdim, sentence_maxlength_cap, word_vector_type))\n",
    "\n",
    "        # Train the Fasttext NN model (1 hidden layer)\n",
    "        model = Sequential()\n",
    "        if word_vector_type == \"glove\":\n",
    "            embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = False)\n",
    "            #embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = True)\n",
    "        elif word_vector_type == \"gensim\":\n",
    "            embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = True)\n",
    "        else:\n",
    "            embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap)\n",
    "        model.add(embedded_layer)\n",
    "        #model.add(Flatten())\n",
    "        #model.add(Dense(20,activation='relu'))\n",
    "        #model.add(Dropout(0.75))\n",
    "        #model.add(Flatten())\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(3,activation='softmax'))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    # CNN model\n",
    "    def create_CNN(vocab_size, wordvecdim, sentence_maxlength_cap, embedding_matrix, word_vector_type):\n",
    "\n",
    "        print(\"Creating CNN model vocab_size: {}, wordvecdim {}, sentence_maxlength_cap {}, word_vector_type {} \".format( \n",
    "              vocab_size, wordvecdim, sentence_maxlength_cap, word_vector_type))\n",
    "\n",
    "        # Train CNN model (1 hidden layer)\n",
    "        model = Sequential()\n",
    "        if word_vector_type == \"glove\":\n",
    "            embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = False)\n",
    "            #embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = True)\n",
    "        elif word_vector_type == \"gensim\":\n",
    "            embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = True)\n",
    "        else:\n",
    "            embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap)\n",
    "        model.add(embedded_layer)\n",
    "        #model.add(Flatten())\n",
    "        #model.add(Dense(20,activation='relu'))\n",
    "        model.add(Conv1D(filters = 32, kernel_size = 4, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Dropout(0.8))\n",
    "        #model.add(Conv1D(filters = 16, kernel_size = 4, activation='relu'))\n",
    "        #model.add(MaxPooling1D(pool_size=2))\n",
    "        #model.add(Dropout(0.8))\n",
    "        #model.add(Flatten())\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(3,activation='softmax'))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    # Function to train the models\n",
    "    def run_model(model, tokenizer, \n",
    "                  sentence_maxlength_cap, sentences_encoded, \n",
    "                  y_train_one_hot, X_train, X_test, train_raw, test_raw):\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Pre training loss\n",
    "        loss, score = model.evaluate(sentences_encoded, y_train_one_hot)\n",
    "        print(\"Before training loss, score are: {} {}\".format(loss,score))\n",
    "\n",
    "        # Fit the model to train data\n",
    "        #num_epochs = 200\n",
    "        num_epochs = 500\n",
    "        checkpointer = EarlyStopping(patience=3, monitor='val_loss')\n",
    "        model.fit(sentences_encoded, y_train_one_hot,\n",
    "                  epochs= num_epochs, \n",
    "                  batch_size=128, \n",
    "                  verbose=1, \n",
    "                  validation_split=0.05, shuffle = True,\n",
    "                  callbacks=[checkpointer])\n",
    "\n",
    "        # Training loss and predictions:\n",
    "        loss, score = model.evaluate(sentences_encoded, y_train_one_hot)\n",
    "        print(\"training loss, score are: {} {}\".format(loss,score))\n",
    "        pred_train = model.predict(sentences_encoded)\n",
    "\n",
    "        # Test loss and predictions:\n",
    "        sentences_encoded_test = tokenizer.texts_to_sequences(X_test) \n",
    "        sentences_encoded_test = pad_sequences(sentences_encoded_test, maxlen=sentence_maxlength_cap)\n",
    "        loss, score = model.evaluate(sentences_encoded_test, y_test_one_hot)\n",
    "        print(\"Test loss, score are: {} {}\".format(loss,score))\n",
    "        pred_test = model.predict(sentences_encoded_test)\n",
    "\n",
    "        # Final submission test data predictions:\n",
    "        X_final_test = test_raw['text'].copy()\n",
    "        X_final_test = preprocess(X_final_test)\n",
    "        sentences_encoded_ftest = tokenizer.texts_to_sequences(X_final_test) \n",
    "        sentences_encoded_ftest = pad_sequences(sentences_encoded_ftest, maxlen=sentence_maxlength_cap)\n",
    "        pred_ftest = model.predict(sentences_encoded_ftest)\n",
    "\n",
    "        return pred_train, pred_test, pred_ftest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing for NN - top level function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing (top level function) needed for Neural networks\n",
    "\n",
    "def run_NN_preprocess(X_train, X_test, params):    \n",
    "    \n",
    "    # Make a copy\n",
    "    X_train = train_raw['text'].copy()\n",
    "    X_test = test_raw['text'].copy()\n",
    "    \n",
    "    # Preprocess X_train and X_test data: Separate punctuation, add ngrams\n",
    "    x_train = preprocess(X_train)\n",
    "    #print(x_train.head())\n",
    "    #print(x_train.iloc[0])\n",
    "    x_test = preprocess(X_test)\n",
    "    #print(x_test.head())\n",
    "    #print(x_test.iloc[0])\n",
    "    \n",
    "    # Word vector type:\n",
    "    word_vector_type = params['word_vector_type']\n",
    "    \n",
    "    # Char level tokenize\n",
    "    char_level_tokenize = params['char_level']\n",
    "    \n",
    "    # Create tokenizer and encoded sentences:\n",
    "    (wordvecdim, sentence_maxlength_cap, \n",
    "     sentences_encoded_train, tokenizer, vocab_size) = create_tokenizer(x_train, 0)\n",
    "    # For debug \n",
    "    if False:\n",
    "        print(X_train.head())\n",
    "        #print(tokenizer.word_index)\n",
    "        firstfewwords = {k: tokenizer.word_index[k] for k in list(tokenizer.word_index)[0:5]}\n",
    "        print(\"first few words are\", firstfewwords)\n",
    "        print(sentences_encoded_train[0])\n",
    "        print(tokenizer.document_count)\n",
    "        print(len(sentences_encoded_train))\n",
    "        print(y_train.shape)\n",
    "        print(len(tokenizer.word_counts.keys()))  \n",
    "        print(list(tokenizer.word_index)[:100]) \n",
    "    \n",
    "    # Create encoded sentences for test set:\n",
    "    sentences_encoded_test = tokenizer.texts_to_sequences(x_test) \n",
    "    sentences_encoded_test = pad_sequences(sentences_encoded_test, \n",
    "                                            maxlen=sentence_maxlength_cap)    \n",
    "    \n",
    "    # Create word2vec using gensim:\n",
    "    if word_vector_type is not None:\n",
    "        wordvecdim, embedding_matrix = create_word_embedding_matrix(word_vector_type, \n",
    "                                                                      vocab_size,\n",
    "                                                                      tokenizer)\n",
    "    else:\n",
    "        embedding_matrix = None\n",
    "        \n",
    "    # Store the output params\n",
    "    out_params = {'vocab_size': vocab_size,\n",
    "                 'wordvecdim': wordvecdim,\n",
    "                 'sentence_maxlength_cap': sentence_maxlength_cap,\n",
    "                 'embedding_matrix': embedding_matrix,\n",
    "                 'word_vector_type': word_vector_type,\n",
    "                 'tokenizer': tokenizer}\n",
    "    \n",
    "    return sentences_encoded_train, sentences_encoded_test, out_params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText model definition and running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fast text model \n",
    "\n",
    "def run_Fasttext(sentences_encoded_train, y_train_one_hot,\n",
    "                 sentences_encoded_val, y_val_one_hot,\n",
    "                 sentences_encoded_test, \n",
    "                 model_params):  \n",
    "\n",
    "    # All model params\n",
    "    vocab_size = model_params['vocab_size'] \n",
    "    wordvecdim = model_params['wordvecdim']\n",
    "    sentence_maxlength_cap = model_params['sentence_maxlength_cap']\n",
    "    embedding_matrix = model_params['embedding_matrix']\n",
    "    word_vector_type = model_params['word_vector_type']\n",
    "    tokenizer =  model_params['tokenizer']    \n",
    "    print(\"Creating Fasttext model vocab_size: {}, wordvecdim {}, sentence_maxlength_cap {}, word_vector_type {} \".format( \n",
    "          vocab_size, wordvecdim, sentence_maxlength_cap, word_vector_type))\n",
    "    \n",
    "    # Train the Fasttext NN model (1 hidden layer)\n",
    "    model = Sequential()\n",
    "    if word_vector_type == \"glove\":\n",
    "        embedded_layer = Embedding(vocab_size, wordvecdim, \n",
    "                                   input_length = sentence_maxlength_cap, \n",
    "                                   weights = [embedding_matrix], trainable = False)\n",
    "        #embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = True)\n",
    "    elif word_vector_type == \"gensim\":\n",
    "        embedded_layer = Embedding(vocab_size, wordvecdim, \n",
    "                                   input_length = sentence_maxlength_cap, \n",
    "                                   weights = [embedding_matrix], trainable = True)\n",
    "    else:\n",
    "        embedded_layer = Embedding(vocab_size, wordvecdim, \n",
    "                                   input_length = sentence_maxlength_cap)\n",
    "    model.add(embedded_layer)\n",
    "    #model.add(Flatten())\n",
    "    #model.add(Dense(20,activation='relu'))\n",
    "    #model.add(Conv1D(filters = 8, kernel_size = 5,activation='relu'))\n",
    "    #model.add(MaxPooling1D(pool_size=10))\n",
    "    #model.add(Dropout(0.75))\n",
    "    #model.add(Flatten())\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Pre training loss\n",
    "    loss, score = model.evaluate(sentences_encoded_train, y_train_one_hot)\n",
    "    print(\"Before training loss, score are: {} {}\".format(loss,score))\n",
    "\n",
    "    # Fit the model to train data\n",
    "    #num_epochs = 200\n",
    "    num_epochs = 150\n",
    "    validation_data = (sentences_encoded_val, y_val_one_hot)\n",
    "    checkpointer = EarlyStopping(patience=3, monitor='val_loss')\n",
    "    model.fit(sentences_encoded_train, y_train_one_hot,\n",
    "              epochs= num_epochs, \n",
    "              batch_size=128, \n",
    "              verbose=1, \n",
    "              validation_data = validation_data,\n",
    "              #validation_split = 0.05, shuffle = True,              \n",
    "              callbacks=[checkpointer])\n",
    "\n",
    "    # Training loss and predictions:\n",
    "    loss, score = model.evaluate(sentences_encoded_train, y_train_one_hot)\n",
    "    print(\"==== Training loss, score are: {} {} =======\".format(loss,score))\n",
    "    pred_train = model.predict(sentences_encoded_train)\n",
    "\n",
    "    # Val loss and predictions:\n",
    "    loss, score = model.evaluate(sentences_encoded_val, y_val_one_hot)\n",
    "    print(\"==== CV loss, score are: {} {} =======\".format(loss,score))\n",
    "    pred_val = model.predict(sentences_encoded_val)\n",
    "    \n",
    "    # Test loss and predictions:\n",
    "    pred_test = model.predict(sentences_encoded_test)\n",
    "\n",
    "    return pred_val, pred_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 2018-Jan-14 02:51:50\n",
      "Running kfold training with model fast_text_none\n",
      "Shapes: x_train_raw.shape (19579, 18), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 17)\n",
      "Running preprocess: <function run_NN_preprocess at 0x7f620ee8eea0>\n",
      "No of unique words found is 247346\n",
      "No of words occuring more than 2 times is 75104\n",
      "No of unique words found is after capping is 247346\n",
      "max sentence length is 1439\n",
      "mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000\n",
      "After padding: max sentence size is 161\n",
      "After padding: mean, median length of sentence is 161.000000 161.000000\n",
      "Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)\n",
      "Running fold 1\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_4 ( (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 0s 26us/step\n",
      "Before training loss, score are: 1.0999214982100542 0.3116261252728731\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 1.0817 - acc: 0.4067 - val_loss: 1.0771 - val_acc: 0.3927\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 1.0559 - acc: 0.4184 - val_loss: 1.0477 - val_acc: 0.4162\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 1.0115 - acc: 0.4684 - val_loss: 0.9995 - val_acc: 0.5005\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.9472 - acc: 0.5881 - val_loss: 0.9409 - val_acc: 0.5534\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.8723 - acc: 0.6940 - val_loss: 0.8768 - val_acc: 0.6402\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.7958 - acc: 0.7617 - val_loss: 0.8149 - val_acc: 0.7086\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.7237 - acc: 0.8058 - val_loss: 0.7601 - val_acc: 0.7388\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.6581 - acc: 0.8340 - val_loss: 0.7109 - val_acc: 0.7684\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.6010 - acc: 0.8569 - val_loss: 0.6698 - val_acc: 0.7778\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.5500 - acc: 0.8710 - val_loss: 0.6376 - val_acc: 0.7735\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.5054 - acc: 0.8841 - val_loss: 0.6039 - val_acc: 0.7972\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.4658 - acc: 0.8959 - val_loss: 0.5771 - val_acc: 0.8077\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.4302 - acc: 0.9066 - val_loss: 0.5518 - val_acc: 0.8169\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3983 - acc: 0.9169 - val_loss: 0.5316 - val_acc: 0.8192\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3698 - acc: 0.9252 - val_loss: 0.5138 - val_acc: 0.8218\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3436 - acc: 0.9310 - val_loss: 0.4964 - val_acc: 0.8276\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3197 - acc: 0.9389 - val_loss: 0.4805 - val_acc: 0.8366\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2979 - acc: 0.9443 - val_loss: 0.4677 - val_acc: 0.8361\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2782 - acc: 0.9490 - val_loss: 0.4555 - val_acc: 0.8407\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2599 - acc: 0.9536 - val_loss: 0.4444 - val_acc: 0.8404\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.2432 - acc: 0.9588 - val_loss: 0.4352 - val_acc: 0.8412\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2275 - acc: 0.9621 - val_loss: 0.4262 - val_acc: 0.8435\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2132 - acc: 0.9653 - val_loss: 0.4189 - val_acc: 0.8481\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.1999 - acc: 0.9688 - val_loss: 0.4143 - val_acc: 0.8453\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1875 - acc: 0.9703 - val_loss: 0.4059 - val_acc: 0.8460\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1760 - acc: 0.9732 - val_loss: 0.3975 - val_acc: 0.8529\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1651 - acc: 0.9756 - val_loss: 0.3922 - val_acc: 0.8539\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1551 - acc: 0.9781 - val_loss: 0.3858 - val_acc: 0.8557\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1458 - acc: 0.9799 - val_loss: 0.3846 - val_acc: 0.8527\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1372 - acc: 0.9808 - val_loss: 0.3793 - val_acc: 0.8542\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1290 - acc: 0.9826 - val_loss: 0.3731 - val_acc: 0.8583\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1216 - acc: 0.9842 - val_loss: 0.3721 - val_acc: 0.8560\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1144 - acc: 0.9857 - val_loss: 0.3701 - val_acc: 0.8560\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1080 - acc: 0.9870 - val_loss: 0.3648 - val_acc: 0.8565\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1018 - acc: 0.9879 - val_loss: 0.3636 - val_acc: 0.8565\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0960 - acc: 0.9888 - val_loss: 0.3627 - val_acc: 0.8565\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0904 - acc: 0.9902 - val_loss: 0.3592 - val_acc: 0.8580\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0852 - acc: 0.9906 - val_loss: 0.3587 - val_acc: 0.8588\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0804 - acc: 0.9911 - val_loss: 0.3555 - val_acc: 0.8596\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0761 - acc: 0.9917 - val_loss: 0.3526 - val_acc: 0.8616\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0720 - acc: 0.9922 - val_loss: 0.3549 - val_acc: 0.8593\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0677 - acc: 0.9928 - val_loss: 0.3507 - val_acc: 0.8613\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0641 - acc: 0.9931 - val_loss: 0.3515 - val_acc: 0.8593\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0604 - acc: 0.9938 - val_loss: 0.3495 - val_acc: 0.8596\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0571 - acc: 0.9941 - val_loss: 0.3479 - val_acc: 0.8616\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0540 - acc: 0.9950 - val_loss: 0.3510 - val_acc: 0.8598\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0512 - acc: 0.9953 - val_loss: 0.3486 - val_acc: 0.8603\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0483 - acc: 0.9959 - val_loss: 0.3561 - val_acc: 0.8555\n",
      "15663/15663 [==============================] - 0s 20us/step\n",
      "==== Training loss, score are: 0.0470402358821782 0.9943178190640363 =======\n",
      "3916/3916 [==============================] - 0s 21us/step\n",
      "==== CV loss, score are: 0.3561393990427773 0.855464759959142 =======\n",
      "Running fold 2\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_5 ( (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 0s 25us/step\n",
      "Before training loss, score are: 1.0954445080153445 0.4033071570170228\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 53us/step - loss: 1.0814 - acc: 0.4059 - val_loss: 1.0767 - val_acc: 0.3953\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 1.0565 - acc: 0.4172 - val_loss: 1.0492 - val_acc: 0.4147\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 1.0120 - acc: 0.4737 - val_loss: 1.0031 - val_acc: 0.4737\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.9472 - acc: 0.5951 - val_loss: 0.9434 - val_acc: 0.5991\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.8717 - acc: 0.7018 - val_loss: 0.8806 - val_acc: 0.6900\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.7953 - acc: 0.7695 - val_loss: 0.8209 - val_acc: 0.7441\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.7242 - acc: 0.8080 - val_loss: 0.7677 - val_acc: 0.7482\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.6592 - acc: 0.8313 - val_loss: 0.7205 - val_acc: 0.7704\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.6024 - acc: 0.8507 - val_loss: 0.6800 - val_acc: 0.7806\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.5518 - acc: 0.8688 - val_loss: 0.6451 - val_acc: 0.7878\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5074 - acc: 0.8838 - val_loss: 0.6135 - val_acc: 0.8029\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4678 - acc: 0.8936 - val_loss: 0.5866 - val_acc: 0.8141\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4323 - acc: 0.9061 - val_loss: 0.5628 - val_acc: 0.8159\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.4000 - acc: 0.9150 - val_loss: 0.5423 - val_acc: 0.8207\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3712 - acc: 0.9244 - val_loss: 0.5224 - val_acc: 0.8304\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3452 - acc: 0.9325 - val_loss: 0.5053 - val_acc: 0.8307\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3213 - acc: 0.9384 - val_loss: 0.4899 - val_acc: 0.8368\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2993 - acc: 0.9457 - val_loss: 0.4759 - val_acc: 0.8427\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2794 - acc: 0.9513 - val_loss: 0.4639 - val_acc: 0.8419\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2610 - acc: 0.9551 - val_loss: 0.4529 - val_acc: 0.8453\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2440 - acc: 0.9593 - val_loss: 0.4418 - val_acc: 0.8501\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2280 - acc: 0.9641 - val_loss: 0.4330 - val_acc: 0.8504\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2136 - acc: 0.9669 - val_loss: 0.4235 - val_acc: 0.8516\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2003 - acc: 0.9701 - val_loss: 0.4156 - val_acc: 0.8524\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1878 - acc: 0.9725 - val_loss: 0.4091 - val_acc: 0.8547\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1762 - acc: 0.9747 - val_loss: 0.4021 - val_acc: 0.8544\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1655 - acc: 0.9771 - val_loss: 0.3961 - val_acc: 0.8562\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1554 - acc: 0.9780 - val_loss: 0.3907 - val_acc: 0.8573\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1460 - acc: 0.9797 - val_loss: 0.3858 - val_acc: 0.8578\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1374 - acc: 0.9821 - val_loss: 0.3812 - val_acc: 0.8588\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1291 - acc: 0.9827 - val_loss: 0.3780 - val_acc: 0.8603\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1215 - acc: 0.9845 - val_loss: 0.3744 - val_acc: 0.8606\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1144 - acc: 0.9856 - val_loss: 0.3698 - val_acc: 0.8611\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1076 - acc: 0.9868 - val_loss: 0.3675 - val_acc: 0.8613\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1014 - acc: 0.9874 - val_loss: 0.3662 - val_acc: 0.8603\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0957 - acc: 0.9882 - val_loss: 0.3629 - val_acc: 0.8629\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.0901 - acc: 0.9894 - val_loss: 0.3603 - val_acc: 0.8652\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0850 - acc: 0.9900 - val_loss: 0.3585 - val_acc: 0.8647\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0802 - acc: 0.9908 - val_loss: 0.3556 - val_acc: 0.8629\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0756 - acc: 0.9922 - val_loss: 0.3568 - val_acc: 0.8621\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0715 - acc: 0.9923 - val_loss: 0.3535 - val_acc: 0.8636\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0675 - acc: 0.9933 - val_loss: 0.3524 - val_acc: 0.8631\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0635 - acc: 0.9937 - val_loss: 0.3506 - val_acc: 0.8647\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0600 - acc: 0.9943 - val_loss: 0.3508 - val_acc: 0.8629\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0566 - acc: 0.9946 - val_loss: 0.3525 - val_acc: 0.8621\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0534 - acc: 0.9950 - val_loss: 0.3493 - val_acc: 0.8626\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0505 - acc: 0.9951 - val_loss: 0.3490 - val_acc: 0.8634\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0477 - acc: 0.9955 - val_loss: 0.3487 - val_acc: 0.8626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0451 - acc: 0.9959 - val_loss: 0.3493 - val_acc: 0.8616\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0427 - acc: 0.9962 - val_loss: 0.3494 - val_acc: 0.8639\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0403 - acc: 0.9967 - val_loss: 0.3494 - val_acc: 0.8636\n",
      "15663/15663 [==============================] - 0s 21us/step\n",
      "==== Training loss, score are: 0.038422147226279964 0.9968077635191215 =======\n",
      "3916/3916 [==============================] - 0s 21us/step\n",
      "==== CV loss, score are: 0.34937799246306317 0.8636363636363636 =======\n",
      "Running fold 3\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_6 ( (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 0s 27us/step\n",
      "Before training loss, score are: 1.1024929768721172 0.2903658303073682\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 57us/step - loss: 1.0847 - acc: 0.3955 - val_loss: 1.0753 - val_acc: 0.3996\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 1.0591 - acc: 0.4112 - val_loss: 1.0491 - val_acc: 0.4150\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 1.0167 - acc: 0.4551 - val_loss: 1.0046 - val_acc: 0.5015\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.9533 - acc: 0.5809 - val_loss: 0.9458 - val_acc: 0.6065\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.8774 - acc: 0.6940 - val_loss: 0.8824 - val_acc: 0.6627\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.7988 - acc: 0.7743 - val_loss: 0.8208 - val_acc: 0.7137\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.7250 - acc: 0.8109 - val_loss: 0.7650 - val_acc: 0.7549\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.6593 - acc: 0.8392 - val_loss: 0.7163 - val_acc: 0.7804\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6013 - acc: 0.8565 - val_loss: 0.6745 - val_acc: 0.7829\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5499 - acc: 0.8714 - val_loss: 0.6391 - val_acc: 0.8026\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5053 - acc: 0.8870 - val_loss: 0.6071 - val_acc: 0.8003\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4656 - acc: 0.8973 - val_loss: 0.5798 - val_acc: 0.8151\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4300 - acc: 0.9061 - val_loss: 0.5560 - val_acc: 0.8289\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3981 - acc: 0.9179 - val_loss: 0.5344 - val_acc: 0.8304\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3693 - acc: 0.9259 - val_loss: 0.5155 - val_acc: 0.8338\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3430 - acc: 0.9325 - val_loss: 0.4982 - val_acc: 0.8327\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3196 - acc: 0.9387 - val_loss: 0.4830 - val_acc: 0.8396\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2979 - acc: 0.9434 - val_loss: 0.4695 - val_acc: 0.8432\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2782 - acc: 0.9503 - val_loss: 0.4571 - val_acc: 0.8414\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2600 - acc: 0.9537 - val_loss: 0.4472 - val_acc: 0.8475\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2430 - acc: 0.9586 - val_loss: 0.4352 - val_acc: 0.8516\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2275 - acc: 0.9624 - val_loss: 0.4251 - val_acc: 0.8555\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2134 - acc: 0.9655 - val_loss: 0.4175 - val_acc: 0.8567\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1999 - acc: 0.9685 - val_loss: 0.4089 - val_acc: 0.8552\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1874 - acc: 0.9708 - val_loss: 0.4028 - val_acc: 0.8588\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1762 - acc: 0.9740 - val_loss: 0.3951 - val_acc: 0.8565\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1654 - acc: 0.9759 - val_loss: 0.3890 - val_acc: 0.8580\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1556 - acc: 0.9775 - val_loss: 0.3829 - val_acc: 0.8601\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.1463 - acc: 0.9794 - val_loss: 0.3785 - val_acc: 0.8585\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1376 - acc: 0.9814 - val_loss: 0.3730 - val_acc: 0.8639\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1295 - acc: 0.9832 - val_loss: 0.3692 - val_acc: 0.8618\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1218 - acc: 0.9845 - val_loss: 0.3646 - val_acc: 0.8641\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1149 - acc: 0.9857 - val_loss: 0.3614 - val_acc: 0.8657\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1079 - acc: 0.9867 - val_loss: 0.3574 - val_acc: 0.8680\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.1018 - acc: 0.9879 - val_loss: 0.3567 - val_acc: 0.8659\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0962 - acc: 0.9884 - val_loss: 0.3523 - val_acc: 0.8687\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0906 - acc: 0.9902 - val_loss: 0.3521 - val_acc: 0.8664\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0854 - acc: 0.9907 - val_loss: 0.3470 - val_acc: 0.8680\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0806 - acc: 0.9913 - val_loss: 0.3450 - val_acc: 0.8693\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0761 - acc: 0.9920 - val_loss: 0.3431 - val_acc: 0.8687\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0718 - acc: 0.9921 - val_loss: 0.3421 - val_acc: 0.8698\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0679 - acc: 0.9932 - val_loss: 0.3412 - val_acc: 0.8710\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.0640 - acc: 0.9936 - val_loss: 0.3400 - val_acc: 0.8716\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.0605 - acc: 0.9941 - val_loss: 0.3393 - val_acc: 0.8695\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0571 - acc: 0.9946 - val_loss: 0.3371 - val_acc: 0.8713\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0539 - acc: 0.9949 - val_loss: 0.3366 - val_acc: 0.8710\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.0511 - acc: 0.9952 - val_loss: 0.3352 - val_acc: 0.8700\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0483 - acc: 0.9956 - val_loss: 0.3342 - val_acc: 0.8718\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0456 - acc: 0.9961 - val_loss: 0.3338 - val_acc: 0.8718\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0433 - acc: 0.9963 - val_loss: 0.3348 - val_acc: 0.8713\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0407 - acc: 0.9963 - val_loss: 0.3348 - val_acc: 0.8705\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0387 - acc: 0.9967 - val_loss: 0.3349 - val_acc: 0.8703\n",
      "15663/15663 [==============================] - 0s 20us/step\n",
      "==== Training loss, score are: 0.036956799540566215 0.9964885398710337 =======\n",
      "3916/3916 [==============================] - 0s 20us/step\n",
      "==== CV loss, score are: 0.3349311209309454 0.8702757916849895 =======\n",
      "Running fold 4\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 0s 28us/step\n",
      "Before training loss, score are: 1.096575716221796 0.4000510758065267\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 1.0830 - acc: 0.4022 - val_loss: 1.0706 - val_acc: 0.4134\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 1.0583 - acc: 0.4156 - val_loss: 1.0432 - val_acc: 0.4331\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 1.0152 - acc: 0.4713 - val_loss: 0.9988 - val_acc: 0.5130\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.9534 - acc: 0.5846 - val_loss: 0.9427 - val_acc: 0.6193\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.8811 - acc: 0.6966 - val_loss: 0.8821 - val_acc: 0.6828\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.8069 - acc: 0.7678 - val_loss: 0.8230 - val_acc: 0.7153\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7364 - acc: 0.7994 - val_loss: 0.7701 - val_acc: 0.7594\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.6716 - acc: 0.8319 - val_loss: 0.7226 - val_acc: 0.7707\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.6137 - acc: 0.8497 - val_loss: 0.6816 - val_acc: 0.7840\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5626 - acc: 0.8674 - val_loss: 0.6465 - val_acc: 0.8001\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.5176 - acc: 0.8838 - val_loss: 0.6145 - val_acc: 0.8018\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.4772 - acc: 0.8937 - val_loss: 0.5857 - val_acc: 0.8069\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.4410 - acc: 0.9048 - val_loss: 0.5618 - val_acc: 0.8141\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.4083 - acc: 0.9136 - val_loss: 0.5402 - val_acc: 0.8174\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3790 - acc: 0.9224 - val_loss: 0.5222 - val_acc: 0.8269\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.3525 - acc: 0.9321 - val_loss: 0.5044 - val_acc: 0.8241\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3283 - acc: 0.9373 - val_loss: 0.4896 - val_acc: 0.8330\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3062 - acc: 0.9434 - val_loss: 0.4756 - val_acc: 0.8373\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2858 - acc: 0.9499 - val_loss: 0.4623 - val_acc: 0.8409\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.2671 - acc: 0.9530 - val_loss: 0.4516 - val_acc: 0.8453\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2499 - acc: 0.9579 - val_loss: 0.4401 - val_acc: 0.8473\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2339 - acc: 0.9621 - val_loss: 0.4309 - val_acc: 0.8509\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2193 - acc: 0.9650 - val_loss: 0.4216 - val_acc: 0.8488\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2055 - acc: 0.9690 - val_loss: 0.4136 - val_acc: 0.8532\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1929 - acc: 0.9716 - val_loss: 0.4061 - val_acc: 0.8542\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1812 - acc: 0.9738 - val_loss: 0.4002 - val_acc: 0.8567\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1704 - acc: 0.9749 - val_loss: 0.3934 - val_acc: 0.8562\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1602 - acc: 0.9771 - val_loss: 0.3880 - val_acc: 0.8547\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1506 - acc: 0.9787 - val_loss: 0.3831 - val_acc: 0.8573\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1418 - acc: 0.9806 - val_loss: 0.3776 - val_acc: 0.8611\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1336 - acc: 0.9824 - val_loss: 0.3737 - val_acc: 0.8618\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1258 - acc: 0.9836 - val_loss: 0.3695 - val_acc: 0.8624\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1186 - acc: 0.9850 - val_loss: 0.3670 - val_acc: 0.8588\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1117 - acc: 0.9857 - val_loss: 0.3624 - val_acc: 0.8634\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1055 - acc: 0.9871 - val_loss: 0.3599 - val_acc: 0.8608\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0994 - acc: 0.9879 - val_loss: 0.3564 - val_acc: 0.8667\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0938 - acc: 0.9887 - val_loss: 0.3539 - val_acc: 0.8654\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0883 - acc: 0.9899 - val_loss: 0.3524 - val_acc: 0.8682\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0834 - acc: 0.9906 - val_loss: 0.3493 - val_acc: 0.8672\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0787 - acc: 0.9911 - val_loss: 0.3476 - val_acc: 0.8670\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0744 - acc: 0.9916 - val_loss: 0.3459 - val_acc: 0.8662\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0703 - acc: 0.9924 - val_loss: 0.3456 - val_acc: 0.8639\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0665 - acc: 0.9930 - val_loss: 0.3436 - val_acc: 0.8682\n",
      "Epoch 44/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0627 - acc: 0.9936 - val_loss: 0.3420 - val_acc: 0.8664\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0596 - acc: 0.9941 - val_loss: 0.3410 - val_acc: 0.8685\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0560 - acc: 0.9948 - val_loss: 0.3402 - val_acc: 0.8685\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0531 - acc: 0.9948 - val_loss: 0.3394 - val_acc: 0.8687\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0501 - acc: 0.9957 - val_loss: 0.3399 - val_acc: 0.8659\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0474 - acc: 0.9957 - val_loss: 0.3386 - val_acc: 0.8690\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0449 - acc: 0.9960 - val_loss: 0.3384 - val_acc: 0.8682\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0424 - acc: 0.9965 - val_loss: 0.3380 - val_acc: 0.8695\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0402 - acc: 0.9971 - val_loss: 0.3392 - val_acc: 0.8672\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0380 - acc: 0.9969 - val_loss: 0.3384 - val_acc: 0.8703\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0360 - acc: 0.9970 - val_loss: 0.3378 - val_acc: 0.8680\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0341 - acc: 0.9976 - val_loss: 0.3387 - val_acc: 0.8672\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0323 - acc: 0.9973 - val_loss: 0.3395 - val_acc: 0.8675\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0305 - acc: 0.9976 - val_loss: 0.3396 - val_acc: 0.8695\n",
      "15663/15663 [==============================] - 0s 20us/step\n",
      "==== Training loss, score are: 0.029169710141814797 0.9976377450041499 =======\n",
      "3916/3916 [==============================] - 0s 20us/step\n",
      "==== CV loss, score are: 0.3396222192463032 0.8695097037793666 =======\n",
      "Running fold 5\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_8 ( (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15664/15664 [==============================] - 0s 29us/step\n",
      "Before training loss, score are: 1.101763122902462 0.31205311542390196\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/150\n",
      "15664/15664 [==============================] - 1s 56us/step - loss: 1.0860 - acc: 0.3878 - val_loss: 1.0697 - val_acc: 0.4212\n",
      "Epoch 2/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 1.0614 - acc: 0.4044 - val_loss: 1.0443 - val_acc: 0.4383\n",
      "Epoch 3/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 1.0187 - acc: 0.4578 - val_loss: 0.9989 - val_acc: 0.5188\n",
      "Epoch 4/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.9545 - acc: 0.5830 - val_loss: 0.9405 - val_acc: 0.6542\n",
      "Epoch 5/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.8779 - acc: 0.7065 - val_loss: 0.8758 - val_acc: 0.7195\n",
      "Epoch 6/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.7991 - acc: 0.7811 - val_loss: 0.8124 - val_acc: 0.7280\n",
      "Epoch 7/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.7248 - acc: 0.8181 - val_loss: 0.7567 - val_acc: 0.7678\n",
      "Epoch 8/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.6583 - acc: 0.8412 - val_loss: 0.7086 - val_acc: 0.7750\n",
      "Epoch 9/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.6003 - acc: 0.8573 - val_loss: 0.6665 - val_acc: 0.7985\n",
      "Epoch 10/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.5490 - acc: 0.8753 - val_loss: 0.6304 - val_acc: 0.8095\n",
      "Epoch 11/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.5045 - acc: 0.8889 - val_loss: 0.6004 - val_acc: 0.8156\n",
      "Epoch 12/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.4648 - acc: 0.8982 - val_loss: 0.5732 - val_acc: 0.8232\n",
      "Epoch 13/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.4294 - acc: 0.9088 - val_loss: 0.5501 - val_acc: 0.8309\n",
      "Epoch 14/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.3977 - acc: 0.9185 - val_loss: 0.5287 - val_acc: 0.8324\n",
      "Epoch 15/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.3695 - acc: 0.9248 - val_loss: 0.5108 - val_acc: 0.8396\n",
      "Epoch 16/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.3437 - acc: 0.9333 - val_loss: 0.4944 - val_acc: 0.8437\n",
      "Epoch 17/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.3202 - acc: 0.9400 - val_loss: 0.4791 - val_acc: 0.8437\n",
      "Epoch 18/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.2985 - acc: 0.9457 - val_loss: 0.4675 - val_acc: 0.8511\n",
      "Epoch 19/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.2788 - acc: 0.9508 - val_loss: 0.4539 - val_acc: 0.8506\n",
      "Epoch 20/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.2608 - acc: 0.9554 - val_loss: 0.4429 - val_acc: 0.8531\n",
      "Epoch 21/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.2441 - acc: 0.9580 - val_loss: 0.4330 - val_acc: 0.8526\n",
      "Epoch 22/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.2288 - acc: 0.9622 - val_loss: 0.4238 - val_acc: 0.8572\n",
      "Epoch 23/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.2144 - acc: 0.9651 - val_loss: 0.4165 - val_acc: 0.8587\n",
      "Epoch 24/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.2012 - acc: 0.9683 - val_loss: 0.4079 - val_acc: 0.8554\n",
      "Epoch 25/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.1890 - acc: 0.9710 - val_loss: 0.4010 - val_acc: 0.8554\n",
      "Epoch 26/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.1774 - acc: 0.9734 - val_loss: 0.3943 - val_acc: 0.8590\n",
      "Epoch 27/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.1667 - acc: 0.9746 - val_loss: 0.3886 - val_acc: 0.8595\n",
      "Epoch 28/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.1567 - acc: 0.9771 - val_loss: 0.3829 - val_acc: 0.8600\n",
      "Epoch 29/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.1476 - acc: 0.9787 - val_loss: 0.3782 - val_acc: 0.8616\n",
      "Epoch 30/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.1389 - acc: 0.9813 - val_loss: 0.3740 - val_acc: 0.8590\n",
      "Epoch 31/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.1309 - acc: 0.9826 - val_loss: 0.3699 - val_acc: 0.8649\n",
      "Epoch 32/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.1232 - acc: 0.9843 - val_loss: 0.3657 - val_acc: 0.8626\n",
      "Epoch 33/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.1161 - acc: 0.9854 - val_loss: 0.3630 - val_acc: 0.8633\n",
      "Epoch 34/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.1095 - acc: 0.9860 - val_loss: 0.3598 - val_acc: 0.8674\n",
      "Epoch 35/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.1033 - acc: 0.9874 - val_loss: 0.3577 - val_acc: 0.8613\n",
      "Epoch 36/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0975 - acc: 0.9884 - val_loss: 0.3538 - val_acc: 0.8649\n",
      "Epoch 37/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0920 - acc: 0.9900 - val_loss: 0.3518 - val_acc: 0.8667\n",
      "Epoch 38/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0869 - acc: 0.9904 - val_loss: 0.3495 - val_acc: 0.8644\n",
      "Epoch 39/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0819 - acc: 0.9913 - val_loss: 0.3474 - val_acc: 0.8669\n",
      "Epoch 40/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0773 - acc: 0.9917 - val_loss: 0.3459 - val_acc: 0.8651\n",
      "Epoch 41/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0731 - acc: 0.9924 - val_loss: 0.3449 - val_acc: 0.8685\n",
      "Epoch 42/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0692 - acc: 0.9928 - val_loss: 0.3436 - val_acc: 0.8641\n",
      "Epoch 43/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0654 - acc: 0.9930 - val_loss: 0.3424 - val_acc: 0.8646\n",
      "Epoch 44/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0618 - acc: 0.9938 - val_loss: 0.3420 - val_acc: 0.8656\n",
      "Epoch 45/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0584 - acc: 0.9945 - val_loss: 0.3407 - val_acc: 0.8651\n",
      "Epoch 46/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0554 - acc: 0.9948 - val_loss: 0.3399 - val_acc: 0.8679\n",
      "Epoch 47/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0523 - acc: 0.9949 - val_loss: 0.3393 - val_acc: 0.8651\n",
      "Epoch 48/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0495 - acc: 0.9955 - val_loss: 0.3384 - val_acc: 0.8672\n",
      "Epoch 49/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0468 - acc: 0.9959 - val_loss: 0.3385 - val_acc: 0.8667\n",
      "Epoch 50/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.0443 - acc: 0.9960 - val_loss: 0.3382 - val_acc: 0.8659\n",
      "Epoch 51/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0420 - acc: 0.9962 - val_loss: 0.3389 - val_acc: 0.8649\n",
      "Epoch 52/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.0399 - acc: 0.9964 - val_loss: 0.3386 - val_acc: 0.8623\n",
      "Epoch 53/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0377 - acc: 0.9965 - val_loss: 0.3388 - val_acc: 0.8628\n",
      "15664/15664 [==============================] - 0s 20us/step\n",
      "==== Training loss, score are: 0.03597937589672996 0.9966164453524005 =======\n",
      "3915/3915 [==============================] - 0s 20us/step\n",
      "==== CV loss, score are: 0.3387813589962658 0.862835249118269 =======\n",
      "\n",
      "\n",
      "===== Model: fast_text_none  ========:\n",
      " Cross-val log losses are: [0.35613939757900015, 0.34937798852393853, 0.33493111868839998, 0.33962221644782714, 0.3387813549996288]\n",
      "====== Mean cross-val log loss is: 0.34377041524775886 =========\n",
      "\n",
      "\n",
      "Timestamp: 2018-Jan-14 02:55:16\n",
      "Running kfold training with model fast_text_gensim\n",
      "Shapes: x_train_raw.shape (19579, 21), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 20)\n",
      "Running preprocess: <function run_NN_preprocess at 0x7f620ee8eea0>\n",
      "No of unique words found is 247346\n",
      "No of words occuring more than 2 times is 75104\n",
      "No of unique words found is after capping is 247346\n",
      "max sentence length is 1439\n",
      "mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000\n",
      "After padding: max sentence size is 161\n",
      "After padding: mean, median length of sentence is 161.000000 161.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9584it [00:00, 95826.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word embedding matrix for gensim vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76597it [00:00, 96042.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76597 word vectors.\n",
      "of--mirth 247345\n",
      "None\n",
      "(247346, 20)\n",
      "Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)\n",
      "Running fold 1\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_9 ( (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 0s 29us/step\n",
      "Before training loss, score are: 1.1140749234772045 0.3340356253610293\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 57us/step - loss: 1.0628 - acc: 0.4271 - val_loss: 1.0470 - val_acc: 0.4341\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 1.0204 - acc: 0.4891 - val_loss: 1.0104 - val_acc: 0.4844\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.9839 - acc: 0.5383 - val_loss: 0.9759 - val_acc: 0.5406\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.9479 - acc: 0.5811 - val_loss: 0.9433 - val_acc: 0.5896\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.9118 - acc: 0.6154 - val_loss: 0.9101 - val_acc: 0.6162\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.8749 - acc: 0.6483 - val_loss: 0.8788 - val_acc: 0.6254\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.8383 - acc: 0.6741 - val_loss: 0.8478 - val_acc: 0.6466\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.8018 - acc: 0.6966 - val_loss: 0.8178 - val_acc: 0.6739\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.7660 - acc: 0.7171 - val_loss: 0.7901 - val_acc: 0.6862\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.7314 - acc: 0.7358 - val_loss: 0.7635 - val_acc: 0.7056\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.6977 - acc: 0.7522 - val_loss: 0.7386 - val_acc: 0.7199\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.6655 - acc: 0.7696 - val_loss: 0.7155 - val_acc: 0.7255\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.6346 - acc: 0.7841 - val_loss: 0.6939 - val_acc: 0.7337\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.6052 - acc: 0.8002 - val_loss: 0.6733 - val_acc: 0.7441\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.5768 - acc: 0.8116 - val_loss: 0.6537 - val_acc: 0.7551\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.5501 - acc: 0.8235 - val_loss: 0.6361 - val_acc: 0.7615\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.5245 - acc: 0.8368 - val_loss: 0.6189 - val_acc: 0.7699\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.5002 - acc: 0.8481 - val_loss: 0.6052 - val_acc: 0.7699\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.4771 - acc: 0.8550 - val_loss: 0.5910 - val_acc: 0.7771\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.4552 - acc: 0.8650 - val_loss: 0.5760 - val_acc: 0.7835\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.4343 - acc: 0.8722 - val_loss: 0.5623 - val_acc: 0.7906\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.4140 - acc: 0.8819 - val_loss: 0.5500 - val_acc: 0.7972\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3954 - acc: 0.8894 - val_loss: 0.5393 - val_acc: 0.7978\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3769 - acc: 0.8956 - val_loss: 0.5297 - val_acc: 0.8008\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3601 - acc: 0.9013 - val_loss: 0.5185 - val_acc: 0.8080\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3436 - acc: 0.9088 - val_loss: 0.5087 - val_acc: 0.8092\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3278 - acc: 0.9137 - val_loss: 0.5010 - val_acc: 0.8108\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.3130 - acc: 0.9197 - val_loss: 0.4934 - val_acc: 0.8126\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2989 - acc: 0.9250 - val_loss: 0.4839 - val_acc: 0.8172\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.2852 - acc: 0.9300 - val_loss: 0.4764 - val_acc: 0.8200\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2723 - acc: 0.9343 - val_loss: 0.4699 - val_acc: 0.8225\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2602 - acc: 0.9383 - val_loss: 0.4642 - val_acc: 0.8233\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2484 - acc: 0.9420 - val_loss: 0.4587 - val_acc: 0.8212\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2371 - acc: 0.9468 - val_loss: 0.4522 - val_acc: 0.8241\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2264 - acc: 0.9492 - val_loss: 0.4462 - val_acc: 0.8281\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2163 - acc: 0.9536 - val_loss: 0.4407 - val_acc: 0.8294\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2064 - acc: 0.9568 - val_loss: 0.4366 - val_acc: 0.8292\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1972 - acc: 0.9591 - val_loss: 0.4313 - val_acc: 0.8320\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1884 - acc: 0.9612 - val_loss: 0.4272 - val_acc: 0.8340\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1800 - acc: 0.9628 - val_loss: 0.4252 - val_acc: 0.8322\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1719 - acc: 0.9658 - val_loss: 0.4202 - val_acc: 0.8348\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1642 - acc: 0.9678 - val_loss: 0.4153 - val_acc: 0.8355\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1569 - acc: 0.9697 - val_loss: 0.4126 - val_acc: 0.8358\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1500 - acc: 0.9714 - val_loss: 0.4091 - val_acc: 0.8384\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1432 - acc: 0.9736 - val_loss: 0.4090 - val_acc: 0.8376\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1369 - acc: 0.9744 - val_loss: 0.4046 - val_acc: 0.8389\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1308 - acc: 0.9767 - val_loss: 0.4044 - val_acc: 0.8389\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1251 - acc: 0.9779 - val_loss: 0.3994 - val_acc: 0.8417\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1195 - acc: 0.9785 - val_loss: 0.3975 - val_acc: 0.8417\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1143 - acc: 0.9805 - val_loss: 0.3967 - val_acc: 0.8424\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1092 - acc: 0.9810 - val_loss: 0.3935 - val_acc: 0.8424\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1043 - acc: 0.9827 - val_loss: 0.3933 - val_acc: 0.8445\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0997 - acc: 0.9836 - val_loss: 0.3906 - val_acc: 0.8440\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0953 - acc: 0.9848 - val_loss: 0.3898 - val_acc: 0.8475\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0914 - acc: 0.9856 - val_loss: 0.3876 - val_acc: 0.8463\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0872 - acc: 0.9861 - val_loss: 0.3862 - val_acc: 0.8473\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0833 - acc: 0.9872 - val_loss: 0.3860 - val_acc: 0.8473\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0797 - acc: 0.9877 - val_loss: 0.3848 - val_acc: 0.8504\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0762 - acc: 0.9886 - val_loss: 0.3852 - val_acc: 0.8498\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0728 - acc: 0.9891 - val_loss: 0.3835 - val_acc: 0.8498\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0697 - acc: 0.9897 - val_loss: 0.3827 - val_acc: 0.8498\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0667 - acc: 0.9900 - val_loss: 0.3824 - val_acc: 0.8501\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0638 - acc: 0.9907 - val_loss: 0.3821 - val_acc: 0.8509\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0610 - acc: 0.9910 - val_loss: 0.3825 - val_acc: 0.8511\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0583 - acc: 0.9921 - val_loss: 0.3817 - val_acc: 0.8514\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0558 - acc: 0.9923 - val_loss: 0.3855 - val_acc: 0.8498\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0534 - acc: 0.9928 - val_loss: 0.3831 - val_acc: 0.8519\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.0511 - acc: 0.9934 - val_loss: 0.3826 - val_acc: 0.8514\n",
      "15663/15663 [==============================] - 0s 20us/step\n",
      "==== Training loss, score are: 0.04918092788635737 0.9938709059567132 =======\n",
      "3916/3916 [==============================] - 0s 20us/step\n",
      "==== CV loss, score are: 0.38258479510465365 0.8513789581205311 =======\n",
      "Running fold 2\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_10  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 0s 30us/step\n",
      "Before training loss, score are: 1.120552753242262 0.2824490838376436\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 57us/step - loss: 1.0913 - acc: 0.3937 - val_loss: 1.0707 - val_acc: 0.4193\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 1.0492 - acc: 0.4501 - val_loss: 1.0366 - val_acc: 0.4448\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 1.0120 - acc: 0.4997 - val_loss: 1.0031 - val_acc: 0.5074\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.9763 - acc: 0.5468 - val_loss: 0.9716 - val_acc: 0.5603\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.9414 - acc: 0.5905 - val_loss: 0.9426 - val_acc: 0.5751\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.9063 - acc: 0.6205 - val_loss: 0.9122 - val_acc: 0.6085\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.8713 - acc: 0.6494 - val_loss: 0.8835 - val_acc: 0.6272\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.8365 - acc: 0.6746 - val_loss: 0.8557 - val_acc: 0.6522\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.8026 - acc: 0.6984 - val_loss: 0.8292 - val_acc: 0.6596\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.7687 - acc: 0.7170 - val_loss: 0.8035 - val_acc: 0.6759\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.7361 - acc: 0.7366 - val_loss: 0.7788 - val_acc: 0.6938\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.7039 - acc: 0.7526 - val_loss: 0.7565 - val_acc: 0.7025\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.6730 - acc: 0.7678 - val_loss: 0.7341 - val_acc: 0.7114\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.6433 - acc: 0.7820 - val_loss: 0.7133 - val_acc: 0.7194\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.6145 - acc: 0.7943 - val_loss: 0.6931 - val_acc: 0.7291\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.5868 - acc: 0.8078 - val_loss: 0.6750 - val_acc: 0.7360\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5602 - acc: 0.8169 - val_loss: 0.6567 - val_acc: 0.7490\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.5349 - acc: 0.8309 - val_loss: 0.6412 - val_acc: 0.7505\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.5105 - acc: 0.8401 - val_loss: 0.6237 - val_acc: 0.7615\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4874 - acc: 0.8513 - val_loss: 0.6088 - val_acc: 0.7684\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4651 - acc: 0.8592 - val_loss: 0.5950 - val_acc: 0.7697\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4440 - acc: 0.8704 - val_loss: 0.5809 - val_acc: 0.7760\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.4234 - acc: 0.8779 - val_loss: 0.5682 - val_acc: 0.7801\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4042 - acc: 0.8846 - val_loss: 0.5562 - val_acc: 0.7840\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3858 - acc: 0.8921 - val_loss: 0.5446 - val_acc: 0.7911\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3682 - acc: 0.8999 - val_loss: 0.5337 - val_acc: 0.7939\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3514 - acc: 0.9051 - val_loss: 0.5248 - val_acc: 0.8018\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.3352 - acc: 0.9129 - val_loss: 0.5139 - val_acc: 0.8067\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3198 - acc: 0.9194 - val_loss: 0.5047 - val_acc: 0.8103\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3053 - acc: 0.9252 - val_loss: 0.4964 - val_acc: 0.8126\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2914 - acc: 0.9301 - val_loss: 0.4880 - val_acc: 0.8154\n",
      "Epoch 32/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2780 - acc: 0.9351 - val_loss: 0.4808 - val_acc: 0.8195\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2655 - acc: 0.9386 - val_loss: 0.4733 - val_acc: 0.8195\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.2534 - acc: 0.9427 - val_loss: 0.4669 - val_acc: 0.8233\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.2421 - acc: 0.9467 - val_loss: 0.4605 - val_acc: 0.8228\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.2309 - acc: 0.9504 - val_loss: 0.4548 - val_acc: 0.8246\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2205 - acc: 0.9528 - val_loss: 0.4489 - val_acc: 0.8266\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.2106 - acc: 0.9549 - val_loss: 0.4436 - val_acc: 0.8297\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.2009 - acc: 0.9588 - val_loss: 0.4390 - val_acc: 0.8299\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1920 - acc: 0.9608 - val_loss: 0.4331 - val_acc: 0.8335\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1834 - acc: 0.9640 - val_loss: 0.4301 - val_acc: 0.8327\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1751 - acc: 0.9653 - val_loss: 0.4247 - val_acc: 0.8343\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 46us/step - loss: 0.1673 - acc: 0.9678 - val_loss: 0.4210 - val_acc: 0.8353\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1597 - acc: 0.9701 - val_loss: 0.4179 - val_acc: 0.8348\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1525 - acc: 0.9715 - val_loss: 0.4134 - val_acc: 0.8407\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1458 - acc: 0.9738 - val_loss: 0.4099 - val_acc: 0.8389\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1393 - acc: 0.9749 - val_loss: 0.4070 - val_acc: 0.8396\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1329 - acc: 0.9770 - val_loss: 0.4054 - val_acc: 0.8414\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1272 - acc: 0.9780 - val_loss: 0.4017 - val_acc: 0.8427\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1215 - acc: 0.9795 - val_loss: 0.3987 - val_acc: 0.8447\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1160 - acc: 0.9804 - val_loss: 0.3963 - val_acc: 0.8463\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1109 - acc: 0.9817 - val_loss: 0.3943 - val_acc: 0.8468\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1057 - acc: 0.9831 - val_loss: 0.3933 - val_acc: 0.8455\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1011 - acc: 0.9838 - val_loss: 0.3908 - val_acc: 0.8470\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0964 - acc: 0.9850 - val_loss: 0.3894 - val_acc: 0.8473\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0923 - acc: 0.9856 - val_loss: 0.3878 - val_acc: 0.8470\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0881 - acc: 0.9860 - val_loss: 0.3861 - val_acc: 0.8486\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0842 - acc: 0.9868 - val_loss: 0.3851 - val_acc: 0.8486\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0804 - acc: 0.9879 - val_loss: 0.3840 - val_acc: 0.8486\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0767 - acc: 0.9887 - val_loss: 0.3840 - val_acc: 0.8506\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0735 - acc: 0.9893 - val_loss: 0.3820 - val_acc: 0.8496\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0702 - acc: 0.9902 - val_loss: 0.3813 - val_acc: 0.8491\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0671 - acc: 0.9905 - val_loss: 0.3801 - val_acc: 0.8483\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0640 - acc: 0.9909 - val_loss: 0.3794 - val_acc: 0.8496\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0613 - acc: 0.9913 - val_loss: 0.3790 - val_acc: 0.8483\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0585 - acc: 0.9923 - val_loss: 0.3786 - val_acc: 0.8501\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0560 - acc: 0.9928 - val_loss: 0.3787 - val_acc: 0.8509\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0535 - acc: 0.9933 - val_loss: 0.3782 - val_acc: 0.8501\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0512 - acc: 0.9936 - val_loss: 0.3780 - val_acc: 0.8501\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0489 - acc: 0.9939 - val_loss: 0.3785 - val_acc: 0.8504\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0467 - acc: 0.9942 - val_loss: 0.3783 - val_acc: 0.8516\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0446 - acc: 0.9946 - val_loss: 0.3789 - val_acc: 0.8498\n",
      "15663/15663 [==============================] - 0s 20us/step\n",
      "==== Training loss, score are: 0.0429314952176025 0.9947647321713593 =======\n",
      "3916/3916 [==============================] - 0s 20us/step\n",
      "==== CV loss, score are: 0.3788707141400114 0.849846782431052 =======\n",
      "Running fold 3\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_11  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 0s 30us/step\n",
      "Before training loss, score are: 1.2773297683021951 0.30837004406618246\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 57us/step - loss: 1.1382 - acc: 0.3461 - val_loss: 1.0863 - val_acc: 0.3864\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 1.0679 - acc: 0.4151 - val_loss: 1.0496 - val_acc: 0.4489\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 1.0291 - acc: 0.4800 - val_loss: 1.0163 - val_acc: 0.4962\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.9923 - acc: 0.5299 - val_loss: 0.9844 - val_acc: 0.5472\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.9562 - acc: 0.5737 - val_loss: 0.9533 - val_acc: 0.5802\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.9201 - acc: 0.6119 - val_loss: 0.9233 - val_acc: 0.5955\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.8840 - acc: 0.6391 - val_loss: 0.8930 - val_acc: 0.6364\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.8481 - acc: 0.6651 - val_loss: 0.8642 - val_acc: 0.6438\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.8126 - acc: 0.6881 - val_loss: 0.8379 - val_acc: 0.6486\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.7782 - acc: 0.7087 - val_loss: 0.8095 - val_acc: 0.6790\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.7441 - acc: 0.7273 - val_loss: 0.7849 - val_acc: 0.6813\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.7118 - acc: 0.7456 - val_loss: 0.7601 - val_acc: 0.6984\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.6802 - acc: 0.7600 - val_loss: 0.7369 - val_acc: 0.7148\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.6498 - acc: 0.7770 - val_loss: 0.7152 - val_acc: 0.7165\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.6206 - acc: 0.7903 - val_loss: 0.6944 - val_acc: 0.7324\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.5923 - acc: 0.8042 - val_loss: 0.6752 - val_acc: 0.7357\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.5656 - acc: 0.8145 - val_loss: 0.6560 - val_acc: 0.7477\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.5395 - acc: 0.8270 - val_loss: 0.6385 - val_acc: 0.7541\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.5148 - acc: 0.8391 - val_loss: 0.6216 - val_acc: 0.7610\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.4909 - acc: 0.8493 - val_loss: 0.6062 - val_acc: 0.7648\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.4683 - acc: 0.8573 - val_loss: 0.5910 - val_acc: 0.7748\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.4468 - acc: 0.8687 - val_loss: 0.5777 - val_acc: 0.7689\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.4262 - acc: 0.8742 - val_loss: 0.5634 - val_acc: 0.7850\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.4063 - acc: 0.8851 - val_loss: 0.5504 - val_acc: 0.7891\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.3874 - acc: 0.8911 - val_loss: 0.5387 - val_acc: 0.7934\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.3695 - acc: 0.8989 - val_loss: 0.5273 - val_acc: 0.7967\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.3524 - acc: 0.9051 - val_loss: 0.5166 - val_acc: 0.8008\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.3361 - acc: 0.9110 - val_loss: 0.5071 - val_acc: 0.8067\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.3207 - acc: 0.9172 - val_loss: 0.4971 - val_acc: 0.8108\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.3058 - acc: 0.9224 - val_loss: 0.4879 - val_acc: 0.8131\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.2917 - acc: 0.9273 - val_loss: 0.4788 - val_acc: 0.8159\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.2783 - acc: 0.9321 - val_loss: 0.4710 - val_acc: 0.8151\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.2655 - acc: 0.9356 - val_loss: 0.4634 - val_acc: 0.8179\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.2533 - acc: 0.9404 - val_loss: 0.4556 - val_acc: 0.8235\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.2417 - acc: 0.9440 - val_loss: 0.4486 - val_acc: 0.8281\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.2308 - acc: 0.9469 - val_loss: 0.4422 - val_acc: 0.8302\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.2202 - acc: 0.9514 - val_loss: 0.4356 - val_acc: 0.8338\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.2100 - acc: 0.9554 - val_loss: 0.4306 - val_acc: 0.8340\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.2007 - acc: 0.9581 - val_loss: 0.4259 - val_acc: 0.8350\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1916 - acc: 0.9612 - val_loss: 0.4199 - val_acc: 0.8378\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1829 - acc: 0.9628 - val_loss: 0.4142 - val_acc: 0.8401\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1746 - acc: 0.9660 - val_loss: 0.4094 - val_acc: 0.8432\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1667 - acc: 0.9684 - val_loss: 0.4052 - val_acc: 0.8458\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1592 - acc: 0.9699 - val_loss: 0.4022 - val_acc: 0.8437\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1520 - acc: 0.9718 - val_loss: 0.3980 - val_acc: 0.8478\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1452 - acc: 0.9741 - val_loss: 0.3936 - val_acc: 0.8506\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1388 - acc: 0.9754 - val_loss: 0.3899 - val_acc: 0.8521\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1324 - acc: 0.9771 - val_loss: 0.3874 - val_acc: 0.8521\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1267 - acc: 0.9784 - val_loss: 0.3839 - val_acc: 0.8537\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1209 - acc: 0.9801 - val_loss: 0.3814 - val_acc: 0.8529\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1154 - acc: 0.9814 - val_loss: 0.3782 - val_acc: 0.8547\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1102 - acc: 0.9822 - val_loss: 0.3757 - val_acc: 0.8552\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1052 - acc: 0.9834 - val_loss: 0.3733 - val_acc: 0.8580\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1006 - acc: 0.9845 - val_loss: 0.3713 - val_acc: 0.8573\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0962 - acc: 0.9859 - val_loss: 0.3703 - val_acc: 0.8562\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0919 - acc: 0.9861 - val_loss: 0.3683 - val_acc: 0.8608\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0880 - acc: 0.9872 - val_loss: 0.3667 - val_acc: 0.8585\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0839 - acc: 0.9881 - val_loss: 0.3642 - val_acc: 0.8608\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0802 - acc: 0.9887 - val_loss: 0.3624 - val_acc: 0.8616\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0766 - acc: 0.9894 - val_loss: 0.3611 - val_acc: 0.8613\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0733 - acc: 0.9899 - val_loss: 0.3601 - val_acc: 0.8603\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0699 - acc: 0.9901 - val_loss: 0.3587 - val_acc: 0.8621\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0669 - acc: 0.9906 - val_loss: 0.3584 - val_acc: 0.8603\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0639 - acc: 0.9913 - val_loss: 0.3571 - val_acc: 0.8618\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0611 - acc: 0.9916 - val_loss: 0.3584 - val_acc: 0.8636\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0584 - acc: 0.9919 - val_loss: 0.3556 - val_acc: 0.8639\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0558 - acc: 0.9923 - val_loss: 0.3546 - val_acc: 0.8611\n",
      "Epoch 68/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0533 - acc: 0.9927 - val_loss: 0.3544 - val_acc: 0.8649\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0510 - acc: 0.9928 - val_loss: 0.3549 - val_acc: 0.8639\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0487 - acc: 0.9936 - val_loss: 0.3543 - val_acc: 0.8649\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0465 - acc: 0.9947 - val_loss: 0.3553 - val_acc: 0.8649\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0445 - acc: 0.9946 - val_loss: 0.3545 - val_acc: 0.8636\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0426 - acc: 0.9951 - val_loss: 0.3540 - val_acc: 0.8672\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0407 - acc: 0.9953 - val_loss: 0.3535 - val_acc: 0.8636\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0389 - acc: 0.9958 - val_loss: 0.3543 - val_acc: 0.8659\n",
      "Epoch 76/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0373 - acc: 0.9958 - val_loss: 0.3542 - val_acc: 0.8672\n",
      "Epoch 77/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0357 - acc: 0.9959 - val_loss: 0.3541 - val_acc: 0.8675\n",
      "15663/15663 [==============================] - 0s 20us/step\n",
      "==== Training loss, score are: 0.034232947076361334 0.9964246951414161 =======\n",
      "3916/3916 [==============================] - 0s 20us/step\n",
      "==== CV loss, score are: 0.35414784215200423 0.8674668028600613 =======\n",
      "Running fold 4\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_12  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 0s 31us/step\n",
      "Before training loss, score are: 1.1268716988751983 0.3182659771464409\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 58us/step - loss: 1.0819 - acc: 0.3912 - val_loss: 1.0605 - val_acc: 0.4277\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 1.0375 - acc: 0.4562 - val_loss: 1.0222 - val_acc: 0.4780\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.9988 - acc: 0.5181 - val_loss: 0.9869 - val_acc: 0.5332\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.9609 - acc: 0.5667 - val_loss: 0.9528 - val_acc: 0.5720\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.9228 - acc: 0.6160 - val_loss: 0.9193 - val_acc: 0.6210\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.8838 - acc: 0.6479 - val_loss: 0.8861 - val_acc: 0.6443\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.8448 - acc: 0.6817 - val_loss: 0.8542 - val_acc: 0.6601\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.8063 - acc: 0.7008 - val_loss: 0.8237 - val_acc: 0.6846\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.7690 - acc: 0.7234 - val_loss: 0.7946 - val_acc: 0.6925\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.7330 - acc: 0.7391 - val_loss: 0.7679 - val_acc: 0.7053\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.6985 - acc: 0.7587 - val_loss: 0.7426 - val_acc: 0.7155\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.6658 - acc: 0.7723 - val_loss: 0.7200 - val_acc: 0.7316\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.6347 - acc: 0.7883 - val_loss: 0.6982 - val_acc: 0.7388\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.6053 - acc: 0.8013 - val_loss: 0.6768 - val_acc: 0.7444\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.5773 - acc: 0.8136 - val_loss: 0.6605 - val_acc: 0.7533\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.5507 - acc: 0.8241 - val_loss: 0.6400 - val_acc: 0.7582\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.5254 - acc: 0.8335 - val_loss: 0.6229 - val_acc: 0.7640\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.5013 - acc: 0.8449 - val_loss: 0.6070 - val_acc: 0.7730\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.4787 - acc: 0.8546 - val_loss: 0.5923 - val_acc: 0.7758\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.4568 - acc: 0.8631 - val_loss: 0.5790 - val_acc: 0.7860\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.4360 - acc: 0.8714 - val_loss: 0.5661 - val_acc: 0.7886\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.4160 - acc: 0.8823 - val_loss: 0.5532 - val_acc: 0.7944\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.3972 - acc: 0.8911 - val_loss: 0.5407 - val_acc: 0.7947\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.3792 - acc: 0.8964 - val_loss: 0.5299 - val_acc: 0.8031\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.3620 - acc: 0.9031 - val_loss: 0.5190 - val_acc: 0.8057\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.3455 - acc: 0.9085 - val_loss: 0.5091 - val_acc: 0.8115\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.3301 - acc: 0.9143 - val_loss: 0.5007 - val_acc: 0.8164\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.3150 - acc: 0.9199 - val_loss: 0.4910 - val_acc: 0.8144\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.3009 - acc: 0.9256 - val_loss: 0.4825 - val_acc: 0.8184\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.2875 - acc: 0.9306 - val_loss: 0.4746 - val_acc: 0.8225\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.2745 - acc: 0.9346 - val_loss: 0.4671 - val_acc: 0.8274\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.2622 - acc: 0.9385 - val_loss: 0.4599 - val_acc: 0.8287\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.2504 - acc: 0.9418 - val_loss: 0.4539 - val_acc: 0.8302\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.2393 - acc: 0.9457 - val_loss: 0.4465 - val_acc: 0.8315\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.2286 - acc: 0.9485 - val_loss: 0.4406 - val_acc: 0.8332\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.2182 - acc: 0.9523 - val_loss: 0.4371 - val_acc: 0.8366\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.2088 - acc: 0.9558 - val_loss: 0.4297 - val_acc: 0.8361\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1993 - acc: 0.9583 - val_loss: 0.4254 - val_acc: 0.8376\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1906 - acc: 0.9604 - val_loss: 0.4199 - val_acc: 0.8386\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1823 - acc: 0.9641 - val_loss: 0.4152 - val_acc: 0.8414\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1743 - acc: 0.9659 - val_loss: 0.4114 - val_acc: 0.8419\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1666 - acc: 0.9681 - val_loss: 0.4073 - val_acc: 0.8404\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1592 - acc: 0.9700 - val_loss: 0.4031 - val_acc: 0.8455\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1522 - acc: 0.9724 - val_loss: 0.4002 - val_acc: 0.8468\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1455 - acc: 0.9737 - val_loss: 0.3965 - val_acc: 0.8478\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1391 - acc: 0.9754 - val_loss: 0.3928 - val_acc: 0.8493\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1330 - acc: 0.9767 - val_loss: 0.3900 - val_acc: 0.8493\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1271 - acc: 0.9781 - val_loss: 0.3870 - val_acc: 0.8498\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1216 - acc: 0.9795 - val_loss: 0.3849 - val_acc: 0.8519\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1163 - acc: 0.9807 - val_loss: 0.3818 - val_acc: 0.8506\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.1111 - acc: 0.9824 - val_loss: 0.3796 - val_acc: 0.8514\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1062 - acc: 0.9833 - val_loss: 0.3778 - val_acc: 0.8542\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.1016 - acc: 0.9837 - val_loss: 0.3755 - val_acc: 0.8527\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0974 - acc: 0.9849 - val_loss: 0.3740 - val_acc: 0.8527\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0930 - acc: 0.9857 - val_loss: 0.3721 - val_acc: 0.8532\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0889 - acc: 0.9867 - val_loss: 0.3706 - val_acc: 0.8534\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0850 - acc: 0.9872 - val_loss: 0.3691 - val_acc: 0.8562\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0813 - acc: 0.9875 - val_loss: 0.3674 - val_acc: 0.8557\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0778 - acc: 0.9886 - val_loss: 0.3666 - val_acc: 0.8550\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0744 - acc: 0.9891 - val_loss: 0.3650 - val_acc: 0.8583\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0711 - acc: 0.9897 - val_loss: 0.3640 - val_acc: 0.8565\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0680 - acc: 0.9903 - val_loss: 0.3636 - val_acc: 0.8567\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0651 - acc: 0.9905 - val_loss: 0.3621 - val_acc: 0.8585\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0622 - acc: 0.9909 - val_loss: 0.3616 - val_acc: 0.8603\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0596 - acc: 0.9915 - val_loss: 0.3610 - val_acc: 0.8588\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0569 - acc: 0.9918 - val_loss: 0.3604 - val_acc: 0.8601\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0545 - acc: 0.9918 - val_loss: 0.3605 - val_acc: 0.8575\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0521 - acc: 0.9926 - val_loss: 0.3595 - val_acc: 0.8596\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0500 - acc: 0.9933 - val_loss: 0.3605 - val_acc: 0.8590\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0477 - acc: 0.9936 - val_loss: 0.3598 - val_acc: 0.8593\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 1s 44us/step - loss: 0.0457 - acc: 0.9937 - val_loss: 0.3591 - val_acc: 0.8585\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0437 - acc: 0.9944 - val_loss: 0.3589 - val_acc: 0.8611\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0418 - acc: 0.9946 - val_loss: 0.3586 - val_acc: 0.8585\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0400 - acc: 0.9950 - val_loss: 0.3587 - val_acc: 0.8583\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0382 - acc: 0.9956 - val_loss: 0.3599 - val_acc: 0.8562\n",
      "Epoch 76/150\n",
      "15663/15663 [==============================] - 1s 45us/step - loss: 0.0366 - acc: 0.9960 - val_loss: 0.3592 - val_acc: 0.8585\n",
      "15663/15663 [==============================] - 0s 20us/step\n",
      "==== Training loss, score are: 0.0352395822113952 0.9963608504117986 =======\n",
      "3916/3916 [==============================] - 0s 21us/step\n",
      "==== CV loss, score are: 0.3592084593713223 0.8585291113989832 =======\n",
      "Running fold 5\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_13  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15664/15664 [==============================] - 1s 33us/step\n",
      "Before training loss, score are: 1.305587020909092 0.287985188968335\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/150\n",
      "15664/15664 [==============================] - 1s 60us/step - loss: 1.1599 - acc: 0.3168 - val_loss: 1.0961 - val_acc: 0.3898\n",
      "Epoch 2/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 1.0843 - acc: 0.4070 - val_loss: 1.0592 - val_acc: 0.4506\n",
      "Epoch 3/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 1.0468 - acc: 0.4595 - val_loss: 1.0265 - val_acc: 0.5027\n",
      "Epoch 4/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 1.0107 - acc: 0.5057 - val_loss: 0.9931 - val_acc: 0.5423\n",
      "Epoch 5/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.9746 - acc: 0.5522 - val_loss: 0.9607 - val_acc: 0.5954\n",
      "Epoch 6/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.9379 - acc: 0.5999 - val_loss: 0.9271 - val_acc: 0.6238\n",
      "Epoch 7/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.9007 - acc: 0.6440 - val_loss: 0.8942 - val_acc: 0.6455\n",
      "Epoch 8/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.8635 - acc: 0.6710 - val_loss: 0.8631 - val_acc: 0.6746\n",
      "Epoch 9/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.8263 - acc: 0.6990 - val_loss: 0.8323 - val_acc: 0.6912\n",
      "Epoch 10/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.7900 - acc: 0.7191 - val_loss: 0.8032 - val_acc: 0.7055\n",
      "Epoch 11/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.7546 - acc: 0.7393 - val_loss: 0.7749 - val_acc: 0.7160\n",
      "Epoch 12/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.7203 - acc: 0.7562 - val_loss: 0.7494 - val_acc: 0.7290\n",
      "Epoch 13/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.6874 - acc: 0.7703 - val_loss: 0.7242 - val_acc: 0.7367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.6558 - acc: 0.7866 - val_loss: 0.7016 - val_acc: 0.7436\n",
      "Epoch 15/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.6257 - acc: 0.7993 - val_loss: 0.6792 - val_acc: 0.7533\n",
      "Epoch 16/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.5966 - acc: 0.8128 - val_loss: 0.6590 - val_acc: 0.7596\n",
      "Epoch 17/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.5695 - acc: 0.8228 - val_loss: 0.6410 - val_acc: 0.7699\n",
      "Epoch 18/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.5429 - acc: 0.8325 - val_loss: 0.6237 - val_acc: 0.7752\n",
      "Epoch 19/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.5177 - acc: 0.8442 - val_loss: 0.6059 - val_acc: 0.7808\n",
      "Epoch 20/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.4939 - acc: 0.8528 - val_loss: 0.5906 - val_acc: 0.7844\n",
      "Epoch 21/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.4710 - acc: 0.8625 - val_loss: 0.5757 - val_acc: 0.7888\n",
      "Epoch 22/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.4493 - acc: 0.8697 - val_loss: 0.5616 - val_acc: 0.7923\n",
      "Epoch 23/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.4288 - acc: 0.8784 - val_loss: 0.5498 - val_acc: 0.7997\n",
      "Epoch 24/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.4088 - acc: 0.8855 - val_loss: 0.5366 - val_acc: 0.8026\n",
      "Epoch 25/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.3900 - acc: 0.8931 - val_loss: 0.5259 - val_acc: 0.8059\n",
      "Epoch 26/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.3723 - acc: 0.8987 - val_loss: 0.5161 - val_acc: 0.8146\n",
      "Epoch 27/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.3548 - acc: 0.9056 - val_loss: 0.5052 - val_acc: 0.8181\n",
      "Epoch 28/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.3386 - acc: 0.9129 - val_loss: 0.4950 - val_acc: 0.8202\n",
      "Epoch 29/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.3229 - acc: 0.9189 - val_loss: 0.4870 - val_acc: 0.8230\n",
      "Epoch 30/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.3083 - acc: 0.9247 - val_loss: 0.4768 - val_acc: 0.8250\n",
      "Epoch 31/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.2942 - acc: 0.9284 - val_loss: 0.4702 - val_acc: 0.8263\n",
      "Epoch 32/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.2808 - acc: 0.9334 - val_loss: 0.4636 - val_acc: 0.8286\n",
      "Epoch 33/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.2680 - acc: 0.9376 - val_loss: 0.4540 - val_acc: 0.8314\n",
      "Epoch 34/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.2559 - acc: 0.9418 - val_loss: 0.4481 - val_acc: 0.8335\n",
      "Epoch 35/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.2441 - acc: 0.9462 - val_loss: 0.4410 - val_acc: 0.8342\n",
      "Epoch 36/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.2329 - acc: 0.9497 - val_loss: 0.4349 - val_acc: 0.8368\n",
      "Epoch 37/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.2224 - acc: 0.9523 - val_loss: 0.4295 - val_acc: 0.8388\n",
      "Epoch 38/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.2124 - acc: 0.9556 - val_loss: 0.4235 - val_acc: 0.8406\n",
      "Epoch 39/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.2029 - acc: 0.9584 - val_loss: 0.4197 - val_acc: 0.8427\n",
      "Epoch 40/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1936 - acc: 0.9611 - val_loss: 0.4134 - val_acc: 0.8450\n",
      "Epoch 41/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.1848 - acc: 0.9636 - val_loss: 0.4091 - val_acc: 0.8450\n",
      "Epoch 42/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.1767 - acc: 0.9656 - val_loss: 0.4046 - val_acc: 0.8465\n",
      "Epoch 43/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.1686 - acc: 0.9676 - val_loss: 0.4009 - val_acc: 0.8465\n",
      "Epoch 44/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.1610 - acc: 0.9692 - val_loss: 0.3992 - val_acc: 0.8490\n",
      "Epoch 45/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1538 - acc: 0.9713 - val_loss: 0.3936 - val_acc: 0.8496\n",
      "Epoch 46/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.1470 - acc: 0.9727 - val_loss: 0.3909 - val_acc: 0.8511\n",
      "Epoch 47/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1402 - acc: 0.9748 - val_loss: 0.3883 - val_acc: 0.8511\n",
      "Epoch 48/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1341 - acc: 0.9756 - val_loss: 0.3847 - val_acc: 0.8508\n",
      "Epoch 49/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.1281 - acc: 0.9778 - val_loss: 0.3821 - val_acc: 0.8529\n",
      "Epoch 50/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.1224 - acc: 0.9794 - val_loss: 0.3794 - val_acc: 0.8536\n",
      "Epoch 51/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.1168 - acc: 0.9802 - val_loss: 0.3767 - val_acc: 0.8570\n",
      "Epoch 52/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.1117 - acc: 0.9814 - val_loss: 0.3750 - val_acc: 0.8544\n",
      "Epoch 53/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.1067 - acc: 0.9826 - val_loss: 0.3718 - val_acc: 0.8559\n",
      "Epoch 54/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.1019 - acc: 0.9833 - val_loss: 0.3707 - val_acc: 0.8549\n",
      "Epoch 55/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0974 - acc: 0.9841 - val_loss: 0.3701 - val_acc: 0.8554\n",
      "Epoch 56/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0931 - acc: 0.9848 - val_loss: 0.3674 - val_acc: 0.8562\n",
      "Epoch 57/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.0891 - acc: 0.9860 - val_loss: 0.3650 - val_acc: 0.8564\n",
      "Epoch 58/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0851 - acc: 0.9870 - val_loss: 0.3639 - val_acc: 0.8570\n",
      "Epoch 59/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0812 - acc: 0.9877 - val_loss: 0.3631 - val_acc: 0.8559\n",
      "Epoch 60/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0777 - acc: 0.9887 - val_loss: 0.3614 - val_acc: 0.8559\n",
      "Epoch 61/150\n",
      "15664/15664 [==============================] - 1s 45us/step - loss: 0.0743 - acc: 0.9888 - val_loss: 0.3603 - val_acc: 0.8564\n",
      "Epoch 62/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.0710 - acc: 0.9896 - val_loss: 0.3604 - val_acc: 0.8564\n",
      "Epoch 63/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0678 - acc: 0.9903 - val_loss: 0.3585 - val_acc: 0.8562\n",
      "Epoch 64/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0648 - acc: 0.9910 - val_loss: 0.3579 - val_acc: 0.8577\n",
      "Epoch 65/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.0621 - acc: 0.9916 - val_loss: 0.3582 - val_acc: 0.8590\n",
      "Epoch 66/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0594 - acc: 0.9918 - val_loss: 0.3575 - val_acc: 0.8575\n",
      "Epoch 67/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0567 - acc: 0.9926 - val_loss: 0.3565 - val_acc: 0.8582\n",
      "Epoch 68/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0543 - acc: 0.9928 - val_loss: 0.3562 - val_acc: 0.8593\n",
      "Epoch 69/150\n",
      "15664/15664 [==============================] - 1s 46us/step - loss: 0.0520 - acc: 0.9930 - val_loss: 0.3564 - val_acc: 0.8577\n",
      "Epoch 70/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.0495 - acc: 0.9934 - val_loss: 0.3583 - val_acc: 0.8585\n",
      "Epoch 71/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0474 - acc: 0.9938 - val_loss: 0.3566 - val_acc: 0.8587\n",
      "15664/15664 [==============================] - 0s 22us/step\n",
      "==== Training loss, score are: 0.04577144218058886 0.994573544433095 =======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3915/3915 [==============================] - 0s 22us/step\n",
      "==== CV loss, score are: 0.35661753699240556 0.8587484035607651 =======\n",
      "\n",
      "\n",
      "===== Model: fast_text_gensim  ========:\n",
      " Cross-val log losses are: [0.38271956085864439, 0.378870709472864, 0.35414783893485785, 0.35920845544401081, 0.35661753207869984]\n",
      "====== Mean cross-val log loss is: 0.3663128193578154 =========\n",
      "\n",
      "\n",
      "Timestamp: 2018-Jan-14 02:59:52\n",
      "Running kfold training with model fast_text_glove\n",
      "Shapes: x_train_raw.shape (19579, 24), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 23)\n",
      "Running preprocess: <function run_NN_preprocess at 0x7f620ee8eea0>\n",
      "No of unique words found is 247346\n",
      "No of words occuring more than 2 times is 75104\n",
      "No of unique words found is after capping is 247346\n",
      "max sentence length is 1439\n",
      "mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000\n",
      "After padding: max sentence size is 161\n",
      "After padding: mean, median length of sentence is 161.000000 161.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5007it [00:00, 50060.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word embedding matrix for glove vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:08, 49133.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "of--mirth 247345\n",
      "None\n",
      "(247346, 100)\n",
      "Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)\n",
      "Running fold 1\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_14  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 24,734,903\n",
      "Trainable params: 303\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 37us/step\n",
      "Before training loss, score are: 1.0987276865864732 0.31181765946172585\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 0s 31us/step - loss: 1.0848 - acc: 0.3996 - val_loss: 1.0793 - val_acc: 0.3907\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0713 - acc: 0.4118 - val_loss: 1.0675 - val_acc: 0.4002\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0599 - acc: 0.4299 - val_loss: 1.0573 - val_acc: 0.4132\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0496 - acc: 0.4449 - val_loss: 1.0475 - val_acc: 0.4374\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0403 - acc: 0.4615 - val_loss: 1.0386 - val_acc: 0.4535\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0318 - acc: 0.4744 - val_loss: 1.0307 - val_acc: 0.4630\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0238 - acc: 0.4823 - val_loss: 1.0224 - val_acc: 0.5010\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0166 - acc: 0.5004 - val_loss: 1.0157 - val_acc: 0.4977\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0098 - acc: 0.5074 - val_loss: 1.0094 - val_acc: 0.5056\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0032 - acc: 0.5145 - val_loss: 1.0028 - val_acc: 0.5207\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9971 - acc: 0.5212 - val_loss: 0.9968 - val_acc: 0.5217\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9914 - acc: 0.5286 - val_loss: 0.9912 - val_acc: 0.5309\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9858 - acc: 0.5380 - val_loss: 0.9866 - val_acc: 0.5299\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9806 - acc: 0.5408 - val_loss: 0.9810 - val_acc: 0.5396\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9757 - acc: 0.5433 - val_loss: 0.9760 - val_acc: 0.5434\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9707 - acc: 0.5528 - val_loss: 0.9724 - val_acc: 0.5434\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9664 - acc: 0.5536 - val_loss: 0.9668 - val_acc: 0.5554\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9621 - acc: 0.5593 - val_loss: 0.9629 - val_acc: 0.5580\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9578 - acc: 0.5617 - val_loss: 0.9586 - val_acc: 0.5626\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9538 - acc: 0.5667 - val_loss: 0.9548 - val_acc: 0.5623\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9502 - acc: 0.5687 - val_loss: 0.9511 - val_acc: 0.5666\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 0s 20us/step - loss: 0.9464 - acc: 0.5728 - val_loss: 0.9475 - val_acc: 0.5735\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.9429 - acc: 0.5740 - val_loss: 0.9437 - val_acc: 0.5753\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9395 - acc: 0.5794 - val_loss: 0.9409 - val_acc: 0.5723\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9362 - acc: 0.5799 - val_loss: 0.9373 - val_acc: 0.5751\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9331 - acc: 0.5825 - val_loss: 0.9343 - val_acc: 0.5809\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9300 - acc: 0.5863 - val_loss: 0.9313 - val_acc: 0.5771\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9270 - acc: 0.5879 - val_loss: 0.9285 - val_acc: 0.5840\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9242 - acc: 0.5896 - val_loss: 0.9254 - val_acc: 0.5858\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9214 - acc: 0.5932 - val_loss: 0.9232 - val_acc: 0.5863\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9187 - acc: 0.5928 - val_loss: 0.9209 - val_acc: 0.5909\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9162 - acc: 0.5939 - val_loss: 0.9179 - val_acc: 0.5899\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9136 - acc: 0.5965 - val_loss: 0.9150 - val_acc: 0.5876\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9113 - acc: 0.5986 - val_loss: 0.9128 - val_acc: 0.5904\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9089 - acc: 0.5991 - val_loss: 0.9103 - val_acc: 0.5965\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9067 - acc: 0.6003 - val_loss: 0.9080 - val_acc: 0.5968\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9044 - acc: 0.6024 - val_loss: 0.9063 - val_acc: 0.5940\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9023 - acc: 0.6030 - val_loss: 0.9042 - val_acc: 0.6004\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9002 - acc: 0.6031 - val_loss: 0.9019 - val_acc: 0.5960\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8982 - acc: 0.6061 - val_loss: 0.8998 - val_acc: 0.6014\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8962 - acc: 0.6071 - val_loss: 0.8978 - val_acc: 0.6032\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8943 - acc: 0.6074 - val_loss: 0.8960 - val_acc: 0.6044\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8924 - acc: 0.6092 - val_loss: 0.8941 - val_acc: 0.6019\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8905 - acc: 0.6099 - val_loss: 0.8926 - val_acc: 0.6052\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8887 - acc: 0.6102 - val_loss: 0.8907 - val_acc: 0.6060\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8870 - acc: 0.6118 - val_loss: 0.8893 - val_acc: 0.6062\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8853 - acc: 0.6116 - val_loss: 0.8876 - val_acc: 0.6113\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8837 - acc: 0.6132 - val_loss: 0.8857 - val_acc: 0.6118\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8820 - acc: 0.6144 - val_loss: 0.8839 - val_acc: 0.6144\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8805 - acc: 0.6158 - val_loss: 0.8827 - val_acc: 0.6126\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8790 - acc: 0.6157 - val_loss: 0.8810 - val_acc: 0.6147\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8774 - acc: 0.6171 - val_loss: 0.8793 - val_acc: 0.6159\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8761 - acc: 0.6179 - val_loss: 0.8782 - val_acc: 0.6164\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8746 - acc: 0.6180 - val_loss: 0.8767 - val_acc: 0.6177\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8731 - acc: 0.6195 - val_loss: 0.8754 - val_acc: 0.6170\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8718 - acc: 0.6195 - val_loss: 0.8738 - val_acc: 0.6175\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8706 - acc: 0.6218 - val_loss: 0.8727 - val_acc: 0.6185\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8692 - acc: 0.6215 - val_loss: 0.8712 - val_acc: 0.6198\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8679 - acc: 0.6213 - val_loss: 0.8701 - val_acc: 0.6208\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8666 - acc: 0.6222 - val_loss: 0.8693 - val_acc: 0.6190\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8656 - acc: 0.6217 - val_loss: 0.8677 - val_acc: 0.6218\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8644 - acc: 0.6240 - val_loss: 0.8663 - val_acc: 0.6182\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8632 - acc: 0.6241 - val_loss: 0.8657 - val_acc: 0.6231\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8620 - acc: 0.6257 - val_loss: 0.8644 - val_acc: 0.6223\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8608 - acc: 0.6253 - val_loss: 0.8640 - val_acc: 0.6231\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8598 - acc: 0.6249 - val_loss: 0.8621 - val_acc: 0.6226\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8587 - acc: 0.6263 - val_loss: 0.8611 - val_acc: 0.6254\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8576 - acc: 0.6269 - val_loss: 0.8603 - val_acc: 0.6221\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8566 - acc: 0.6268 - val_loss: 0.8596 - val_acc: 0.6264\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8557 - acc: 0.6283 - val_loss: 0.8578 - val_acc: 0.6259\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8546 - acc: 0.6273 - val_loss: 0.8569 - val_acc: 0.6264\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8536 - acc: 0.6289 - val_loss: 0.8563 - val_acc: 0.6264\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8526 - acc: 0.6291 - val_loss: 0.8550 - val_acc: 0.6302\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8518 - acc: 0.6282 - val_loss: 0.8547 - val_acc: 0.6295\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8509 - acc: 0.6293 - val_loss: 0.8541 - val_acc: 0.6279\n",
      "Epoch 76/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8500 - acc: 0.6286 - val_loss: 0.8526 - val_acc: 0.6315\n",
      "Epoch 77/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8490 - acc: 0.6315 - val_loss: 0.8528 - val_acc: 0.6292\n",
      "Epoch 78/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8483 - acc: 0.6300 - val_loss: 0.8511 - val_acc: 0.6313\n",
      "Epoch 79/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8474 - acc: 0.6302 - val_loss: 0.8505 - val_acc: 0.6279\n",
      "Epoch 80/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8466 - acc: 0.6307 - val_loss: 0.8495 - val_acc: 0.6310\n",
      "Epoch 81/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8458 - acc: 0.6319 - val_loss: 0.8485 - val_acc: 0.6336\n",
      "Epoch 82/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8450 - acc: 0.6320 - val_loss: 0.8479 - val_acc: 0.6346\n",
      "Epoch 83/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8442 - acc: 0.6319 - val_loss: 0.8469 - val_acc: 0.6361\n",
      "Epoch 84/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8434 - acc: 0.6327 - val_loss: 0.8464 - val_acc: 0.6318\n",
      "Epoch 85/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8426 - acc: 0.6318 - val_loss: 0.8461 - val_acc: 0.6315\n",
      "Epoch 86/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8420 - acc: 0.6329 - val_loss: 0.8450 - val_acc: 0.6361\n",
      "Epoch 87/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8411 - acc: 0.6322 - val_loss: 0.8439 - val_acc: 0.6369\n",
      "Epoch 88/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8403 - acc: 0.6326 - val_loss: 0.8435 - val_acc: 0.6356\n",
      "Epoch 89/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8397 - acc: 0.6339 - val_loss: 0.8427 - val_acc: 0.6376\n",
      "Epoch 90/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8392 - acc: 0.6326 - val_loss: 0.8419 - val_acc: 0.6376\n",
      "Epoch 91/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8384 - acc: 0.6334 - val_loss: 0.8414 - val_acc: 0.6387\n",
      "Epoch 92/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8376 - acc: 0.6334 - val_loss: 0.8406 - val_acc: 0.6374\n",
      "Epoch 93/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8370 - acc: 0.6353 - val_loss: 0.8401 - val_acc: 0.6389\n",
      "Epoch 94/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8363 - acc: 0.6352 - val_loss: 0.8396 - val_acc: 0.6404\n",
      "Epoch 95/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8357 - acc: 0.6354 - val_loss: 0.8385 - val_acc: 0.6374\n",
      "Epoch 96/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8350 - acc: 0.6365 - val_loss: 0.8379 - val_acc: 0.6404\n",
      "Epoch 97/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8344 - acc: 0.6365 - val_loss: 0.8373 - val_acc: 0.6407\n",
      "Epoch 98/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8338 - acc: 0.6377 - val_loss: 0.8371 - val_acc: 0.6402\n",
      "Epoch 99/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8331 - acc: 0.6369 - val_loss: 0.8366 - val_acc: 0.6422\n",
      "Epoch 100/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8327 - acc: 0.6370 - val_loss: 0.8358 - val_acc: 0.6417\n",
      "Epoch 101/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8319 - acc: 0.6371 - val_loss: 0.8350 - val_acc: 0.6399\n",
      "Epoch 102/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8315 - acc: 0.6376 - val_loss: 0.8347 - val_acc: 0.6402\n",
      "Epoch 103/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8308 - acc: 0.6381 - val_loss: 0.8343 - val_acc: 0.6415\n",
      "Epoch 104/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8303 - acc: 0.6380 - val_loss: 0.8335 - val_acc: 0.6399\n",
      "Epoch 105/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8297 - acc: 0.6391 - val_loss: 0.8330 - val_acc: 0.6397\n",
      "Epoch 106/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8292 - acc: 0.6393 - val_loss: 0.8325 - val_acc: 0.6417\n",
      "Epoch 107/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8287 - acc: 0.6391 - val_loss: 0.8319 - val_acc: 0.6435\n",
      "Epoch 108/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8280 - acc: 0.6407 - val_loss: 0.8316 - val_acc: 0.6438\n",
      "Epoch 109/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8275 - acc: 0.6404 - val_loss: 0.8312 - val_acc: 0.6438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8270 - acc: 0.6397 - val_loss: 0.8306 - val_acc: 0.6438\n",
      "Epoch 111/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8265 - acc: 0.6400 - val_loss: 0.8299 - val_acc: 0.6461\n",
      "Epoch 112/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8260 - acc: 0.6412 - val_loss: 0.8297 - val_acc: 0.6448\n",
      "Epoch 113/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8256 - acc: 0.6407 - val_loss: 0.8293 - val_acc: 0.6461\n",
      "Epoch 114/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8250 - acc: 0.6409 - val_loss: 0.8287 - val_acc: 0.6461\n",
      "Epoch 115/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8245 - acc: 0.6409 - val_loss: 0.8280 - val_acc: 0.6463\n",
      "Epoch 116/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8240 - acc: 0.6418 - val_loss: 0.8277 - val_acc: 0.6463\n",
      "Epoch 117/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8236 - acc: 0.6418 - val_loss: 0.8272 - val_acc: 0.6489\n",
      "Epoch 118/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8231 - acc: 0.6402 - val_loss: 0.8267 - val_acc: 0.6499\n",
      "Epoch 119/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8227 - acc: 0.6426 - val_loss: 0.8265 - val_acc: 0.6484\n",
      "Epoch 120/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8221 - acc: 0.6427 - val_loss: 0.8259 - val_acc: 0.6491\n",
      "Epoch 121/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8218 - acc: 0.6425 - val_loss: 0.8258 - val_acc: 0.6481\n",
      "Epoch 122/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8213 - acc: 0.6429 - val_loss: 0.8250 - val_acc: 0.6502\n",
      "Epoch 123/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8209 - acc: 0.6437 - val_loss: 0.8247 - val_acc: 0.6514\n",
      "Epoch 124/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8204 - acc: 0.6431 - val_loss: 0.8247 - val_acc: 0.6496\n",
      "Epoch 125/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8201 - acc: 0.6425 - val_loss: 0.8238 - val_acc: 0.6519\n",
      "Epoch 126/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8196 - acc: 0.6416 - val_loss: 0.8234 - val_acc: 0.6502\n",
      "Epoch 127/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8193 - acc: 0.6433 - val_loss: 0.8230 - val_acc: 0.6519\n",
      "Epoch 128/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8189 - acc: 0.6432 - val_loss: 0.8228 - val_acc: 0.6522\n",
      "Epoch 129/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8185 - acc: 0.6425 - val_loss: 0.8224 - val_acc: 0.6532\n",
      "Epoch 130/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8179 - acc: 0.6437 - val_loss: 0.8221 - val_acc: 0.6517\n",
      "Epoch 131/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8177 - acc: 0.6440 - val_loss: 0.8215 - val_acc: 0.6537\n",
      "Epoch 132/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8173 - acc: 0.6439 - val_loss: 0.8218 - val_acc: 0.6504\n",
      "Epoch 133/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8169 - acc: 0.6440 - val_loss: 0.8210 - val_acc: 0.6530\n",
      "Epoch 134/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8165 - acc: 0.6446 - val_loss: 0.8207 - val_acc: 0.6522\n",
      "Epoch 135/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8161 - acc: 0.6447 - val_loss: 0.8202 - val_acc: 0.6525\n",
      "Epoch 136/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8158 - acc: 0.6436 - val_loss: 0.8202 - val_acc: 0.6514\n",
      "Epoch 137/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8153 - acc: 0.6455 - val_loss: 0.8200 - val_acc: 0.6514\n",
      "Epoch 138/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8150 - acc: 0.6450 - val_loss: 0.8193 - val_acc: 0.6547\n",
      "Epoch 139/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8146 - acc: 0.6452 - val_loss: 0.8189 - val_acc: 0.6542\n",
      "Epoch 140/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8144 - acc: 0.6451 - val_loss: 0.8187 - val_acc: 0.6535\n",
      "Epoch 141/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8139 - acc: 0.6454 - val_loss: 0.8181 - val_acc: 0.6542\n",
      "Epoch 142/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8137 - acc: 0.6471 - val_loss: 0.8182 - val_acc: 0.6517\n",
      "Epoch 143/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8134 - acc: 0.6453 - val_loss: 0.8177 - val_acc: 0.6545\n",
      "Epoch 144/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8129 - acc: 0.6459 - val_loss: 0.8173 - val_acc: 0.6563\n",
      "Epoch 145/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8126 - acc: 0.6467 - val_loss: 0.8173 - val_acc: 0.6542\n",
      "Epoch 146/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8123 - acc: 0.6454 - val_loss: 0.8168 - val_acc: 0.6553\n",
      "Epoch 147/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8119 - acc: 0.6466 - val_loss: 0.8166 - val_acc: 0.6553\n",
      "Epoch 148/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8117 - acc: 0.6460 - val_loss: 0.8162 - val_acc: 0.6553\n",
      "Epoch 149/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8112 - acc: 0.6469 - val_loss: 0.8164 - val_acc: 0.6530\n",
      "Epoch 150/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8110 - acc: 0.6469 - val_loss: 0.8155 - val_acc: 0.6555\n",
      "15663/15663 [==============================] - 0s 22us/step\n",
      "==== Training loss, score are: 0.8106370004461101 0.6479601609153567 =======\n",
      "3916/3916 [==============================] - 0s 21us/step\n",
      "==== CV loss, score are: 0.8155102588548359 0.6555158325430078 =======\n",
      "Running fold 2\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_15  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 24,734,903\n",
      "Trainable params: 303\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 35us/step\n",
      "Before training loss, score are: 1.1002171779283199 0.3076039073003067\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 0s 30us/step - loss: 1.0859 - acc: 0.3962 - val_loss: 1.0800 - val_acc: 0.3943\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0720 - acc: 0.4082 - val_loss: 1.0686 - val_acc: 0.4032\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0604 - acc: 0.4216 - val_loss: 1.0583 - val_acc: 0.4446\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0501 - acc: 0.4483 - val_loss: 1.0489 - val_acc: 0.4538\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0405 - acc: 0.4604 - val_loss: 1.0403 - val_acc: 0.4706\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0319 - acc: 0.4742 - val_loss: 1.0326 - val_acc: 0.4714\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0239 - acc: 0.4919 - val_loss: 1.0252 - val_acc: 0.4816\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0164 - acc: 0.5004 - val_loss: 1.0182 - val_acc: 0.4944\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0096 - acc: 0.5125 - val_loss: 1.0117 - val_acc: 0.5010\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0030 - acc: 0.5178 - val_loss: 1.0056 - val_acc: 0.5064\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9968 - acc: 0.5299 - val_loss: 0.9999 - val_acc: 0.5115\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9910 - acc: 0.5326 - val_loss: 0.9945 - val_acc: 0.5199\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9856 - acc: 0.5394 - val_loss: 0.9893 - val_acc: 0.5250\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9804 - acc: 0.5448 - val_loss: 0.9842 - val_acc: 0.5419\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9754 - acc: 0.5490 - val_loss: 0.9797 - val_acc: 0.5429\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9706 - acc: 0.5550 - val_loss: 0.9751 - val_acc: 0.5465\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9660 - acc: 0.5546 - val_loss: 0.9707 - val_acc: 0.5526\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9618 - acc: 0.5623 - val_loss: 0.9665 - val_acc: 0.5575\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9575 - acc: 0.5642 - val_loss: 0.9631 - val_acc: 0.5557\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9535 - acc: 0.5680 - val_loss: 0.9586 - val_acc: 0.5656\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9497 - acc: 0.5726 - val_loss: 0.9552 - val_acc: 0.5664\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9458 - acc: 0.5741 - val_loss: 0.9520 - val_acc: 0.5697\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9425 - acc: 0.5775 - val_loss: 0.9482 - val_acc: 0.5697\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9389 - acc: 0.5799 - val_loss: 0.9455 - val_acc: 0.5687\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9356 - acc: 0.5832 - val_loss: 0.9420 - val_acc: 0.5733\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9325 - acc: 0.5853 - val_loss: 0.9387 - val_acc: 0.5812\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9294 - acc: 0.5888 - val_loss: 0.9358 - val_acc: 0.5815\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9263 - acc: 0.5891 - val_loss: 0.9329 - val_acc: 0.5822\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9235 - acc: 0.5918 - val_loss: 0.9305 - val_acc: 0.5820\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9207 - acc: 0.5924 - val_loss: 0.9274 - val_acc: 0.5827\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9182 - acc: 0.5962 - val_loss: 0.9250 - val_acc: 0.5838\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9154 - acc: 0.5962 - val_loss: 0.9226 - val_acc: 0.5876\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9129 - acc: 0.5970 - val_loss: 0.9200 - val_acc: 0.5884\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9105 - acc: 0.6025 - val_loss: 0.9178 - val_acc: 0.5909\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9080 - acc: 0.6028 - val_loss: 0.9152 - val_acc: 0.5947\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9058 - acc: 0.6038 - val_loss: 0.9131 - val_acc: 0.5942\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9035 - acc: 0.6052 - val_loss: 0.9114 - val_acc: 0.5914\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9014 - acc: 0.6074 - val_loss: 0.9091 - val_acc: 0.5958\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8993 - acc: 0.6071 - val_loss: 0.9068 - val_acc: 0.5998\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8972 - acc: 0.6088 - val_loss: 0.9052 - val_acc: 0.5983\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8953 - acc: 0.6102 - val_loss: 0.9034 - val_acc: 0.5993\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8933 - acc: 0.6105 - val_loss: 0.9015 - val_acc: 0.6027\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8914 - acc: 0.6137 - val_loss: 0.8997 - val_acc: 0.6037\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8896 - acc: 0.6135 - val_loss: 0.8980 - val_acc: 0.6037\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8878 - acc: 0.6134 - val_loss: 0.8967 - val_acc: 0.6004\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8860 - acc: 0.6147 - val_loss: 0.8944 - val_acc: 0.6078\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8845 - acc: 0.6174 - val_loss: 0.8931 - val_acc: 0.6067\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8827 - acc: 0.6157 - val_loss: 0.8915 - val_acc: 0.6083\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8811 - acc: 0.6161 - val_loss: 0.8900 - val_acc: 0.6113\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8795 - acc: 0.6197 - val_loss: 0.8885 - val_acc: 0.6090\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8779 - acc: 0.6181 - val_loss: 0.8868 - val_acc: 0.6103\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8765 - acc: 0.6197 - val_loss: 0.8852 - val_acc: 0.6124\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8750 - acc: 0.6203 - val_loss: 0.8839 - val_acc: 0.6118\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8735 - acc: 0.6220 - val_loss: 0.8827 - val_acc: 0.6139\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8722 - acc: 0.6199 - val_loss: 0.8815 - val_acc: 0.6129\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8707 - acc: 0.6216 - val_loss: 0.8799 - val_acc: 0.6162\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8694 - acc: 0.6227 - val_loss: 0.8789 - val_acc: 0.6152\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8681 - acc: 0.6224 - val_loss: 0.8776 - val_acc: 0.6175\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8667 - acc: 0.6236 - val_loss: 0.8769 - val_acc: 0.6129\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8655 - acc: 0.6251 - val_loss: 0.8753 - val_acc: 0.6177\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8644 - acc: 0.6238 - val_loss: 0.8737 - val_acc: 0.6149\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8632 - acc: 0.6260 - val_loss: 0.8730 - val_acc: 0.6162\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8620 - acc: 0.6261 - val_loss: 0.8716 - val_acc: 0.6177\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8607 - acc: 0.6255 - val_loss: 0.8705 - val_acc: 0.6187\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8597 - acc: 0.6273 - val_loss: 0.8696 - val_acc: 0.6200\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8584 - acc: 0.6260 - val_loss: 0.8685 - val_acc: 0.6157\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8574 - acc: 0.6277 - val_loss: 0.8676 - val_acc: 0.6170\n",
      "Epoch 68/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8564 - acc: 0.6291 - val_loss: 0.8667 - val_acc: 0.6193\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8553 - acc: 0.6283 - val_loss: 0.8662 - val_acc: 0.6177\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8543 - acc: 0.6287 - val_loss: 0.8646 - val_acc: 0.6162\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8533 - acc: 0.6298 - val_loss: 0.8634 - val_acc: 0.6198\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8523 - acc: 0.6295 - val_loss: 0.8625 - val_acc: 0.6203\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8512 - acc: 0.6319 - val_loss: 0.8628 - val_acc: 0.6185\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8504 - acc: 0.6302 - val_loss: 0.8616 - val_acc: 0.6195\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8494 - acc: 0.6319 - val_loss: 0.8600 - val_acc: 0.6216\n",
      "Epoch 76/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8485 - acc: 0.6323 - val_loss: 0.8590 - val_acc: 0.6221\n",
      "Epoch 77/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8476 - acc: 0.6323 - val_loss: 0.8582 - val_acc: 0.6213\n",
      "Epoch 78/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8468 - acc: 0.6336 - val_loss: 0.8578 - val_acc: 0.6216\n",
      "Epoch 79/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8459 - acc: 0.6337 - val_loss: 0.8566 - val_acc: 0.6239\n",
      "Epoch 80/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8451 - acc: 0.6342 - val_loss: 0.8559 - val_acc: 0.6244\n",
      "Epoch 81/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8443 - acc: 0.6353 - val_loss: 0.8554 - val_acc: 0.6233\n",
      "Epoch 82/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8434 - acc: 0.6357 - val_loss: 0.8549 - val_acc: 0.6226\n",
      "Epoch 83/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8428 - acc: 0.6346 - val_loss: 0.8540 - val_acc: 0.6231\n",
      "Epoch 84/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8418 - acc: 0.6358 - val_loss: 0.8530 - val_acc: 0.6239\n",
      "Epoch 85/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8410 - acc: 0.6364 - val_loss: 0.8524 - val_acc: 0.6236\n",
      "Epoch 86/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8403 - acc: 0.6372 - val_loss: 0.8519 - val_acc: 0.6236\n",
      "Epoch 87/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8397 - acc: 0.6353 - val_loss: 0.8506 - val_acc: 0.6251\n",
      "Epoch 88/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8388 - acc: 0.6384 - val_loss: 0.8503 - val_acc: 0.6241\n",
      "Epoch 89/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8381 - acc: 0.6370 - val_loss: 0.8495 - val_acc: 0.6241\n",
      "Epoch 90/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8374 - acc: 0.6389 - val_loss: 0.8488 - val_acc: 0.6249\n",
      "Epoch 91/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8366 - acc: 0.6391 - val_loss: 0.8487 - val_acc: 0.6246\n",
      "Epoch 92/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8360 - acc: 0.6382 - val_loss: 0.8474 - val_acc: 0.6249\n",
      "Epoch 93/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8353 - acc: 0.6391 - val_loss: 0.8475 - val_acc: 0.6254\n",
      "Epoch 94/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8347 - acc: 0.6400 - val_loss: 0.8462 - val_acc: 0.6261\n",
      "Epoch 95/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8340 - acc: 0.6399 - val_loss: 0.8459 - val_acc: 0.6249\n",
      "Epoch 96/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8335 - acc: 0.6400 - val_loss: 0.8450 - val_acc: 0.6259\n",
      "Epoch 97/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8328 - acc: 0.6404 - val_loss: 0.8443 - val_acc: 0.6249\n",
      "Epoch 98/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8321 - acc: 0.6416 - val_loss: 0.8445 - val_acc: 0.6228\n",
      "Epoch 99/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8317 - acc: 0.6411 - val_loss: 0.8433 - val_acc: 0.6264\n",
      "Epoch 100/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8311 - acc: 0.6409 - val_loss: 0.8428 - val_acc: 0.6264\n",
      "Epoch 101/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8303 - acc: 0.6403 - val_loss: 0.8426 - val_acc: 0.6267\n",
      "Epoch 102/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8297 - acc: 0.6412 - val_loss: 0.8416 - val_acc: 0.6259\n",
      "Epoch 103/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8293 - acc: 0.6418 - val_loss: 0.8416 - val_acc: 0.6249\n",
      "Epoch 104/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8287 - acc: 0.6411 - val_loss: 0.8409 - val_acc: 0.6267\n",
      "Epoch 105/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8284 - acc: 0.6425 - val_loss: 0.8405 - val_acc: 0.6279\n",
      "Epoch 106/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8276 - acc: 0.6416 - val_loss: 0.8396 - val_acc: 0.6277\n",
      "Epoch 107/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8270 - acc: 0.6427 - val_loss: 0.8392 - val_acc: 0.6274\n",
      "Epoch 108/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8266 - acc: 0.6430 - val_loss: 0.8389 - val_acc: 0.6267\n",
      "Epoch 109/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8259 - acc: 0.6437 - val_loss: 0.8385 - val_acc: 0.6259\n",
      "Epoch 110/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8254 - acc: 0.6434 - val_loss: 0.8378 - val_acc: 0.6264\n",
      "Epoch 111/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8250 - acc: 0.6431 - val_loss: 0.8372 - val_acc: 0.6279\n",
      "Epoch 112/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8244 - acc: 0.6436 - val_loss: 0.8367 - val_acc: 0.6272\n",
      "Epoch 113/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8240 - acc: 0.6441 - val_loss: 0.8364 - val_acc: 0.6277\n",
      "Epoch 114/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8234 - acc: 0.6446 - val_loss: 0.8358 - val_acc: 0.6277\n",
      "Epoch 115/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8230 - acc: 0.6450 - val_loss: 0.8357 - val_acc: 0.6277\n",
      "Epoch 116/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8225 - acc: 0.6447 - val_loss: 0.8350 - val_acc: 0.6267\n",
      "Epoch 117/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8220 - acc: 0.6453 - val_loss: 0.8347 - val_acc: 0.6267\n",
      "Epoch 118/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8216 - acc: 0.6459 - val_loss: 0.8350 - val_acc: 0.6272\n",
      "Epoch 119/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8211 - acc: 0.6468 - val_loss: 0.8337 - val_acc: 0.6287\n",
      "Epoch 120/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8206 - acc: 0.6467 - val_loss: 0.8336 - val_acc: 0.6269\n",
      "Epoch 121/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8202 - acc: 0.6460 - val_loss: 0.8330 - val_acc: 0.6292\n",
      "Epoch 122/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8199 - acc: 0.6461 - val_loss: 0.8325 - val_acc: 0.6290\n",
      "Epoch 123/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8193 - acc: 0.6463 - val_loss: 0.8321 - val_acc: 0.6300\n",
      "Epoch 124/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8189 - acc: 0.6476 - val_loss: 0.8318 - val_acc: 0.6284\n",
      "Epoch 125/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8184 - acc: 0.6468 - val_loss: 0.8322 - val_acc: 0.6264\n",
      "Epoch 126/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8181 - acc: 0.6466 - val_loss: 0.8310 - val_acc: 0.6287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8177 - acc: 0.6469 - val_loss: 0.8314 - val_acc: 0.6282\n",
      "Epoch 128/150\n",
      "15663/15663 [==============================] - 0s 18us/step - loss: 0.8173 - acc: 0.6480 - val_loss: 0.8304 - val_acc: 0.6295\n",
      "Epoch 129/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8168 - acc: 0.6476 - val_loss: 0.8300 - val_acc: 0.6310\n",
      "Epoch 130/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8164 - acc: 0.6477 - val_loss: 0.8295 - val_acc: 0.6323\n",
      "Epoch 131/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8160 - acc: 0.6485 - val_loss: 0.8302 - val_acc: 0.6284\n",
      "Epoch 132/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8157 - acc: 0.6484 - val_loss: 0.8289 - val_acc: 0.6302\n",
      "Epoch 133/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8153 - acc: 0.6462 - val_loss: 0.8283 - val_acc: 0.6315\n",
      "Epoch 134/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8151 - acc: 0.6482 - val_loss: 0.8281 - val_acc: 0.6310\n",
      "Epoch 135/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8146 - acc: 0.6482 - val_loss: 0.8278 - val_acc: 0.6313\n",
      "Epoch 136/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8144 - acc: 0.6492 - val_loss: 0.8273 - val_acc: 0.6325\n",
      "Epoch 137/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8138 - acc: 0.6494 - val_loss: 0.8272 - val_acc: 0.6300\n",
      "Epoch 138/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8134 - acc: 0.6497 - val_loss: 0.8269 - val_acc: 0.6313\n",
      "Epoch 139/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8130 - acc: 0.6497 - val_loss: 0.8267 - val_acc: 0.6336\n",
      "Epoch 140/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8127 - acc: 0.6495 - val_loss: 0.8266 - val_acc: 0.6320\n",
      "Epoch 141/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8124 - acc: 0.6497 - val_loss: 0.8262 - val_acc: 0.6328\n",
      "Epoch 142/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8121 - acc: 0.6492 - val_loss: 0.8254 - val_acc: 0.6320\n",
      "Epoch 143/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8118 - acc: 0.6490 - val_loss: 0.8253 - val_acc: 0.6333\n",
      "Epoch 144/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8114 - acc: 0.6502 - val_loss: 0.8252 - val_acc: 0.6325\n",
      "Epoch 145/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8109 - acc: 0.6501 - val_loss: 0.8244 - val_acc: 0.6325\n",
      "Epoch 146/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8108 - acc: 0.6491 - val_loss: 0.8243 - val_acc: 0.6336\n",
      "Epoch 147/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8104 - acc: 0.6506 - val_loss: 0.8250 - val_acc: 0.6325\n",
      "Epoch 148/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8102 - acc: 0.6500 - val_loss: 0.8240 - val_acc: 0.6336\n",
      "Epoch 149/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8098 - acc: 0.6503 - val_loss: 0.8240 - val_acc: 0.6333\n",
      "Epoch 150/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8095 - acc: 0.6511 - val_loss: 0.8236 - val_acc: 0.6330\n",
      "15663/15663 [==============================] - 0s 21us/step\n",
      "==== Training loss, score are: 0.8092060357547237 0.6507693290147244 =======\n",
      "3916/3916 [==============================] - 0s 24us/step\n",
      "==== CV loss, score are: 0.823633152508273 0.633043922369765 =======\n",
      "Running fold 3\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_16  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 24,734,903\n",
      "Trainable params: 303\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 38us/step\n",
      "Before training loss, score are: 1.1061448133106715 0.30179403690891327\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 0s 32us/step - loss: 1.0897 - acc: 0.3903 - val_loss: 1.0821 - val_acc: 0.3999\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0754 - acc: 0.4069 - val_loss: 1.0705 - val_acc: 0.4099\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0641 - acc: 0.4226 - val_loss: 1.0600 - val_acc: 0.4208\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0537 - acc: 0.4382 - val_loss: 1.0504 - val_acc: 0.4456\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0444 - acc: 0.4521 - val_loss: 1.0415 - val_acc: 0.4630\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0358 - acc: 0.4687 - val_loss: 1.0333 - val_acc: 0.4831\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0278 - acc: 0.4839 - val_loss: 1.0258 - val_acc: 0.4852\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0203 - acc: 0.4948 - val_loss: 1.0189 - val_acc: 0.4900\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0133 - acc: 0.5025 - val_loss: 1.0118 - val_acc: 0.5003\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0067 - acc: 0.5143 - val_loss: 1.0055 - val_acc: 0.5105\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0004 - acc: 0.5196 - val_loss: 0.9995 - val_acc: 0.5197\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9944 - acc: 0.5263 - val_loss: 0.9940 - val_acc: 0.5245\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9889 - acc: 0.5344 - val_loss: 0.9884 - val_acc: 0.5281\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9836 - acc: 0.5406 - val_loss: 0.9833 - val_acc: 0.5383\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9784 - acc: 0.5491 - val_loss: 0.9785 - val_acc: 0.5322\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9736 - acc: 0.5526 - val_loss: 0.9739 - val_acc: 0.5345\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9690 - acc: 0.5547 - val_loss: 0.9693 - val_acc: 0.5575\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9646 - acc: 0.5593 - val_loss: 0.9649 - val_acc: 0.5501\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9602 - acc: 0.5643 - val_loss: 0.9608 - val_acc: 0.5531\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9563 - acc: 0.5667 - val_loss: 0.9568 - val_acc: 0.5649\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9524 - acc: 0.5720 - val_loss: 0.9531 - val_acc: 0.5605\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9485 - acc: 0.5732 - val_loss: 0.9497 - val_acc: 0.5595\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9448 - acc: 0.5762 - val_loss: 0.9460 - val_acc: 0.5738\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9415 - acc: 0.5801 - val_loss: 0.9425 - val_acc: 0.5725\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9382 - acc: 0.5822 - val_loss: 0.9393 - val_acc: 0.5730\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9348 - acc: 0.5860 - val_loss: 0.9361 - val_acc: 0.5733\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9316 - acc: 0.5911 - val_loss: 0.9336 - val_acc: 0.5712\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9287 - acc: 0.5878 - val_loss: 0.9303 - val_acc: 0.5766\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9259 - acc: 0.5938 - val_loss: 0.9274 - val_acc: 0.5774\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9229 - acc: 0.5938 - val_loss: 0.9248 - val_acc: 0.5822\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9204 - acc: 0.5959 - val_loss: 0.9221 - val_acc: 0.5817\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9175 - acc: 0.5992 - val_loss: 0.9196 - val_acc: 0.5820\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9150 - acc: 0.5985 - val_loss: 0.9171 - val_acc: 0.5845\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9125 - acc: 0.6009 - val_loss: 0.9149 - val_acc: 0.5827\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9101 - acc: 0.6020 - val_loss: 0.9126 - val_acc: 0.5878\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9077 - acc: 0.6060 - val_loss: 0.9103 - val_acc: 0.5881\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9055 - acc: 0.6053 - val_loss: 0.9081 - val_acc: 0.5899\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9033 - acc: 0.6056 - val_loss: 0.9059 - val_acc: 0.5930\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9012 - acc: 0.6094 - val_loss: 0.9038 - val_acc: 0.5932\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8990 - acc: 0.6109 - val_loss: 0.9019 - val_acc: 0.5914\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8970 - acc: 0.6109 - val_loss: 0.8999 - val_acc: 0.5958\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8950 - acc: 0.6130 - val_loss: 0.8982 - val_acc: 0.5960\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8932 - acc: 0.6130 - val_loss: 0.8963 - val_acc: 0.5963\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8913 - acc: 0.6138 - val_loss: 0.8945 - val_acc: 0.5960\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8893 - acc: 0.6153 - val_loss: 0.8928 - val_acc: 0.6004\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8876 - acc: 0.6160 - val_loss: 0.8911 - val_acc: 0.5988\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8858 - acc: 0.6161 - val_loss: 0.8894 - val_acc: 0.6009\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8842 - acc: 0.6187 - val_loss: 0.8879 - val_acc: 0.6006\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8825 - acc: 0.6179 - val_loss: 0.8863 - val_acc: 0.6039\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8810 - acc: 0.6202 - val_loss: 0.8849 - val_acc: 0.6021\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8793 - acc: 0.6214 - val_loss: 0.8833 - val_acc: 0.6067\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8778 - acc: 0.6206 - val_loss: 0.8818 - val_acc: 0.6039\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8763 - acc: 0.6218 - val_loss: 0.8805 - val_acc: 0.6047\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8749 - acc: 0.6217 - val_loss: 0.8792 - val_acc: 0.6042\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8734 - acc: 0.6218 - val_loss: 0.8778 - val_acc: 0.6055\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8722 - acc: 0.6230 - val_loss: 0.8765 - val_acc: 0.6065\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8707 - acc: 0.6247 - val_loss: 0.8752 - val_acc: 0.6093\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8693 - acc: 0.6257 - val_loss: 0.8740 - val_acc: 0.6078\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8681 - acc: 0.6241 - val_loss: 0.8727 - val_acc: 0.6083\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8667 - acc: 0.6254 - val_loss: 0.8717 - val_acc: 0.6093\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8655 - acc: 0.6258 - val_loss: 0.8704 - val_acc: 0.6106\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8643 - acc: 0.6263 - val_loss: 0.8693 - val_acc: 0.6111\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8631 - acc: 0.6272 - val_loss: 0.8686 - val_acc: 0.6096\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8618 - acc: 0.6271 - val_loss: 0.8671 - val_acc: 0.6134\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8608 - acc: 0.6296 - val_loss: 0.8662 - val_acc: 0.6103\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8597 - acc: 0.6290 - val_loss: 0.8652 - val_acc: 0.6111\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8585 - acc: 0.6286 - val_loss: 0.8640 - val_acc: 0.6136\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8575 - acc: 0.6295 - val_loss: 0.8630 - val_acc: 0.6154\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8564 - acc: 0.6309 - val_loss: 0.8620 - val_acc: 0.6154\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8555 - acc: 0.6310 - val_loss: 0.8610 - val_acc: 0.6141\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8544 - acc: 0.6328 - val_loss: 0.8601 - val_acc: 0.6167\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8534 - acc: 0.6308 - val_loss: 0.8593 - val_acc: 0.6177\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8524 - acc: 0.6327 - val_loss: 0.8585 - val_acc: 0.6164\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8515 - acc: 0.6329 - val_loss: 0.8574 - val_acc: 0.6175\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8506 - acc: 0.6326 - val_loss: 0.8566 - val_acc: 0.6187\n",
      "Epoch 76/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8496 - acc: 0.6340 - val_loss: 0.8557 - val_acc: 0.6177\n",
      "Epoch 77/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8487 - acc: 0.6335 - val_loss: 0.8553 - val_acc: 0.6159\n",
      "Epoch 78/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8478 - acc: 0.6338 - val_loss: 0.8541 - val_acc: 0.6180\n",
      "Epoch 79/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8469 - acc: 0.6346 - val_loss: 0.8533 - val_acc: 0.6200\n",
      "Epoch 80/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8460 - acc: 0.6354 - val_loss: 0.8526 - val_acc: 0.6208\n",
      "Epoch 81/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8453 - acc: 0.6367 - val_loss: 0.8518 - val_acc: 0.6187\n",
      "Epoch 82/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8444 - acc: 0.6368 - val_loss: 0.8510 - val_acc: 0.6210\n",
      "Epoch 83/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8437 - acc: 0.6372 - val_loss: 0.8505 - val_acc: 0.6190\n",
      "Epoch 84/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8428 - acc: 0.6370 - val_loss: 0.8500 - val_acc: 0.6177\n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8421 - acc: 0.6366 - val_loss: 0.8490 - val_acc: 0.6195\n",
      "Epoch 86/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8412 - acc: 0.6369 - val_loss: 0.8483 - val_acc: 0.6259\n",
      "Epoch 87/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8405 - acc: 0.6377 - val_loss: 0.8479 - val_acc: 0.6200\n",
      "Epoch 88/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8398 - acc: 0.6384 - val_loss: 0.8470 - val_acc: 0.6228\n",
      "Epoch 89/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8390 - acc: 0.6392 - val_loss: 0.8464 - val_acc: 0.6261\n",
      "Epoch 90/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8383 - acc: 0.6386 - val_loss: 0.8456 - val_acc: 0.6241\n",
      "Epoch 91/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8376 - acc: 0.6383 - val_loss: 0.8451 - val_acc: 0.6246\n",
      "Epoch 92/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8369 - acc: 0.6391 - val_loss: 0.8444 - val_acc: 0.6256\n",
      "Epoch 93/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8362 - acc: 0.6409 - val_loss: 0.8439 - val_acc: 0.6205\n",
      "Epoch 94/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8356 - acc: 0.6400 - val_loss: 0.8432 - val_acc: 0.6236\n",
      "Epoch 95/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8349 - acc: 0.6394 - val_loss: 0.8426 - val_acc: 0.6236\n",
      "Epoch 96/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8344 - acc: 0.6404 - val_loss: 0.8420 - val_acc: 0.6236\n",
      "Epoch 97/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8336 - acc: 0.6417 - val_loss: 0.8419 - val_acc: 0.6233\n",
      "Epoch 98/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8330 - acc: 0.6420 - val_loss: 0.8410 - val_acc: 0.6236\n",
      "Epoch 99/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8323 - acc: 0.6406 - val_loss: 0.8403 - val_acc: 0.6251\n",
      "Epoch 100/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8318 - acc: 0.6418 - val_loss: 0.8398 - val_acc: 0.6259\n",
      "Epoch 101/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8312 - acc: 0.6418 - val_loss: 0.8392 - val_acc: 0.6261\n",
      "Epoch 102/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8306 - acc: 0.6419 - val_loss: 0.8389 - val_acc: 0.6233\n",
      "Epoch 103/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8299 - acc: 0.6427 - val_loss: 0.8385 - val_acc: 0.6249\n",
      "Epoch 104/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8295 - acc: 0.6419 - val_loss: 0.8377 - val_acc: 0.6236\n",
      "Epoch 105/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8288 - acc: 0.6427 - val_loss: 0.8372 - val_acc: 0.6246\n",
      "Epoch 106/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8282 - acc: 0.6416 - val_loss: 0.8369 - val_acc: 0.6267\n",
      "Epoch 107/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8277 - acc: 0.6422 - val_loss: 0.8362 - val_acc: 0.6241\n",
      "Epoch 108/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8272 - acc: 0.6439 - val_loss: 0.8358 - val_acc: 0.6239\n",
      "Epoch 109/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8267 - acc: 0.6442 - val_loss: 0.8353 - val_acc: 0.6264\n",
      "Epoch 110/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8261 - acc: 0.6440 - val_loss: 0.8349 - val_acc: 0.6259\n",
      "Epoch 111/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8256 - acc: 0.6441 - val_loss: 0.8346 - val_acc: 0.6277\n",
      "Epoch 112/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8252 - acc: 0.6429 - val_loss: 0.8340 - val_acc: 0.6287\n",
      "Epoch 113/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8245 - acc: 0.6447 - val_loss: 0.8337 - val_acc: 0.6274\n",
      "Epoch 114/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8241 - acc: 0.6436 - val_loss: 0.8330 - val_acc: 0.6246\n",
      "Epoch 115/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8236 - acc: 0.6448 - val_loss: 0.8325 - val_acc: 0.6261\n",
      "Epoch 116/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8232 - acc: 0.6452 - val_loss: 0.8323 - val_acc: 0.6277\n",
      "Epoch 117/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8227 - acc: 0.6451 - val_loss: 0.8317 - val_acc: 0.6272\n",
      "Epoch 118/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8222 - acc: 0.6455 - val_loss: 0.8314 - val_acc: 0.6290\n",
      "Epoch 119/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8217 - acc: 0.6452 - val_loss: 0.8312 - val_acc: 0.6284\n",
      "Epoch 120/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8213 - acc: 0.6449 - val_loss: 0.8306 - val_acc: 0.6302\n",
      "Epoch 121/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8208 - acc: 0.6461 - val_loss: 0.8302 - val_acc: 0.6300\n",
      "Epoch 122/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8203 - acc: 0.6457 - val_loss: 0.8297 - val_acc: 0.6300\n",
      "Epoch 123/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8200 - acc: 0.6469 - val_loss: 0.8296 - val_acc: 0.6297\n",
      "Epoch 124/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8195 - acc: 0.6466 - val_loss: 0.8291 - val_acc: 0.6307\n",
      "Epoch 125/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8190 - acc: 0.6474 - val_loss: 0.8286 - val_acc: 0.6318\n",
      "Epoch 126/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8186 - acc: 0.6467 - val_loss: 0.8282 - val_acc: 0.6315\n",
      "Epoch 127/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8182 - acc: 0.6464 - val_loss: 0.8281 - val_acc: 0.6310\n",
      "Epoch 128/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8180 - acc: 0.6474 - val_loss: 0.8280 - val_acc: 0.6297\n",
      "Epoch 129/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8175 - acc: 0.6470 - val_loss: 0.8272 - val_acc: 0.6297\n",
      "Epoch 130/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8170 - acc: 0.6478 - val_loss: 0.8269 - val_acc: 0.6300\n",
      "Epoch 131/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8167 - acc: 0.6468 - val_loss: 0.8266 - val_acc: 0.6323\n",
      "Epoch 132/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8162 - acc: 0.6478 - val_loss: 0.8263 - val_acc: 0.6282\n",
      "Epoch 133/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8157 - acc: 0.6478 - val_loss: 0.8265 - val_acc: 0.6297\n",
      "Epoch 134/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8156 - acc: 0.6481 - val_loss: 0.8255 - val_acc: 0.6302\n",
      "Epoch 135/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8152 - acc: 0.6478 - val_loss: 0.8252 - val_acc: 0.6297\n",
      "Epoch 136/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8147 - acc: 0.6490 - val_loss: 0.8250 - val_acc: 0.6310\n",
      "Epoch 137/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8144 - acc: 0.6478 - val_loss: 0.8245 - val_acc: 0.6305\n",
      "Epoch 138/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8140 - acc: 0.6496 - val_loss: 0.8242 - val_acc: 0.6302\n",
      "Epoch 139/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8136 - acc: 0.6492 - val_loss: 0.8239 - val_acc: 0.6297\n",
      "Epoch 140/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8132 - acc: 0.6485 - val_loss: 0.8236 - val_acc: 0.6310\n",
      "Epoch 141/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8130 - acc: 0.6488 - val_loss: 0.8233 - val_acc: 0.6300\n",
      "Epoch 142/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8127 - acc: 0.6488 - val_loss: 0.8230 - val_acc: 0.6292\n",
      "Epoch 143/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8124 - acc: 0.6483 - val_loss: 0.8228 - val_acc: 0.6292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8119 - acc: 0.6499 - val_loss: 0.8225 - val_acc: 0.6300\n",
      "Epoch 145/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8117 - acc: 0.6501 - val_loss: 0.8225 - val_acc: 0.6300\n",
      "Epoch 146/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8113 - acc: 0.6491 - val_loss: 0.8218 - val_acc: 0.6292\n",
      "Epoch 147/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8109 - acc: 0.6497 - val_loss: 0.8220 - val_acc: 0.6302\n",
      "Epoch 148/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8108 - acc: 0.6501 - val_loss: 0.8213 - val_acc: 0.6292\n",
      "Epoch 149/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8103 - acc: 0.6505 - val_loss: 0.8211 - val_acc: 0.6297\n",
      "Epoch 150/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8100 - acc: 0.6508 - val_loss: 0.8207 - val_acc: 0.6297\n",
      "15663/15663 [==============================] - 0s 22us/step\n",
      "==== Training loss, score are: 0.8096670357460858 0.6505777948296771 =======\n",
      "3916/3916 [==============================] - 0s 24us/step\n",
      "==== CV loss, score are: 0.8207469563075057 0.6297242083758938 =======\n",
      "Running fold 4\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_17  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 24,734,903\n",
      "Trainable params: 303\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 38us/step\n",
      "Before training loss, score are: 1.0959287749726234 0.401200280918713\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 33us/step - loss: 1.0847 - acc: 0.4019 - val_loss: 1.0758 - val_acc: 0.4132\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0712 - acc: 0.4098 - val_loss: 1.0636 - val_acc: 0.4349\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 1.0599 - acc: 0.4301 - val_loss: 1.0528 - val_acc: 0.4372\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0497 - acc: 0.4451 - val_loss: 1.0432 - val_acc: 0.4729\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0405 - acc: 0.4633 - val_loss: 1.0337 - val_acc: 0.4740\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0318 - acc: 0.4790 - val_loss: 1.0256 - val_acc: 0.4931\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0238 - acc: 0.4938 - val_loss: 1.0178 - val_acc: 0.4895\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0165 - acc: 0.5021 - val_loss: 1.0105 - val_acc: 0.5097\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0096 - acc: 0.5120 - val_loss: 1.0038 - val_acc: 0.5235\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0029 - acc: 0.5229 - val_loss: 0.9975 - val_acc: 0.5189\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9967 - acc: 0.5267 - val_loss: 0.9915 - val_acc: 0.5401\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9908 - acc: 0.5330 - val_loss: 0.9860 - val_acc: 0.5521\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9854 - acc: 0.5422 - val_loss: 0.9808 - val_acc: 0.5582\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9800 - acc: 0.5511 - val_loss: 0.9754 - val_acc: 0.5513\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9750 - acc: 0.5515 - val_loss: 0.9710 - val_acc: 0.5674\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9703 - acc: 0.5600 - val_loss: 0.9659 - val_acc: 0.5651\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9656 - acc: 0.5629 - val_loss: 0.9616 - val_acc: 0.5666\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9613 - acc: 0.5672 - val_loss: 0.9574 - val_acc: 0.5733\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9571 - acc: 0.5701 - val_loss: 0.9537 - val_acc: 0.5815\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9531 - acc: 0.5717 - val_loss: 0.9500 - val_acc: 0.5848\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9494 - acc: 0.5780 - val_loss: 0.9459 - val_acc: 0.5881\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9457 - acc: 0.5783 - val_loss: 0.9423 - val_acc: 0.5901\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9420 - acc: 0.5819 - val_loss: 0.9389 - val_acc: 0.5940\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9387 - acc: 0.5855 - val_loss: 0.9354 - val_acc: 0.5850\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9355 - acc: 0.5858 - val_loss: 0.9325 - val_acc: 0.6011\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9323 - acc: 0.5877 - val_loss: 0.9295 - val_acc: 0.6027\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9292 - acc: 0.5903 - val_loss: 0.9266 - val_acc: 0.6019\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9262 - acc: 0.5916 - val_loss: 0.9235 - val_acc: 0.6019\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9235 - acc: 0.5921 - val_loss: 0.9206 - val_acc: 0.6034\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9207 - acc: 0.5938 - val_loss: 0.9181 - val_acc: 0.6052\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9179 - acc: 0.5958 - val_loss: 0.9155 - val_acc: 0.6024\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9155 - acc: 0.5984 - val_loss: 0.9129 - val_acc: 0.6073\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9130 - acc: 0.5998 - val_loss: 0.9104 - val_acc: 0.6085\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9107 - acc: 0.5988 - val_loss: 0.9083 - val_acc: 0.6113\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.9082 - acc: 0.6021 - val_loss: 0.9058 - val_acc: 0.6090\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9060 - acc: 0.6035 - val_loss: 0.9036 - val_acc: 0.6098\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9039 - acc: 0.6039 - val_loss: 0.9015 - val_acc: 0.6118\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9016 - acc: 0.6054 - val_loss: 0.8993 - val_acc: 0.6149\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8997 - acc: 0.6067 - val_loss: 0.8975 - val_acc: 0.6147\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8975 - acc: 0.6085 - val_loss: 0.8954 - val_acc: 0.6141\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8956 - acc: 0.6079 - val_loss: 0.8942 - val_acc: 0.6175\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8938 - acc: 0.6114 - val_loss: 0.8915 - val_acc: 0.6182\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8920 - acc: 0.6105 - val_loss: 0.8900 - val_acc: 0.6164\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8902 - acc: 0.6126 - val_loss: 0.8887 - val_acc: 0.6208\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8886 - acc: 0.6131 - val_loss: 0.8863 - val_acc: 0.6193\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8868 - acc: 0.6137 - val_loss: 0.8847 - val_acc: 0.6210\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8851 - acc: 0.6150 - val_loss: 0.8830 - val_acc: 0.6213\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8836 - acc: 0.6158 - val_loss: 0.8813 - val_acc: 0.6203\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8820 - acc: 0.6171 - val_loss: 0.8801 - val_acc: 0.6223\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8805 - acc: 0.6178 - val_loss: 0.8783 - val_acc: 0.6221\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8790 - acc: 0.6180 - val_loss: 0.8768 - val_acc: 0.6241\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8776 - acc: 0.6176 - val_loss: 0.8754 - val_acc: 0.6254\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8761 - acc: 0.6183 - val_loss: 0.8738 - val_acc: 0.6231\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8746 - acc: 0.6192 - val_loss: 0.8729 - val_acc: 0.6256\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8734 - acc: 0.6203 - val_loss: 0.8715 - val_acc: 0.6236\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8721 - acc: 0.6202 - val_loss: 0.8699 - val_acc: 0.6256\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8708 - acc: 0.6213 - val_loss: 0.8686 - val_acc: 0.6256\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8695 - acc: 0.6208 - val_loss: 0.8673 - val_acc: 0.6305\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8682 - acc: 0.6218 - val_loss: 0.8665 - val_acc: 0.6323\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8670 - acc: 0.6220 - val_loss: 0.8650 - val_acc: 0.6274\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8660 - acc: 0.6229 - val_loss: 0.8636 - val_acc: 0.6274\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8647 - acc: 0.6240 - val_loss: 0.8624 - val_acc: 0.6282\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8636 - acc: 0.6231 - val_loss: 0.8615 - val_acc: 0.6318\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8625 - acc: 0.6243 - val_loss: 0.8601 - val_acc: 0.6305\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8614 - acc: 0.6253 - val_loss: 0.8591 - val_acc: 0.6290\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8602 - acc: 0.6245 - val_loss: 0.8584 - val_acc: 0.6323\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8594 - acc: 0.6264 - val_loss: 0.8572 - val_acc: 0.6315\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8582 - acc: 0.6270 - val_loss: 0.8561 - val_acc: 0.6369\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8573 - acc: 0.6266 - val_loss: 0.8549 - val_acc: 0.6333\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8563 - acc: 0.6286 - val_loss: 0.8538 - val_acc: 0.6330\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8553 - acc: 0.6271 - val_loss: 0.8528 - val_acc: 0.6336\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8544 - acc: 0.6294 - val_loss: 0.8519 - val_acc: 0.6346\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8535 - acc: 0.6286 - val_loss: 0.8510 - val_acc: 0.6330\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8526 - acc: 0.6287 - val_loss: 0.8502 - val_acc: 0.6361\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8517 - acc: 0.6308 - val_loss: 0.8492 - val_acc: 0.6364\n",
      "Epoch 76/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8507 - acc: 0.6298 - val_loss: 0.8492 - val_acc: 0.6384\n",
      "Epoch 77/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8500 - acc: 0.6307 - val_loss: 0.8472 - val_acc: 0.6353\n",
      "Epoch 78/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8492 - acc: 0.6307 - val_loss: 0.8464 - val_acc: 0.6369\n",
      "Epoch 79/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8482 - acc: 0.6301 - val_loss: 0.8459 - val_acc: 0.6389\n",
      "Epoch 80/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8475 - acc: 0.6316 - val_loss: 0.8449 - val_acc: 0.6382\n",
      "Epoch 81/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8467 - acc: 0.6318 - val_loss: 0.8440 - val_acc: 0.6389\n",
      "Epoch 82/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8459 - acc: 0.6312 - val_loss: 0.8434 - val_acc: 0.6394\n",
      "Epoch 83/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8452 - acc: 0.6324 - val_loss: 0.8424 - val_acc: 0.6394\n",
      "Epoch 84/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8444 - acc: 0.6321 - val_loss: 0.8421 - val_acc: 0.6394\n",
      "Epoch 85/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8438 - acc: 0.6331 - val_loss: 0.8408 - val_acc: 0.6389\n",
      "Epoch 86/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8429 - acc: 0.6326 - val_loss: 0.8405 - val_acc: 0.6397\n",
      "Epoch 87/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8422 - acc: 0.6337 - val_loss: 0.8392 - val_acc: 0.6382\n",
      "Epoch 88/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8415 - acc: 0.6326 - val_loss: 0.8386 - val_acc: 0.6389\n",
      "Epoch 89/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8409 - acc: 0.6351 - val_loss: 0.8380 - val_acc: 0.6384\n",
      "Epoch 90/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8401 - acc: 0.6338 - val_loss: 0.8371 - val_acc: 0.6389\n",
      "Epoch 91/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8396 - acc: 0.6340 - val_loss: 0.8366 - val_acc: 0.6420\n",
      "Epoch 92/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8389 - acc: 0.6336 - val_loss: 0.8358 - val_acc: 0.6407\n",
      "Epoch 93/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8382 - acc: 0.6351 - val_loss: 0.8351 - val_acc: 0.6438\n",
      "Epoch 94/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8376 - acc: 0.6352 - val_loss: 0.8351 - val_acc: 0.6404\n",
      "Epoch 95/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8370 - acc: 0.6356 - val_loss: 0.8337 - val_acc: 0.6402\n",
      "Epoch 96/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8364 - acc: 0.6351 - val_loss: 0.8331 - val_acc: 0.6402\n",
      "Epoch 97/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8357 - acc: 0.6347 - val_loss: 0.8324 - val_acc: 0.6404\n",
      "Epoch 98/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8352 - acc: 0.6358 - val_loss: 0.8319 - val_acc: 0.6425\n",
      "Epoch 99/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8346 - acc: 0.6362 - val_loss: 0.8315 - val_acc: 0.6415\n",
      "Epoch 100/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8340 - acc: 0.6364 - val_loss: 0.8308 - val_acc: 0.6435\n",
      "Epoch 101/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8334 - acc: 0.6368 - val_loss: 0.8301 - val_acc: 0.6430\n",
      "Epoch 102/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8330 - acc: 0.6380 - val_loss: 0.8294 - val_acc: 0.6427\n",
      "Epoch 103/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8324 - acc: 0.6372 - val_loss: 0.8292 - val_acc: 0.6417\n",
      "Epoch 104/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8318 - acc: 0.6379 - val_loss: 0.8283 - val_acc: 0.6422\n",
      "Epoch 105/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8313 - acc: 0.6376 - val_loss: 0.8276 - val_acc: 0.6440\n",
      "Epoch 106/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8308 - acc: 0.6377 - val_loss: 0.8274 - val_acc: 0.6435\n",
      "Epoch 107/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8302 - acc: 0.6381 - val_loss: 0.8268 - val_acc: 0.6433\n",
      "Epoch 108/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8297 - acc: 0.6372 - val_loss: 0.8264 - val_acc: 0.6443\n",
      "Epoch 109/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8292 - acc: 0.6377 - val_loss: 0.8256 - val_acc: 0.6440\n",
      "Epoch 110/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8287 - acc: 0.6378 - val_loss: 0.8253 - val_acc: 0.6450\n",
      "Epoch 111/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8283 - acc: 0.6378 - val_loss: 0.8248 - val_acc: 0.6463\n",
      "Epoch 112/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8277 - acc: 0.6389 - val_loss: 0.8241 - val_acc: 0.6456\n",
      "Epoch 113/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8274 - acc: 0.6376 - val_loss: 0.8236 - val_acc: 0.6450\n",
      "Epoch 114/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8269 - acc: 0.6386 - val_loss: 0.8230 - val_acc: 0.6440\n",
      "Epoch 115/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8264 - acc: 0.6398 - val_loss: 0.8228 - val_acc: 0.6456\n",
      "Epoch 116/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8259 - acc: 0.6394 - val_loss: 0.8220 - val_acc: 0.6458\n",
      "Epoch 117/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8255 - acc: 0.6404 - val_loss: 0.8220 - val_acc: 0.6433\n",
      "Epoch 118/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8251 - acc: 0.6394 - val_loss: 0.8219 - val_acc: 0.6489\n",
      "Epoch 119/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8245 - acc: 0.6408 - val_loss: 0.8206 - val_acc: 0.6461\n",
      "Epoch 120/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8241 - acc: 0.6403 - val_loss: 0.8209 - val_acc: 0.6491\n",
      "Epoch 121/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8238 - acc: 0.6408 - val_loss: 0.8199 - val_acc: 0.6468\n",
      "Epoch 122/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8234 - acc: 0.6406 - val_loss: 0.8195 - val_acc: 0.6471\n",
      "Epoch 123/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8230 - acc: 0.6412 - val_loss: 0.8189 - val_acc: 0.6479\n",
      "Epoch 124/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8226 - acc: 0.6404 - val_loss: 0.8188 - val_acc: 0.6491\n",
      "Epoch 125/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8222 - acc: 0.6414 - val_loss: 0.8183 - val_acc: 0.6481\n",
      "Epoch 126/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8218 - acc: 0.6420 - val_loss: 0.8177 - val_acc: 0.6473\n",
      "Epoch 127/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8214 - acc: 0.6416 - val_loss: 0.8175 - val_acc: 0.6458\n",
      "Epoch 128/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8210 - acc: 0.6423 - val_loss: 0.8168 - val_acc: 0.6471\n",
      "Epoch 129/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8207 - acc: 0.6416 - val_loss: 0.8166 - val_acc: 0.6499\n",
      "Epoch 130/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8202 - acc: 0.6425 - val_loss: 0.8161 - val_acc: 0.6448\n",
      "Epoch 131/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8199 - acc: 0.6422 - val_loss: 0.8157 - val_acc: 0.6453\n",
      "Epoch 132/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8196 - acc: 0.6429 - val_loss: 0.8153 - val_acc: 0.6481\n",
      "Epoch 133/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8192 - acc: 0.6443 - val_loss: 0.8151 - val_acc: 0.6502\n",
      "Epoch 134/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8188 - acc: 0.6427 - val_loss: 0.8146 - val_acc: 0.6484\n",
      "Epoch 135/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8184 - acc: 0.6434 - val_loss: 0.8143 - val_acc: 0.6507\n",
      "Epoch 136/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8181 - acc: 0.6441 - val_loss: 0.8137 - val_acc: 0.6499\n",
      "Epoch 137/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8177 - acc: 0.6429 - val_loss: 0.8133 - val_acc: 0.6496\n",
      "Epoch 138/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8174 - acc: 0.6437 - val_loss: 0.8134 - val_acc: 0.6517\n",
      "Epoch 139/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8171 - acc: 0.6443 - val_loss: 0.8126 - val_acc: 0.6489\n",
      "Epoch 140/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8166 - acc: 0.6444 - val_loss: 0.8124 - val_acc: 0.6514\n",
      "Epoch 141/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8165 - acc: 0.6444 - val_loss: 0.8121 - val_acc: 0.6481\n",
      "Epoch 142/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8161 - acc: 0.6446 - val_loss: 0.8115 - val_acc: 0.6499\n",
      "Epoch 143/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8157 - acc: 0.6442 - val_loss: 0.8110 - val_acc: 0.6507\n",
      "Epoch 144/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8154 - acc: 0.6449 - val_loss: 0.8111 - val_acc: 0.6491\n",
      "Epoch 145/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8152 - acc: 0.6438 - val_loss: 0.8107 - val_acc: 0.6479\n",
      "Epoch 146/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8148 - acc: 0.6456 - val_loss: 0.8101 - val_acc: 0.6496\n",
      "Epoch 147/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8145 - acc: 0.6450 - val_loss: 0.8097 - val_acc: 0.6496\n",
      "Epoch 148/150\n",
      "15663/15663 [==============================] - 0s 14us/step - loss: 0.8142 - acc: 0.6457 - val_loss: 0.8097 - val_acc: 0.6512\n",
      "Epoch 149/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8140 - acc: 0.6448 - val_loss: 0.8094 - val_acc: 0.6519\n",
      "Epoch 150/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8136 - acc: 0.6447 - val_loss: 0.8091 - val_acc: 0.6530\n",
      "15663/15663 [==============================] - 0s 22us/step\n",
      "==== Training loss, score are: 0.8133315369596216 0.6457894401083594 =======\n",
      "3916/3916 [==============================] - 0s 21us/step\n",
      "==== CV loss, score are: 0.809124727475631 0.6529622063329928 =======\n",
      "Running fold 5\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_18  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 24,734,903\n",
      "Trainable params: 303\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15664/15664 [==============================] - 1s 38us/step\n",
      "Before training loss, score are: 1.0946128056655744 0.3972165474974464\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/150\n",
      "15664/15664 [==============================] - 1s 34us/step - loss: 1.0872 - acc: 0.3994 - val_loss: 1.0766 - val_acc: 0.4212\n",
      "Epoch 2/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 1.0747 - acc: 0.4014 - val_loss: 1.0646 - val_acc: 0.4330\n",
      "Epoch 3/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 1.0636 - acc: 0.4180 - val_loss: 1.0539 - val_acc: 0.4580\n",
      "Epoch 4/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 1.0532 - acc: 0.4427 - val_loss: 1.0435 - val_acc: 0.4600\n",
      "Epoch 5/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 1.0442 - acc: 0.4545 - val_loss: 1.0344 - val_acc: 0.4805\n",
      "Epoch 6/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 1.0357 - acc: 0.4724 - val_loss: 1.0264 - val_acc: 0.5006\n",
      "Epoch 7/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 1.0277 - acc: 0.4865 - val_loss: 1.0180 - val_acc: 0.5052\n",
      "Epoch 8/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 1.0201 - acc: 0.5011 - val_loss: 1.0105 - val_acc: 0.5165\n",
      "Epoch 9/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 1.0130 - acc: 0.5109 - val_loss: 1.0043 - val_acc: 0.5361\n",
      "Epoch 10/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 1.0065 - acc: 0.5202 - val_loss: 0.9977 - val_acc: 0.5336\n",
      "Epoch 11/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 1.0003 - acc: 0.5289 - val_loss: 0.9912 - val_acc: 0.5425\n",
      "Epoch 12/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9945 - acc: 0.5340 - val_loss: 0.9856 - val_acc: 0.5466\n",
      "Epoch 13/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9888 - acc: 0.5410 - val_loss: 0.9797 - val_acc: 0.5517\n",
      "Epoch 14/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9835 - acc: 0.5479 - val_loss: 0.9748 - val_acc: 0.5589\n",
      "Epoch 15/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9785 - acc: 0.5501 - val_loss: 0.9696 - val_acc: 0.5604\n",
      "Epoch 16/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9736 - acc: 0.5561 - val_loss: 0.9653 - val_acc: 0.5678\n",
      "Epoch 17/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.9690 - acc: 0.5577 - val_loss: 0.9615 - val_acc: 0.5788\n",
      "Epoch 18/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9646 - acc: 0.5621 - val_loss: 0.9565 - val_acc: 0.5788\n",
      "Epoch 19/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9604 - acc: 0.5675 - val_loss: 0.9527 - val_acc: 0.5844\n",
      "Epoch 20/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.9564 - acc: 0.5696 - val_loss: 0.9487 - val_acc: 0.5852\n",
      "Epoch 21/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.9525 - acc: 0.5735 - val_loss: 0.9454 - val_acc: 0.5883\n",
      "Epoch 22/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9488 - acc: 0.5773 - val_loss: 0.9414 - val_acc: 0.5949\n",
      "Epoch 23/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.9451 - acc: 0.5796 - val_loss: 0.9375 - val_acc: 0.5893\n",
      "Epoch 24/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9416 - acc: 0.5812 - val_loss: 0.9346 - val_acc: 0.6015\n",
      "Epoch 25/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9384 - acc: 0.5852 - val_loss: 0.9313 - val_acc: 0.5987\n",
      "Epoch 26/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.9352 - acc: 0.5878 - val_loss: 0.9273 - val_acc: 0.5969\n",
      "Epoch 27/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.9321 - acc: 0.5875 - val_loss: 0.9249 - val_acc: 0.6036\n",
      "Epoch 28/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9292 - acc: 0.5905 - val_loss: 0.9224 - val_acc: 0.6061\n",
      "Epoch 29/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9263 - acc: 0.5920 - val_loss: 0.9187 - val_acc: 0.6000\n",
      "Epoch 30/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9235 - acc: 0.5914 - val_loss: 0.9166 - val_acc: 0.6069\n",
      "Epoch 31/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.9208 - acc: 0.5944 - val_loss: 0.9142 - val_acc: 0.6112\n",
      "Epoch 32/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9181 - acc: 0.5950 - val_loss: 0.9117 - val_acc: 0.6135\n",
      "Epoch 33/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9157 - acc: 0.5964 - val_loss: 0.9097 - val_acc: 0.6158\n",
      "Epoch 34/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9134 - acc: 0.5983 - val_loss: 0.9066 - val_acc: 0.6163\n",
      "Epoch 35/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9109 - acc: 0.5995 - val_loss: 0.9040 - val_acc: 0.6166\n",
      "Epoch 36/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9086 - acc: 0.6007 - val_loss: 0.9018 - val_acc: 0.6156\n",
      "Epoch 37/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.9063 - acc: 0.6030 - val_loss: 0.8996 - val_acc: 0.6176\n",
      "Epoch 38/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9042 - acc: 0.6033 - val_loss: 0.8976 - val_acc: 0.6186\n",
      "Epoch 39/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9020 - acc: 0.6046 - val_loss: 0.8952 - val_acc: 0.6156\n",
      "Epoch 40/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.9000 - acc: 0.6057 - val_loss: 0.8936 - val_acc: 0.6181\n",
      "Epoch 41/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8980 - acc: 0.6079 - val_loss: 0.8917 - val_acc: 0.6186\n",
      "Epoch 42/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8962 - acc: 0.6073 - val_loss: 0.8898 - val_acc: 0.6204\n",
      "Epoch 43/150\n",
      "15664/15664 [==============================] - 0s 17us/step - loss: 0.8942 - acc: 0.6099 - val_loss: 0.8880 - val_acc: 0.6197\n",
      "Epoch 44/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8924 - acc: 0.6099 - val_loss: 0.8856 - val_acc: 0.6186\n",
      "Epoch 45/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8907 - acc: 0.6110 - val_loss: 0.8847 - val_acc: 0.6245\n",
      "Epoch 46/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8889 - acc: 0.6115 - val_loss: 0.8822 - val_acc: 0.6194\n",
      "Epoch 47/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8872 - acc: 0.6126 - val_loss: 0.8807 - val_acc: 0.6209\n",
      "Epoch 48/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8856 - acc: 0.6134 - val_loss: 0.8794 - val_acc: 0.6232\n",
      "Epoch 49/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8838 - acc: 0.6148 - val_loss: 0.8775 - val_acc: 0.6248\n",
      "Epoch 50/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8823 - acc: 0.6152 - val_loss: 0.8766 - val_acc: 0.6245\n",
      "Epoch 51/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8808 - acc: 0.6163 - val_loss: 0.8749 - val_acc: 0.6271\n",
      "Epoch 52/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8792 - acc: 0.6179 - val_loss: 0.8731 - val_acc: 0.6289\n",
      "Epoch 53/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8777 - acc: 0.6164 - val_loss: 0.8721 - val_acc: 0.6284\n",
      "Epoch 54/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8764 - acc: 0.6171 - val_loss: 0.8702 - val_acc: 0.6284\n",
      "Epoch 55/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8750 - acc: 0.6181 - val_loss: 0.8689 - val_acc: 0.6309\n",
      "Epoch 56/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8737 - acc: 0.6187 - val_loss: 0.8675 - val_acc: 0.6314\n",
      "Epoch 57/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8722 - acc: 0.6200 - val_loss: 0.8662 - val_acc: 0.6312\n",
      "Epoch 58/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8711 - acc: 0.6205 - val_loss: 0.8646 - val_acc: 0.6312\n",
      "Epoch 59/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8697 - acc: 0.6202 - val_loss: 0.8640 - val_acc: 0.6330\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8684 - acc: 0.6217 - val_loss: 0.8625 - val_acc: 0.6314\n",
      "Epoch 61/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8674 - acc: 0.6216 - val_loss: 0.8616 - val_acc: 0.6350\n",
      "Epoch 62/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8660 - acc: 0.6216 - val_loss: 0.8597 - val_acc: 0.6319\n",
      "Epoch 63/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8649 - acc: 0.6223 - val_loss: 0.8590 - val_acc: 0.6350\n",
      "Epoch 64/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8637 - acc: 0.6231 - val_loss: 0.8582 - val_acc: 0.6360\n",
      "Epoch 65/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8626 - acc: 0.6250 - val_loss: 0.8566 - val_acc: 0.6350\n",
      "Epoch 66/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8614 - acc: 0.6249 - val_loss: 0.8553 - val_acc: 0.6342\n",
      "Epoch 67/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8605 - acc: 0.6247 - val_loss: 0.8544 - val_acc: 0.6340\n",
      "Epoch 68/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8594 - acc: 0.6251 - val_loss: 0.8537 - val_acc: 0.6375\n",
      "Epoch 69/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8585 - acc: 0.6267 - val_loss: 0.8530 - val_acc: 0.6378\n",
      "Epoch 70/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8574 - acc: 0.6263 - val_loss: 0.8517 - val_acc: 0.6388\n",
      "Epoch 71/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8563 - acc: 0.6278 - val_loss: 0.8514 - val_acc: 0.6388\n",
      "Epoch 72/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8555 - acc: 0.6272 - val_loss: 0.8504 - val_acc: 0.6421\n",
      "Epoch 73/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8545 - acc: 0.6296 - val_loss: 0.8486 - val_acc: 0.6381\n",
      "Epoch 74/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8535 - acc: 0.6292 - val_loss: 0.8480 - val_acc: 0.6381\n",
      "Epoch 75/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8526 - acc: 0.6288 - val_loss: 0.8471 - val_acc: 0.6414\n",
      "Epoch 76/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8517 - acc: 0.6287 - val_loss: 0.8462 - val_acc: 0.6381\n",
      "Epoch 77/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8507 - acc: 0.6303 - val_loss: 0.8452 - val_acc: 0.6386\n",
      "Epoch 78/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8499 - acc: 0.6297 - val_loss: 0.8447 - val_acc: 0.6378\n",
      "Epoch 79/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8490 - acc: 0.6307 - val_loss: 0.8444 - val_acc: 0.6442\n",
      "Epoch 80/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8482 - acc: 0.6310 - val_loss: 0.8431 - val_acc: 0.6416\n",
      "Epoch 81/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8474 - acc: 0.6311 - val_loss: 0.8422 - val_acc: 0.6442\n",
      "Epoch 82/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8466 - acc: 0.6322 - val_loss: 0.8418 - val_acc: 0.6455\n",
      "Epoch 83/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8459 - acc: 0.6324 - val_loss: 0.8412 - val_acc: 0.6470\n",
      "Epoch 84/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8451 - acc: 0.6327 - val_loss: 0.8401 - val_acc: 0.6442\n",
      "Epoch 85/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8443 - acc: 0.6325 - val_loss: 0.8394 - val_acc: 0.6447\n",
      "Epoch 86/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8436 - acc: 0.6332 - val_loss: 0.8385 - val_acc: 0.6437\n",
      "Epoch 87/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8427 - acc: 0.6325 - val_loss: 0.8378 - val_acc: 0.6462\n",
      "Epoch 88/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8421 - acc: 0.6333 - val_loss: 0.8365 - val_acc: 0.6447\n",
      "Epoch 89/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8414 - acc: 0.6328 - val_loss: 0.8363 - val_acc: 0.6439\n",
      "Epoch 90/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8408 - acc: 0.6341 - val_loss: 0.8358 - val_acc: 0.6450\n",
      "Epoch 91/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8400 - acc: 0.6339 - val_loss: 0.8348 - val_acc: 0.6442\n",
      "Epoch 92/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8393 - acc: 0.6336 - val_loss: 0.8344 - val_acc: 0.6473\n",
      "Epoch 93/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8386 - acc: 0.6342 - val_loss: 0.8333 - val_acc: 0.6452\n",
      "Epoch 94/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8380 - acc: 0.6350 - val_loss: 0.8326 - val_acc: 0.6442\n",
      "Epoch 95/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8374 - acc: 0.6350 - val_loss: 0.8321 - val_acc: 0.6447\n",
      "Epoch 96/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8367 - acc: 0.6346 - val_loss: 0.8319 - val_acc: 0.6467\n",
      "Epoch 97/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8361 - acc: 0.6358 - val_loss: 0.8312 - val_acc: 0.6465\n",
      "Epoch 98/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8354 - acc: 0.6355 - val_loss: 0.8307 - val_acc: 0.6478\n",
      "Epoch 99/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8348 - acc: 0.6359 - val_loss: 0.8298 - val_acc: 0.6467\n",
      "Epoch 100/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8342 - acc: 0.6374 - val_loss: 0.8298 - val_acc: 0.6506\n",
      "Epoch 101/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8337 - acc: 0.6374 - val_loss: 0.8291 - val_acc: 0.6470\n",
      "Epoch 102/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8331 - acc: 0.6375 - val_loss: 0.8281 - val_acc: 0.6442\n",
      "Epoch 103/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8326 - acc: 0.6368 - val_loss: 0.8282 - val_acc: 0.6503\n",
      "Epoch 104/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8320 - acc: 0.6373 - val_loss: 0.8269 - val_acc: 0.6475\n",
      "Epoch 105/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8314 - acc: 0.6367 - val_loss: 0.8265 - val_acc: 0.6488\n",
      "Epoch 106/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8309 - acc: 0.6376 - val_loss: 0.8262 - val_acc: 0.6498\n",
      "Epoch 107/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8303 - acc: 0.6373 - val_loss: 0.8255 - val_acc: 0.6480\n",
      "Epoch 108/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8299 - acc: 0.6371 - val_loss: 0.8252 - val_acc: 0.6501\n",
      "Epoch 109/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8293 - acc: 0.6383 - val_loss: 0.8245 - val_acc: 0.6498\n",
      "Epoch 110/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8288 - acc: 0.6393 - val_loss: 0.8243 - val_acc: 0.6508\n",
      "Epoch 111/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8283 - acc: 0.6385 - val_loss: 0.8236 - val_acc: 0.6519\n",
      "Epoch 112/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8277 - acc: 0.6389 - val_loss: 0.8234 - val_acc: 0.6513\n",
      "Epoch 113/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8274 - acc: 0.6378 - val_loss: 0.8228 - val_acc: 0.6524\n",
      "Epoch 114/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8268 - acc: 0.6391 - val_loss: 0.8221 - val_acc: 0.6542\n",
      "Epoch 115/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8264 - acc: 0.6387 - val_loss: 0.8219 - val_acc: 0.6531\n",
      "Epoch 116/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8260 - acc: 0.6394 - val_loss: 0.8210 - val_acc: 0.6521\n",
      "Epoch 117/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8254 - acc: 0.6404 - val_loss: 0.8209 - val_acc: 0.6559\n",
      "Epoch 118/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8250 - acc: 0.6411 - val_loss: 0.8204 - val_acc: 0.6524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8245 - acc: 0.6393 - val_loss: 0.8207 - val_acc: 0.6549\n",
      "Epoch 120/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8240 - acc: 0.6410 - val_loss: 0.8198 - val_acc: 0.6544\n",
      "Epoch 121/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8235 - acc: 0.6410 - val_loss: 0.8192 - val_acc: 0.6559\n",
      "Epoch 122/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8231 - acc: 0.6415 - val_loss: 0.8185 - val_acc: 0.6549\n",
      "Epoch 123/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8228 - acc: 0.6418 - val_loss: 0.8183 - val_acc: 0.6557\n",
      "Epoch 124/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8223 - acc: 0.6410 - val_loss: 0.8185 - val_acc: 0.6552\n",
      "Epoch 125/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8218 - acc: 0.6424 - val_loss: 0.8174 - val_acc: 0.6554\n",
      "Epoch 126/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8215 - acc: 0.6417 - val_loss: 0.8175 - val_acc: 0.6557\n",
      "Epoch 127/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8211 - acc: 0.6424 - val_loss: 0.8168 - val_acc: 0.6559\n",
      "Epoch 128/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8206 - acc: 0.6420 - val_loss: 0.8160 - val_acc: 0.6559\n",
      "Epoch 129/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8203 - acc: 0.6414 - val_loss: 0.8157 - val_acc: 0.6552\n",
      "Epoch 130/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8198 - acc: 0.6427 - val_loss: 0.8158 - val_acc: 0.6562\n",
      "Epoch 131/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8194 - acc: 0.6430 - val_loss: 0.8152 - val_acc: 0.6564\n",
      "Epoch 132/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8191 - acc: 0.6424 - val_loss: 0.8147 - val_acc: 0.6554\n",
      "Epoch 133/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8187 - acc: 0.6438 - val_loss: 0.8144 - val_acc: 0.6559\n",
      "Epoch 134/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8182 - acc: 0.6450 - val_loss: 0.8143 - val_acc: 0.6552\n",
      "Epoch 135/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8179 - acc: 0.6434 - val_loss: 0.8133 - val_acc: 0.6547\n",
      "Epoch 136/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8177 - acc: 0.6441 - val_loss: 0.8133 - val_acc: 0.6554\n",
      "Epoch 137/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8172 - acc: 0.6444 - val_loss: 0.8133 - val_acc: 0.6559\n",
      "Epoch 138/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8169 - acc: 0.6443 - val_loss: 0.8125 - val_acc: 0.6559\n",
      "Epoch 139/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8166 - acc: 0.6447 - val_loss: 0.8124 - val_acc: 0.6562\n",
      "Epoch 140/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8162 - acc: 0.6451 - val_loss: 0.8119 - val_acc: 0.6559\n",
      "Epoch 141/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8158 - acc: 0.6465 - val_loss: 0.8122 - val_acc: 0.6562\n",
      "Epoch 142/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8155 - acc: 0.6447 - val_loss: 0.8112 - val_acc: 0.6559\n",
      "Epoch 143/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8152 - acc: 0.6452 - val_loss: 0.8108 - val_acc: 0.6554\n",
      "Epoch 144/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8148 - acc: 0.6449 - val_loss: 0.8116 - val_acc: 0.6567\n",
      "Epoch 145/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8145 - acc: 0.6457 - val_loss: 0.8104 - val_acc: 0.6577\n",
      "Epoch 146/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8142 - acc: 0.6452 - val_loss: 0.8102 - val_acc: 0.6564\n",
      "Epoch 147/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8139 - acc: 0.6457 - val_loss: 0.8098 - val_acc: 0.6567\n",
      "Epoch 148/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8135 - acc: 0.6455 - val_loss: 0.8100 - val_acc: 0.6577\n",
      "Epoch 149/150\n",
      "15664/15664 [==============================] - 0s 15us/step - loss: 0.8132 - acc: 0.6455 - val_loss: 0.8096 - val_acc: 0.6570\n",
      "Epoch 150/150\n",
      "15664/15664 [==============================] - 0s 14us/step - loss: 0.8129 - acc: 0.6464 - val_loss: 0.8088 - val_acc: 0.6590\n",
      "15664/15664 [==============================] - 0s 22us/step\n",
      "==== Training loss, score are: 0.8126655830426163 0.6449182839632278 =======\n",
      "3915/3915 [==============================] - 0s 20us/step\n",
      "==== CV loss, score are: 0.8087702428700824 0.6590038314023998 =======\n",
      "\n",
      "\n",
      "===== Model: fast_text_glove  ========:\n",
      " Cross-val log losses are: [0.81551025462464311, 0.82363314849221192, 0.82074695217256188, 0.80912472312447459, 0.80877023530219094]\n",
      "====== Mean cross-val log loss is: 0.8155570627432166 =========\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Fast text and add predictions as features:\n",
    "\n",
    "# No pre-trained vectors:\n",
    "print('Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "model_name = \"fast_text_none\"\n",
    "model_preprocess_function = run_NN_preprocess\n",
    "model_function = run_Fasttext\n",
    "model_params = {'temp': 0 }\n",
    "model_preprocess_params = {'char_level': 0, 'word_vector_type': None }\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"NN\", \n",
    "                                           train_raw, y_train_one_hot, test_raw, 1) \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n",
    "\n",
    "# Pre-trained word2vec using vectors:\n",
    "print('Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "wordvecsize = 20\n",
    "create_gensim_wordvectors = 0\n",
    "if create_gensim_wordvectors:\n",
    "    create_gensim_wordvec(train_raw['text'], wordvecsize)\n",
    "model_name = \"fast_text_gensim\"\n",
    "model_preprocess_function = run_NN_preprocess\n",
    "model_function = run_Fasttext\n",
    "model_params = {'temp': 0 }\n",
    "model_preprocess_params = {'char_level': 0, 'word_vector_type': \"gensim\" }\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"NN\", \n",
    "                                           train_raw, y_train_one_hot, test_raw, 1) \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n",
    "    \n",
    "# Glove vectors:\n",
    "print('Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "model_name = \"fast_text_glove\"\n",
    "model_preprocess_function = run_NN_preprocess\n",
    "model_function = run_Fasttext\n",
    "model_params = {'temp': 0 }\n",
    "model_preprocess_params = {'char_level': 0, 'word_vector_type': \"glove\" }\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"NN\", \n",
    "                                           train_raw, y_train_one_hot, test_raw, 1) \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model definition and running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN \n",
    "def run_CNN(sentences_encoded_train, y_train_one_hot,\n",
    "                 sentences_encoded_val, y_val_one_hot,\n",
    "                 sentences_encoded_test, \n",
    "                 model_params):  \n",
    "\n",
    "    # All model params\n",
    "    vocab_size = model_params['vocab_size'] \n",
    "    wordvecdim = model_params['wordvecdim']\n",
    "    sentence_maxlength_cap = model_params['sentence_maxlength_cap']\n",
    "    embedding_matrix = model_params['embedding_matrix']\n",
    "    word_vector_type = model_params['word_vector_type']\n",
    "    tokenizer =  model_params['tokenizer']    \n",
    "    print(\"Creating CNN model vocab_size: {}, wordvecdim {}, sentence_maxlength_cap {}, word_vector_type {} \".format( \n",
    "          vocab_size, wordvecdim, sentence_maxlength_cap, word_vector_type))\n",
    "    \n",
    "    # Train CNN model (1 hidden layer)\n",
    "    model = Sequential()\n",
    "    if word_vector_type == \"glove\":\n",
    "        embedded_layer = Embedding(vocab_size, wordvecdim, \n",
    "                                   input_length = sentence_maxlength_cap, \n",
    "                                   weights = [embedding_matrix], trainable = False)\n",
    "        #embedded_layer = Embedding(vocab_size, wordvecdim, input_length = sentence_maxlength_cap, weights = [embedding_matrix], trainable = True)\n",
    "    elif word_vector_type == \"gensim\":\n",
    "        embedded_layer = Embedding(vocab_size, wordvecdim, \n",
    "                                   input_length = sentence_maxlength_cap, \n",
    "                                   weights = [embedding_matrix], trainable = True)\n",
    "    else:\n",
    "        embedded_layer = Embedding(vocab_size, wordvecdim, \n",
    "                                   input_length = sentence_maxlength_cap)\n",
    "    model.add(embedded_layer)\n",
    "    #model.add(Flatten())\n",
    "    #model.add(Dense(20,activation='relu'))\n",
    "    model.add(Conv1D(filters = 32, kernel_size = 4, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.8))\n",
    "    #model.add(Conv1D(filters = 16, kernel_size = 4, activation='relu'))\n",
    "    #model.add(MaxPooling1D(pool_size=2))\n",
    "    #model.add(Dropout(0.8))\n",
    "    #model.add(Flatten())\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Pre training loss\n",
    "    loss, score = model.evaluate(sentences_encoded_train, y_train_one_hot)\n",
    "    print(\"Before training loss, score are: {} {}\".format(loss,score))\n",
    "\n",
    "    # Fit the model to train data\n",
    "    #num_epochs = 200\n",
    "    num_epochs = 150\n",
    "    validation_data = (sentences_encoded_val, y_val_one_hot)\n",
    "    checkpointer = EarlyStopping(patience=3, monitor='val_loss')\n",
    "    model.fit(sentences_encoded_train, y_train_one_hot,\n",
    "              epochs= num_epochs, \n",
    "              batch_size=128, \n",
    "              verbose=1, \n",
    "              validation_data = validation_data,\n",
    "              #validation_split = 0.05, shuffle = True,              \n",
    "              callbacks=[checkpointer])\n",
    "\n",
    "    # Training loss and predictions:\n",
    "    loss, score = model.evaluate(sentences_encoded_train, y_train_one_hot)\n",
    "    print(\"====== Training loss, score are: {} {} =======\".format(loss,score))\n",
    "    pred_train = model.predict(sentences_encoded_train)\n",
    "\n",
    "    # Val loss and predictions:\n",
    "    loss, score = model.evaluate(sentences_encoded_val, y_val_one_hot)\n",
    "    print(\"===== CV loss, score are: {} {} =======\".format(loss,score))\n",
    "    pred_val = model.predict(sentences_encoded_val)\n",
    "    \n",
    "    # Test loss and predictions:\n",
    "    pred_test = model.predict(sentences_encoded_test)\n",
    "\n",
    "    return pred_val, pred_test, model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create DL submission:\n",
    "       \n",
    "create_submission_dl('fasttext', model, sentences_encoded_ftest )\n",
    "#create_submission_dl('test', model, sentences_encoded_test )\n",
    "#pred_test = model.predict(sentences_encoded)\n",
    "#print(pred_test[0:2,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 2018-Jan-14 03:03:10\n",
      "Running kfold training with model cnn_none\n",
      "Shapes: x_train_raw.shape (19579, 27), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 26)\n",
      "Running preprocess: <function run_NN_preprocess at 0x7f620ee8eea0>\n",
      "No of unique words found is 247346\n",
      "No of words occuring more than 2 times is 75104\n",
      "No of unique words found is after capping is 247346\n",
      "max sentence length is 1439\n",
      "mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000\n",
      "After padding: max sentence size is 161\n",
      "After padding: mean, median length of sentence is 161.000000 161.000000\n",
      "Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)\n",
      "Running fold 1\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 158, 32)           2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_19  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 4,949,611\n",
      "Trainable params: 4,949,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 82us/step\n",
      "Before training loss, score are: 1.09661056803902 0.4066909276905594\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 2s 102us/step - loss: 1.0811 - acc: 0.4089 - val_loss: 1.0644 - val_acc: 0.4170\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.9596 - acc: 0.5576 - val_loss: 0.8339 - val_acc: 0.7012\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.6538 - acc: 0.7885 - val_loss: 0.5921 - val_acc: 0.7942\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.4098 - acc: 0.8846 - val_loss: 0.4632 - val_acc: 0.8394\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.2732 - acc: 0.9282 - val_loss: 0.3998 - val_acc: 0.8504\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.1879 - acc: 0.9551 - val_loss: 0.3681 - val_acc: 0.8613\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.1389 - acc: 0.9690 - val_loss: 0.3559 - val_acc: 0.8618\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1035 - acc: 0.9775 - val_loss: 0.3501 - val_acc: 0.8606\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 56us/step - loss: 0.0823 - acc: 0.9834 - val_loss: 0.3505 - val_acc: 0.8613\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0655 - acc: 0.9870 - val_loss: 0.3532 - val_acc: 0.8624\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0527 - acc: 0.9909 - val_loss: 0.3622 - val_acc: 0.8606\n",
      "15663/15663 [==============================] - 0s 27us/step\n",
      "====== Training loss, score are: 0.028422342061788113 0.9966162293302688 =======\n",
      "3916/3916 [==============================] - 0s 45us/step\n",
      "===== CV loss, score are: 0.36218819420963555 0.8605720123182887 =======\n",
      "Running fold 2\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 158, 32)           2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_20  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 4,949,611\n",
      "Trainable params: 4,949,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 46us/step\n",
      "Before training loss, score are: 1.0992496341213842 0.3213943689081668\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 79us/step - loss: 1.0834 - acc: 0.4037 - val_loss: 1.0745 - val_acc: 0.3966\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 57us/step - loss: 0.9889 - acc: 0.5229 - val_loss: 0.8753 - val_acc: 0.6223\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.6710 - acc: 0.7768 - val_loss: 0.6023 - val_acc: 0.8087\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.4187 - acc: 0.8757 - val_loss: 0.4689 - val_acc: 0.8417\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.2781 - acc: 0.9232 - val_loss: 0.4146 - val_acc: 0.8430\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1978 - acc: 0.9477 - val_loss: 0.3792 - val_acc: 0.8624\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.1432 - acc: 0.9650 - val_loss: 0.3646 - val_acc: 0.8573\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1088 - acc: 0.9750 - val_loss: 0.3504 - val_acc: 0.8659\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.0830 - acc: 0.9818 - val_loss: 0.3515 - val_acc: 0.8631\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.0660 - acc: 0.9868 - val_loss: 0.3531 - val_acc: 0.8682\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0539 - acc: 0.9898 - val_loss: 0.3639 - val_acc: 0.8670\n",
      "15663/15663 [==============================] - 0s 26us/step\n",
      "====== Training loss, score are: 0.03306201452471771 0.9972546766264445 =======\n",
      "3916/3916 [==============================] - 0s 25us/step\n",
      "===== CV loss, score are: 0.363909604661433 0.8669560776911182 =======\n",
      "Running fold 3\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 158, 32)           2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_21  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 4,949,611\n",
      "Trainable params: 4,949,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 1s 46us/step\n",
      "Before training loss, score are: 1.0986588536220316 0.32247972931166546\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 78us/step - loss: 1.0828 - acc: 0.4048 - val_loss: 1.0634 - val_acc: 0.4188\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.9692 - acc: 0.5284 - val_loss: 0.8532 - val_acc: 0.6356\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.6736 - acc: 0.7681 - val_loss: 0.5988 - val_acc: 0.7875\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.4240 - acc: 0.8728 - val_loss: 0.4666 - val_acc: 0.8394\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.2837 - acc: 0.9203 - val_loss: 0.4002 - val_acc: 0.8565\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.1974 - acc: 0.9496 - val_loss: 0.3668 - val_acc: 0.8647\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 57us/step - loss: 0.1449 - acc: 0.9640 - val_loss: 0.3495 - val_acc: 0.8657\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1095 - acc: 0.9751 - val_loss: 0.3405 - val_acc: 0.8721\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0837 - acc: 0.9842 - val_loss: 0.3416 - val_acc: 0.8685\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0665 - acc: 0.9861 - val_loss: 0.3388 - val_acc: 0.8690\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0537 - acc: 0.9900 - val_loss: 0.3382 - val_acc: 0.8693\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.0444 - acc: 0.9923 - val_loss: 0.3412 - val_acc: 0.8680\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.0366 - acc: 0.9937 - val_loss: 0.3495 - val_acc: 0.8664\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.0311 - acc: 0.9950 - val_loss: 0.3558 - val_acc: 0.8664\n",
      "15663/15663 [==============================] - 0s 26us/step\n",
      "====== Training loss, score are: 0.014943611511237236 0.9990423290557364 =======\n",
      "3916/3916 [==============================] - 0s 27us/step\n",
      "===== CV loss, score are: 0.3558085434770438 0.8664453524004085 =======\n",
      "Running fold 4\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 158, 32)           2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_22  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 4,949,611\n",
      "Trainable params: 4,949,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 48us/step\n",
      "Before training loss, score are: 1.0996042492288758 0.3130307093216056\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 80us/step - loss: 1.0829 - acc: 0.4020 - val_loss: 1.0603 - val_acc: 0.4275\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.9574 - acc: 0.5685 - val_loss: 0.8192 - val_acc: 0.7135\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.6284 - acc: 0.8043 - val_loss: 0.5672 - val_acc: 0.8039\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.3913 - acc: 0.8892 - val_loss: 0.4511 - val_acc: 0.8340\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.2589 - acc: 0.9314 - val_loss: 0.3920 - val_acc: 0.8529\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1801 - acc: 0.9557 - val_loss: 0.3632 - val_acc: 0.8626\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.1327 - acc: 0.9679 - val_loss: 0.3499 - val_acc: 0.8626\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.1011 - acc: 0.9778 - val_loss: 0.3435 - val_acc: 0.8629\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.0770 - acc: 0.9853 - val_loss: 0.3452 - val_acc: 0.8603\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.0636 - acc: 0.9864 - val_loss: 0.3452 - val_acc: 0.8603\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.0516 - acc: 0.9904 - val_loss: 0.3486 - val_acc: 0.8624\n",
      "15663/15663 [==============================] - 0s 27us/step\n",
      "====== Training loss, score are: 0.026671318204130795 0.9977654344633851 =======\n",
      "3916/3916 [==============================] - 0s 27us/step\n",
      "===== CV loss, score are: 0.34859397109365803 0.862359550622681 =======\n",
      "Running fold 5\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 158, 32)           2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_23  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 4,949,611\n",
      "Trainable params: 4,949,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15664/15664 [==============================] - 1s 50us/step\n",
      "Before training loss, score are: 1.100977397427739 0.28919816138917265\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/150\n",
      "15664/15664 [==============================] - 1s 92us/step - loss: 1.0846 - acc: 0.3942 - val_loss: 1.0570 - val_acc: 0.4281\n",
      "Epoch 2/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.9579 - acc: 0.5699 - val_loss: 0.8181 - val_acc: 0.7374\n",
      "Epoch 3/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.6286 - acc: 0.8092 - val_loss: 0.5688 - val_acc: 0.8347\n",
      "Epoch 4/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.3823 - acc: 0.8969 - val_loss: 0.4450 - val_acc: 0.8506\n",
      "Epoch 5/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.2524 - acc: 0.9371 - val_loss: 0.3938 - val_acc: 0.8618\n",
      "Epoch 6/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.1764 - acc: 0.9581 - val_loss: 0.3642 - val_acc: 0.8639\n",
      "Epoch 7/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.1287 - acc: 0.9735 - val_loss: 0.3476 - val_acc: 0.8656\n",
      "Epoch 8/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.0967 - acc: 0.9791 - val_loss: 0.3453 - val_acc: 0.8677\n",
      "Epoch 9/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.0752 - acc: 0.9854 - val_loss: 0.3468 - val_acc: 0.8646\n",
      "Epoch 10/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.0597 - acc: 0.9891 - val_loss: 0.3456 - val_acc: 0.8649\n",
      "Epoch 11/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.0490 - acc: 0.9922 - val_loss: 0.3506 - val_acc: 0.8646\n",
      "15664/15664 [==============================] - 0s 30us/step\n",
      "====== Training loss, score are: 0.0258067818553557 0.9981486210418795 =======\n",
      "3915/3915 [==============================] - 0s 30us/step\n",
      "===== CV loss, score are: 0.3506191249779815 0.8646232439183641 =======\n",
      "\n",
      "\n",
      "===== Model: cnn_none  ========:\n",
      " Cross-val log losses are: [0.36218818797598962, 0.3639095987832564, 0.35580853542306584, 0.34859396705752815, 0.35061912189149319]\n",
      "====== Mean cross-val log loss is: 0.3562238822262666 =========\n",
      "\n",
      "\n",
      "Timestamp: 2018-Jan-14 03:04:21\n",
      "Running kfold training with model cnn_gensim\n",
      "Shapes: x_train_raw.shape (19579, 30), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 29)\n",
      "Running preprocess: <function run_NN_preprocess at 0x7f620ee8eea0>\n",
      "No of unique words found is 247346\n",
      "No of words occuring more than 2 times is 75104\n",
      "No of unique words found is after capping is 247346\n",
      "max sentence length is 1439\n",
      "mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000\n",
      "After padding: max sentence size is 161\n",
      "After padding: mean, median length of sentence is 161.000000 161.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9786it [00:00, 97837.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word embedding matrix for gensim vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76597it [00:00, 99453.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76597 word vectors.\n",
      "of--mirth 247345\n",
      "None\n",
      "(247346, 20)\n",
      "Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)\n",
      "Running fold 1\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 158, 32)           2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_24  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 4,949,611\n",
      "Trainable params: 4,949,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 51us/step\n",
      "Before training loss, score are: 1.8506783989359727 0.3065823916330851\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 91us/step - loss: 1.1146 - acc: 0.4193 - val_loss: 0.9560 - val_acc: 0.5715\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.9522 - acc: 0.5461 - val_loss: 0.8747 - val_acc: 0.6284\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.8946 - acc: 0.5879 - val_loss: 0.8356 - val_acc: 0.6384\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.8648 - acc: 0.6095 - val_loss: 0.8059 - val_acc: 0.6588\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.8295 - acc: 0.6339 - val_loss: 0.7847 - val_acc: 0.6647\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.7934 - acc: 0.6545 - val_loss: 0.7557 - val_acc: 0.6874\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.7691 - acc: 0.6676 - val_loss: 0.7354 - val_acc: 0.6910\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.7405 - acc: 0.6862 - val_loss: 0.7105 - val_acc: 0.7066\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.7069 - acc: 0.7057 - val_loss: 0.6896 - val_acc: 0.7165\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.6775 - acc: 0.7198 - val_loss: 0.6646 - val_acc: 0.7314\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.6430 - acc: 0.7388 - val_loss: 0.6491 - val_acc: 0.7319\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.6052 - acc: 0.7592 - val_loss: 0.6311 - val_acc: 0.7400\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.5728 - acc: 0.7773 - val_loss: 0.5987 - val_acc: 0.7577\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.5447 - acc: 0.7867 - val_loss: 0.5895 - val_acc: 0.7594\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.5044 - acc: 0.8089 - val_loss: 0.5618 - val_acc: 0.7758\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 56us/step - loss: 0.4702 - acc: 0.8233 - val_loss: 0.5457 - val_acc: 0.7809\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 56us/step - loss: 0.4414 - acc: 0.8368 - val_loss: 0.5322 - val_acc: 0.7852\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.4110 - acc: 0.8473 - val_loss: 0.5125 - val_acc: 0.7962\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.3831 - acc: 0.8616 - val_loss: 0.4976 - val_acc: 0.8054\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.3548 - acc: 0.8750 - val_loss: 0.4842 - val_acc: 0.8121\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.3280 - acc: 0.8848 - val_loss: 0.4809 - val_acc: 0.8149\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.3029 - acc: 0.8931 - val_loss: 0.4648 - val_acc: 0.8205\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.2825 - acc: 0.9047 - val_loss: 0.4568 - val_acc: 0.8243\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.2603 - acc: 0.9114 - val_loss: 0.4475 - val_acc: 0.8253\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.2377 - acc: 0.9212 - val_loss: 0.4410 - val_acc: 0.8279\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 57us/step - loss: 0.2252 - acc: 0.9247 - val_loss: 0.4361 - val_acc: 0.8289\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.2011 - acc: 0.9370 - val_loss: 0.4284 - val_acc: 0.8330\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1875 - acc: 0.9380 - val_loss: 0.4225 - val_acc: 0.8355\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1710 - acc: 0.9460 - val_loss: 0.4195 - val_acc: 0.8361\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 56us/step - loss: 0.1567 - acc: 0.9503 - val_loss: 0.4201 - val_acc: 0.8378\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1488 - acc: 0.9550 - val_loss: 0.4160 - val_acc: 0.8394\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1327 - acc: 0.9604 - val_loss: 0.4132 - val_acc: 0.8404\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1252 - acc: 0.9630 - val_loss: 0.4193 - val_acc: 0.8384\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1127 - acc: 0.9680 - val_loss: 0.4128 - val_acc: 0.8442\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1054 - acc: 0.9704 - val_loss: 0.4141 - val_acc: 0.8440\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0994 - acc: 0.9713 - val_loss: 0.4127 - val_acc: 0.8450\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0908 - acc: 0.9744 - val_loss: 0.4148 - val_acc: 0.8465\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0812 - acc: 0.9765 - val_loss: 0.4196 - val_acc: 0.8493\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0786 - acc: 0.9776 - val_loss: 0.4162 - val_acc: 0.8478\n",
      "15663/15663 [==============================] - 0s 27us/step\n",
      "====== Training loss, score are: 0.04184272022462284 0.9925940113643619 =======\n",
      "3916/3916 [==============================] - 0s 27us/step\n",
      "===== CV loss, score are: 0.41617487559705757 0.8478038815726299 =======\n",
      "Running fold 2\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 158, 32)           2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_25  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 4,949,611\n",
      "Trainable params: 4,949,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 1s 56us/step\n",
      "Before training loss, score are: 1.2340137674107845 0.31047692013309736\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 84us/step - loss: 1.0597 - acc: 0.4475 - val_loss: 0.9445 - val_acc: 0.5873\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.9343 - acc: 0.5609 - val_loss: 0.8813 - val_acc: 0.5996\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.8806 - acc: 0.6004 - val_loss: 0.8379 - val_acc: 0.6310\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.8433 - acc: 0.6203 - val_loss: 0.8119 - val_acc: 0.6433\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.8130 - acc: 0.6430 - val_loss: 0.7843 - val_acc: 0.6581\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.7804 - acc: 0.6656 - val_loss: 0.7633 - val_acc: 0.6752\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.7446 - acc: 0.6833 - val_loss: 0.7448 - val_acc: 0.6851\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.7174 - acc: 0.6983 - val_loss: 0.7173 - val_acc: 0.6966\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.6809 - acc: 0.7179 - val_loss: 0.7027 - val_acc: 0.7094\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.6570 - acc: 0.7299 - val_loss: 0.6719 - val_acc: 0.7199\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.6172 - acc: 0.7513 - val_loss: 0.6527 - val_acc: 0.7298\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.5819 - acc: 0.7700 - val_loss: 0.6290 - val_acc: 0.7423\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.5448 - acc: 0.7889 - val_loss: 0.6068 - val_acc: 0.7546\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 56us/step - loss: 0.5130 - acc: 0.8023 - val_loss: 0.5871 - val_acc: 0.7666\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.4713 - acc: 0.8212 - val_loss: 0.5720 - val_acc: 0.7697\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.4409 - acc: 0.8343 - val_loss: 0.5618 - val_acc: 0.7817\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.4115 - acc: 0.8485 - val_loss: 0.5346 - val_acc: 0.7924\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.3792 - acc: 0.8623 - val_loss: 0.5223 - val_acc: 0.8023\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.3543 - acc: 0.8742 - val_loss: 0.5107 - val_acc: 0.8054\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 56us/step - loss: 0.3281 - acc: 0.8832 - val_loss: 0.4957 - val_acc: 0.8133\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.2965 - acc: 0.8969 - val_loss: 0.4828 - val_acc: 0.8154\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.2783 - acc: 0.9026 - val_loss: 0.4726 - val_acc: 0.8202\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.2551 - acc: 0.9114 - val_loss: 0.4640 - val_acc: 0.8243\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.2382 - acc: 0.9200 - val_loss: 0.4602 - val_acc: 0.8253\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.2149 - acc: 0.9293 - val_loss: 0.4504 - val_acc: 0.8312\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1959 - acc: 0.9369 - val_loss: 0.4467 - val_acc: 0.8327\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1816 - acc: 0.9411 - val_loss: 0.4470 - val_acc: 0.8338\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1649 - acc: 0.9485 - val_loss: 0.4389 - val_acc: 0.8330\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1543 - acc: 0.9508 - val_loss: 0.4391 - val_acc: 0.8340\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1426 - acc: 0.9549 - val_loss: 0.4332 - val_acc: 0.8394\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 56us/step - loss: 0.1314 - acc: 0.9600 - val_loss: 0.4333 - val_acc: 0.8381\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 56us/step - loss: 0.1178 - acc: 0.9638 - val_loss: 0.4329 - val_acc: 0.8391\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1137 - acc: 0.9652 - val_loss: 0.4316 - val_acc: 0.8409\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.1037 - acc: 0.9697 - val_loss: 0.4334 - val_acc: 0.8404\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0988 - acc: 0.9712 - val_loss: 0.4320 - val_acc: 0.8437\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0900 - acc: 0.9743 - val_loss: 0.4315 - val_acc: 0.8460\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0862 - acc: 0.9753 - val_loss: 0.4315 - val_acc: 0.8463\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0771 - acc: 0.9793 - val_loss: 0.4374 - val_acc: 0.8465\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.0738 - acc: 0.9796 - val_loss: 0.4373 - val_acc: 0.8447\n",
      "15663/15663 [==============================] - 0s 26us/step\n",
      "====== Training loss, score are: 0.04190364783788058 0.9927855455532146 =======\n",
      "3916/3916 [==============================] - 0s 27us/step\n",
      "===== CV loss, score are: 0.43734352934470094 0.8447395301327886 =======\n",
      "Running fold 3\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 158, 32)           2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_26  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 4,949,611\n",
      "Trainable params: 4,949,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 54us/step\n",
      "Before training loss, score are: 1.3637523977059192 0.4042648279612863\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 85us/step - loss: 1.0693 - acc: 0.4401 - val_loss: 0.9585 - val_acc: 0.5679\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.9407 - acc: 0.5590 - val_loss: 0.8834 - val_acc: 0.6182\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.8857 - acc: 0.5945 - val_loss: 0.8449 - val_acc: 0.6323\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.8476 - acc: 0.6179 - val_loss: 0.8176 - val_acc: 0.6466\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.8164 - acc: 0.6413 - val_loss: 0.7877 - val_acc: 0.6647\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.7820 - acc: 0.6640 - val_loss: 0.7645 - val_acc: 0.6780\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.7509 - acc: 0.6781 - val_loss: 0.7394 - val_acc: 0.6928\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.7209 - acc: 0.6985 - val_loss: 0.7174 - val_acc: 0.7086\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.6901 - acc: 0.7141 - val_loss: 0.6941 - val_acc: 0.7158\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.6511 - acc: 0.7361 - val_loss: 0.6733 - val_acc: 0.7265\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.6188 - acc: 0.7530 - val_loss: 0.6493 - val_acc: 0.7400\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.5821 - acc: 0.7682 - val_loss: 0.6277 - val_acc: 0.7459\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.5517 - acc: 0.7846 - val_loss: 0.6093 - val_acc: 0.7572\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.5166 - acc: 0.7998 - val_loss: 0.5870 - val_acc: 0.7666\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.4777 - acc: 0.8174 - val_loss: 0.5687 - val_acc: 0.7778\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.4445 - acc: 0.8356 - val_loss: 0.5523 - val_acc: 0.7850\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.4137 - acc: 0.8477 - val_loss: 0.5370 - val_acc: 0.7901\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.3878 - acc: 0.8597 - val_loss: 0.5162 - val_acc: 0.7990\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.3527 - acc: 0.8761 - val_loss: 0.5020 - val_acc: 0.8075\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.3271 - acc: 0.8862 - val_loss: 0.4906 - val_acc: 0.8075\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.2991 - acc: 0.8980 - val_loss: 0.4756 - val_acc: 0.8154\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 56us/step - loss: 0.2725 - acc: 0.9063 - val_loss: 0.4673 - val_acc: 0.8200\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.2530 - acc: 0.9146 - val_loss: 0.4577 - val_acc: 0.8238\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.2325 - acc: 0.9252 - val_loss: 0.4522 - val_acc: 0.8292\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.2143 - acc: 0.9273 - val_loss: 0.4420 - val_acc: 0.8322\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1912 - acc: 0.9403 - val_loss: 0.4351 - val_acc: 0.8355\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1832 - acc: 0.9406 - val_loss: 0.4408 - val_acc: 0.8340\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1622 - acc: 0.9515 - val_loss: 0.4320 - val_acc: 0.8396\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1559 - acc: 0.9509 - val_loss: 0.4235 - val_acc: 0.8417\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1398 - acc: 0.9582 - val_loss: 0.4219 - val_acc: 0.8450\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1309 - acc: 0.9607 - val_loss: 0.4191 - val_acc: 0.8468\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1197 - acc: 0.9639 - val_loss: 0.4166 - val_acc: 0.8463\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1130 - acc: 0.9672 - val_loss: 0.4150 - val_acc: 0.8504\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1012 - acc: 0.9731 - val_loss: 0.4164 - val_acc: 0.8455\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0959 - acc: 0.9719 - val_loss: 0.4139 - val_acc: 0.8509\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.0871 - acc: 0.9759 - val_loss: 0.4132 - val_acc: 0.8511\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0819 - acc: 0.9771 - val_loss: 0.4156 - val_acc: 0.8521\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.0744 - acc: 0.9808 - val_loss: 0.4188 - val_acc: 0.8501\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.0693 - acc: 0.9805 - val_loss: 0.4174 - val_acc: 0.8524\n",
      "15663/15663 [==============================] - 0s 26us/step\n",
      "====== Training loss, score are: 0.037740302610646974 0.9936793717678606 =======\n",
      "3916/3916 [==============================] - 0s 26us/step\n",
      "===== CV loss, score are: 0.41739493480620027 0.8524004085193007 =======\n",
      "Running fold 4\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_27 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 158, 32)           2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_27  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 4,949,611\n",
      "Trainable params: 4,949,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 55us/step\n",
      "Before training loss, score are: 1.3859350388133647 0.3248419842999047\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 87us/step - loss: 1.0710 - acc: 0.4370 - val_loss: 0.9456 - val_acc: 0.5794\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.9395 - acc: 0.5524 - val_loss: 0.8759 - val_acc: 0.6144\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.8860 - acc: 0.5933 - val_loss: 0.8375 - val_acc: 0.6328\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.8489 - acc: 0.6234 - val_loss: 0.8016 - val_acc: 0.6547\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.8186 - acc: 0.6423 - val_loss: 0.7773 - val_acc: 0.6673\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.7815 - acc: 0.6636 - val_loss: 0.7530 - val_acc: 0.6795\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.7553 - acc: 0.6771 - val_loss: 0.7295 - val_acc: 0.6987\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.7207 - acc: 0.6991 - val_loss: 0.7033 - val_acc: 0.7066\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.6843 - acc: 0.7188 - val_loss: 0.6765 - val_acc: 0.7219\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.6516 - acc: 0.7326 - val_loss: 0.6557 - val_acc: 0.7314\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.6113 - acc: 0.7585 - val_loss: 0.6282 - val_acc: 0.7454\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.5773 - acc: 0.7724 - val_loss: 0.6079 - val_acc: 0.7572\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.5397 - acc: 0.7896 - val_loss: 0.5848 - val_acc: 0.7648\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.5088 - acc: 0.8071 - val_loss: 0.5646 - val_acc: 0.7758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.4691 - acc: 0.8248 - val_loss: 0.5436 - val_acc: 0.7855\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.4380 - acc: 0.8394 - val_loss: 0.5269 - val_acc: 0.7932\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.4060 - acc: 0.8495 - val_loss: 0.5130 - val_acc: 0.7993\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.3725 - acc: 0.8679 - val_loss: 0.4950 - val_acc: 0.8062\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.3494 - acc: 0.8759 - val_loss: 0.4809 - val_acc: 0.8156\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.3189 - acc: 0.8849 - val_loss: 0.4705 - val_acc: 0.8192\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.2934 - acc: 0.9008 - val_loss: 0.4579 - val_acc: 0.8258\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.2710 - acc: 0.9069 - val_loss: 0.4495 - val_acc: 0.8223\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.2488 - acc: 0.9182 - val_loss: 0.4399 - val_acc: 0.8330\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.2295 - acc: 0.9242 - val_loss: 0.4335 - val_acc: 0.8338\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 54us/step - loss: 0.2101 - acc: 0.9319 - val_loss: 0.4280 - val_acc: 0.8332\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1901 - acc: 0.9391 - val_loss: 0.4211 - val_acc: 0.8361\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1788 - acc: 0.9421 - val_loss: 0.4191 - val_acc: 0.8407\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1625 - acc: 0.9506 - val_loss: 0.4138 - val_acc: 0.8435\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1540 - acc: 0.9512 - val_loss: 0.4110 - val_acc: 0.8435\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1386 - acc: 0.9560 - val_loss: 0.4084 - val_acc: 0.8455\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1261 - acc: 0.9618 - val_loss: 0.4094 - val_acc: 0.8463\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1208 - acc: 0.9639 - val_loss: 0.4063 - val_acc: 0.8478\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.1068 - acc: 0.9693 - val_loss: 0.4062 - val_acc: 0.8465\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 56us/step - loss: 0.1023 - acc: 0.9698 - val_loss: 0.4114 - val_acc: 0.8442\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 56us/step - loss: 0.0959 - acc: 0.9724 - val_loss: 0.4090 - val_acc: 0.8478\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 55us/step - loss: 0.0867 - acc: 0.9754 - val_loss: 0.4094 - val_acc: 0.8491\n",
      "15663/15663 [==============================] - 0s 27us/step\n",
      "====== Training loss, score are: 0.05056679517277832 0.989465619613101 =======\n",
      "3916/3916 [==============================] - 0s 28us/step\n",
      "===== CV loss, score are: 0.4093880111368733 0.8490806946471957 =======\n",
      "Running fold 5\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_28 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 158, 32)           2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_28  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 4,949,611\n",
      "Trainable params: 4,949,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15664/15664 [==============================] - 1s 57us/step\n",
      "Before training loss, score are: 1.7870014084006476 0.28919816138917265\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/150\n",
      "15664/15664 [==============================] - 1s 88us/step - loss: 1.0962 - acc: 0.4301 - val_loss: 0.9670 - val_acc: 0.5739\n",
      "Epoch 2/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.9525 - acc: 0.5466 - val_loss: 0.8780 - val_acc: 0.6255\n",
      "Epoch 3/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.8963 - acc: 0.5884 - val_loss: 0.8360 - val_acc: 0.6450\n",
      "Epoch 4/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.8595 - acc: 0.6129 - val_loss: 0.8071 - val_acc: 0.6572\n",
      "Epoch 5/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.8282 - acc: 0.6345 - val_loss: 0.7786 - val_acc: 0.6754\n",
      "Epoch 6/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.7948 - acc: 0.6513 - val_loss: 0.7542 - val_acc: 0.6879\n",
      "Epoch 7/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.7661 - acc: 0.6657 - val_loss: 0.7306 - val_acc: 0.6989\n",
      "Epoch 8/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.7303 - acc: 0.6923 - val_loss: 0.7074 - val_acc: 0.7147\n",
      "Epoch 9/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.6990 - acc: 0.7074 - val_loss: 0.6822 - val_acc: 0.7318\n",
      "Epoch 10/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.6646 - acc: 0.7270 - val_loss: 0.6594 - val_acc: 0.7428\n",
      "Epoch 11/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.6329 - acc: 0.7446 - val_loss: 0.6356 - val_acc: 0.7510\n",
      "Epoch 12/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.5953 - acc: 0.7637 - val_loss: 0.6144 - val_acc: 0.7655\n",
      "Epoch 13/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.5653 - acc: 0.7794 - val_loss: 0.5902 - val_acc: 0.7722\n",
      "Epoch 14/150\n",
      "15664/15664 [==============================] - 1s 56us/step - loss: 0.5312 - acc: 0.7925 - val_loss: 0.5735 - val_acc: 0.7808\n",
      "Epoch 15/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.4951 - acc: 0.8086 - val_loss: 0.5496 - val_acc: 0.7890\n",
      "Epoch 16/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.4640 - acc: 0.8269 - val_loss: 0.5323 - val_acc: 0.7957\n",
      "Epoch 17/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.4304 - acc: 0.8433 - val_loss: 0.5195 - val_acc: 0.8003\n",
      "Epoch 18/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.3997 - acc: 0.8531 - val_loss: 0.5027 - val_acc: 0.8036\n",
      "Epoch 19/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.3671 - acc: 0.8696 - val_loss: 0.4855 - val_acc: 0.8146\n",
      "Epoch 20/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.3451 - acc: 0.8781 - val_loss: 0.4756 - val_acc: 0.8156\n",
      "Epoch 21/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.3175 - acc: 0.8900 - val_loss: 0.4664 - val_acc: 0.8232\n",
      "Epoch 22/150\n",
      "15664/15664 [==============================] - 1s 54us/step - loss: 0.2876 - acc: 0.9014 - val_loss: 0.4533 - val_acc: 0.8258\n",
      "Epoch 23/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.2702 - acc: 0.9090 - val_loss: 0.4500 - val_acc: 0.8281\n",
      "Epoch 24/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.2497 - acc: 0.9145 - val_loss: 0.4383 - val_acc: 0.8345\n",
      "Epoch 25/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.2297 - acc: 0.9227 - val_loss: 0.4338 - val_acc: 0.8355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.2123 - acc: 0.9306 - val_loss: 0.4327 - val_acc: 0.8398\n",
      "Epoch 27/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.1920 - acc: 0.9385 - val_loss: 0.4232 - val_acc: 0.8444\n",
      "Epoch 28/150\n",
      "15664/15664 [==============================] - 1s 56us/step - loss: 0.1789 - acc: 0.9447 - val_loss: 0.4201 - val_acc: 0.8457\n",
      "Epoch 29/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.1637 - acc: 0.9484 - val_loss: 0.4171 - val_acc: 0.8480\n",
      "Epoch 30/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.1540 - acc: 0.9510 - val_loss: 0.4192 - val_acc: 0.8452\n",
      "Epoch 31/150\n",
      "15664/15664 [==============================] - 1s 56us/step - loss: 0.1408 - acc: 0.9567 - val_loss: 0.4155 - val_acc: 0.8470\n",
      "Epoch 32/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.1311 - acc: 0.9606 - val_loss: 0.4109 - val_acc: 0.8516\n",
      "Epoch 33/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.1235 - acc: 0.9623 - val_loss: 0.4145 - val_acc: 0.8473\n",
      "Epoch 34/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.1119 - acc: 0.9674 - val_loss: 0.4101 - val_acc: 0.8536\n",
      "Epoch 35/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.1029 - acc: 0.9706 - val_loss: 0.4100 - val_acc: 0.8534\n",
      "Epoch 36/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.0987 - acc: 0.9714 - val_loss: 0.4118 - val_acc: 0.8529\n",
      "Epoch 37/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.0899 - acc: 0.9744 - val_loss: 0.4141 - val_acc: 0.8549\n",
      "Epoch 38/150\n",
      "15664/15664 [==============================] - 1s 55us/step - loss: 0.0830 - acc: 0.9770 - val_loss: 0.4142 - val_acc: 0.8552\n",
      "15664/15664 [==============================] - 0s 28us/step\n",
      "====== Training loss, score are: 0.046418898252806456 0.9911261491317671 =======\n",
      "3915/3915 [==============================] - 0s 28us/step\n",
      "===== CV loss, score are: 0.4142267006224599 0.8551724138692268 =======\n",
      "\n",
      "\n",
      "===== Model: cnn_gensim  ========:\n",
      " Cross-val log losses are: [0.41617487087517757, 0.43917219240889271, 0.42064044705858999, 0.41159092715723844, 0.41786453428926779]\n",
      "====== Mean cross-val log loss is: 0.4210885943578333 =========\n",
      "\n",
      "\n",
      "Timestamp: 2018-Jan-14 03:07:31\n",
      "Running kfold training with model cnn_glove\n",
      "Shapes: x_train_raw.shape (19579, 33), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 32)\n",
      "Running preprocess: <function run_NN_preprocess at 0x7f620ee8eea0>\n",
      "No of unique words found is 247346\n",
      "No of words occuring more than 2 times is 75104\n",
      "No of unique words found is after capping is 247346\n",
      "max sentence length is 1439\n",
      "mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000\n",
      "After padding: max sentence size is 161\n",
      "After padding: mean, median length of sentence is 161.000000 161.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5053it [00:00, 50520.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word embedding matrix for glove vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:08, 49567.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "of--mirth 247345\n",
      "None\n",
      "(247346, 100)\n",
      "Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)\n",
      "Running fold 1\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 158, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_29  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 24,747,531\n",
      "Trainable params: 12,931\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 68us/step\n",
      "Before training loss, score are: 1.1105597337026107 0.2910042776102034\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 84us/step - loss: 1.0482 - acc: 0.4597 - val_loss: 0.9916 - val_acc: 0.5444\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.9595 - acc: 0.5519 - val_loss: 0.9194 - val_acc: 0.5822\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.9139 - acc: 0.5826 - val_loss: 0.8771 - val_acc: 0.6267\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8864 - acc: 0.5991 - val_loss: 0.8543 - val_acc: 0.6417\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8616 - acc: 0.6165 - val_loss: 0.8289 - val_acc: 0.6532\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8449 - acc: 0.6237 - val_loss: 0.8141 - val_acc: 0.6629\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8329 - acc: 0.6311 - val_loss: 0.8029 - val_acc: 0.6670\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8271 - acc: 0.6356 - val_loss: 0.7959 - val_acc: 0.6657\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8135 - acc: 0.6437 - val_loss: 0.7851 - val_acc: 0.6698\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8068 - acc: 0.6450 - val_loss: 0.7763 - val_acc: 0.6775\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8029 - acc: 0.6480 - val_loss: 0.7728 - val_acc: 0.6757\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7925 - acc: 0.6508 - val_loss: 0.7700 - val_acc: 0.6747\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7906 - acc: 0.6576 - val_loss: 0.7651 - val_acc: 0.6772\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7841 - acc: 0.6581 - val_loss: 0.7667 - val_acc: 0.6713\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7810 - acc: 0.6598 - val_loss: 0.7530 - val_acc: 0.6803\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7756 - acc: 0.6614 - val_loss: 0.7508 - val_acc: 0.6826\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7724 - acc: 0.6624 - val_loss: 0.7479 - val_acc: 0.6874\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7724 - acc: 0.6621 - val_loss: 0.7457 - val_acc: 0.6844\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7681 - acc: 0.6616 - val_loss: 0.7417 - val_acc: 0.6851\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7655 - acc: 0.6665 - val_loss: 0.7385 - val_acc: 0.6859\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7593 - acc: 0.6738 - val_loss: 0.7356 - val_acc: 0.6862\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7554 - acc: 0.6715 - val_loss: 0.7342 - val_acc: 0.6872\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7550 - acc: 0.6733 - val_loss: 0.7308 - val_acc: 0.6900\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7500 - acc: 0.6759 - val_loss: 0.7252 - val_acc: 0.6879\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7449 - acc: 0.6766 - val_loss: 0.7237 - val_acc: 0.6928\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7449 - acc: 0.6790 - val_loss: 0.7219 - val_acc: 0.6933\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7460 - acc: 0.6768 - val_loss: 0.7249 - val_acc: 0.6902\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7441 - acc: 0.6782 - val_loss: 0.7179 - val_acc: 0.6948\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7403 - acc: 0.6788 - val_loss: 0.7149 - val_acc: 0.6982\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7416 - acc: 0.6817 - val_loss: 0.7172 - val_acc: 0.6951\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7385 - acc: 0.6805 - val_loss: 0.7170 - val_acc: 0.6931\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7380 - acc: 0.6815 - val_loss: 0.7132 - val_acc: 0.6977\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7328 - acc: 0.6827 - val_loss: 0.7079 - val_acc: 0.7002\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7276 - acc: 0.6873 - val_loss: 0.7121 - val_acc: 0.6954\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7321 - acc: 0.6826 - val_loss: 0.7084 - val_acc: 0.7010\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7295 - acc: 0.6836 - val_loss: 0.7077 - val_acc: 0.7020\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7304 - acc: 0.6812 - val_loss: 0.7093 - val_acc: 0.7053\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7243 - acc: 0.6842 - val_loss: 0.7098 - val_acc: 0.7025\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7263 - acc: 0.6849 - val_loss: 0.7028 - val_acc: 0.7081\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7196 - acc: 0.6919 - val_loss: 0.7001 - val_acc: 0.7040\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7221 - acc: 0.6898 - val_loss: 0.7020 - val_acc: 0.7035\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7221 - acc: 0.6905 - val_loss: 0.7008 - val_acc: 0.7045\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7178 - acc: 0.6898 - val_loss: 0.6955 - val_acc: 0.7053\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7173 - acc: 0.6895 - val_loss: 0.6979 - val_acc: 0.7028\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 0s 24us/step - loss: 0.7179 - acc: 0.6883 - val_loss: 0.6975 - val_acc: 0.7076\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7110 - acc: 0.6958 - val_loss: 0.6928 - val_acc: 0.7063\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7133 - acc: 0.6937 - val_loss: 0.6904 - val_acc: 0.7074\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7128 - acc: 0.6948 - val_loss: 0.6951 - val_acc: 0.7084\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7133 - acc: 0.6915 - val_loss: 0.6897 - val_acc: 0.7127\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7124 - acc: 0.6917 - val_loss: 0.6925 - val_acc: 0.7089\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7115 - acc: 0.6961 - val_loss: 0.6917 - val_acc: 0.7094\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7076 - acc: 0.6918 - val_loss: 0.7023 - val_acc: 0.7045\n",
      "15663/15663 [==============================] - 0s 27us/step\n",
      "====== Training loss, score are: 0.6544604023160632 0.7287237438701666 =======\n",
      "3916/3916 [==============================] - 0s 41us/step\n",
      "===== CV loss, score are: 0.7023483652990612 0.7045454545454546 =======\n",
      "Running fold 2\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 158, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_30  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 24,747,531\n",
      "Trainable params: 12,931\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 60us/step\n",
      "Before training loss, score are: 1.1185543177221147 0.2866628359857437\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 61us/step - loss: 1.0494 - acc: 0.4610 - val_loss: 0.9967 - val_acc: 0.5033\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.9605 - acc: 0.5561 - val_loss: 0.9207 - val_acc: 0.5914\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 0s 27us/step - loss: 0.9091 - acc: 0.5893 - val_loss: 0.8775 - val_acc: 0.6193\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 0s 28us/step - loss: 0.8788 - acc: 0.6067 - val_loss: 0.8553 - val_acc: 0.6310\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8596 - acc: 0.6164 - val_loss: 0.8407 - val_acc: 0.6366\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8393 - acc: 0.6278 - val_loss: 0.8184 - val_acc: 0.6461\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8298 - acc: 0.6297 - val_loss: 0.8204 - val_acc: 0.6491\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.8183 - acc: 0.6363 - val_loss: 0.7971 - val_acc: 0.6596\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8099 - acc: 0.6430 - val_loss: 0.7919 - val_acc: 0.6555\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8022 - acc: 0.6443 - val_loss: 0.7827 - val_acc: 0.6596\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7956 - acc: 0.6521 - val_loss: 0.7844 - val_acc: 0.6583\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7886 - acc: 0.6510 - val_loss: 0.7722 - val_acc: 0.6680\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7883 - acc: 0.6577 - val_loss: 0.7653 - val_acc: 0.6749\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7818 - acc: 0.6604 - val_loss: 0.7667 - val_acc: 0.6719\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7758 - acc: 0.6602 - val_loss: 0.7591 - val_acc: 0.6752\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7765 - acc: 0.6636 - val_loss: 0.7539 - val_acc: 0.6808\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7708 - acc: 0.6638 - val_loss: 0.7537 - val_acc: 0.6742\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7690 - acc: 0.6612 - val_loss: 0.7519 - val_acc: 0.6826\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7666 - acc: 0.6640 - val_loss: 0.7429 - val_acc: 0.6882\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7646 - acc: 0.6666 - val_loss: 0.7452 - val_acc: 0.6800\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7550 - acc: 0.6695 - val_loss: 0.7386 - val_acc: 0.6849\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7609 - acc: 0.6718 - val_loss: 0.7352 - val_acc: 0.6877\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7499 - acc: 0.6733 - val_loss: 0.7328 - val_acc: 0.6864\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7516 - acc: 0.6757 - val_loss: 0.7297 - val_acc: 0.6956\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7480 - acc: 0.6759 - val_loss: 0.7268 - val_acc: 0.6918\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7486 - acc: 0.6754 - val_loss: 0.7259 - val_acc: 0.6925\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7436 - acc: 0.6821 - val_loss: 0.7223 - val_acc: 0.6951\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7414 - acc: 0.6776 - val_loss: 0.7213 - val_acc: 0.6966\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7430 - acc: 0.6769 - val_loss: 0.7254 - val_acc: 0.6946\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7411 - acc: 0.6752 - val_loss: 0.7242 - val_acc: 0.6959\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7318 - acc: 0.6863 - val_loss: 0.7229 - val_acc: 0.6900\n",
      "15663/15663 [==============================] - 0s 27us/step\n",
      "====== Training loss, score are: 0.6886357561173372 0.7111025985033281 =======\n",
      "3916/3916 [==============================] - 0s 29us/step\n",
      "===== CV loss, score are: 0.7228803083650669 0.6899897855562866 =======\n",
      "Running fold 3\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_31 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 158, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_31  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 24,747,531\n",
      "Trainable params: 12,931\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 1s 63us/step\n",
      "Before training loss, score are: 1.1242099555829685 0.28768435166343026\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 64us/step - loss: 1.0430 - acc: 0.4710 - val_loss: 0.9837 - val_acc: 0.5197\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.9544 - acc: 0.5609 - val_loss: 0.9116 - val_acc: 0.6062\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.9069 - acc: 0.5918 - val_loss: 0.8720 - val_acc: 0.6147\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8751 - acc: 0.6112 - val_loss: 0.8502 - val_acc: 0.6279\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8557 - acc: 0.6189 - val_loss: 0.8297 - val_acc: 0.6435\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8382 - acc: 0.6316 - val_loss: 0.8157 - val_acc: 0.6448\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8256 - acc: 0.6391 - val_loss: 0.8032 - val_acc: 0.6504\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8153 - acc: 0.6411 - val_loss: 0.7987 - val_acc: 0.6525\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8100 - acc: 0.6450 - val_loss: 0.7895 - val_acc: 0.6565\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8027 - acc: 0.6466 - val_loss: 0.7892 - val_acc: 0.6581\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7984 - acc: 0.6531 - val_loss: 0.7760 - val_acc: 0.6678\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7876 - acc: 0.6574 - val_loss: 0.7750 - val_acc: 0.6568\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7846 - acc: 0.6543 - val_loss: 0.7685 - val_acc: 0.6719\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7810 - acc: 0.6589 - val_loss: 0.7616 - val_acc: 0.6729\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7762 - acc: 0.6621 - val_loss: 0.7581 - val_acc: 0.6754\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7746 - acc: 0.6633 - val_loss: 0.7544 - val_acc: 0.6754\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7725 - acc: 0.6598 - val_loss: 0.7506 - val_acc: 0.6770\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7641 - acc: 0.6686 - val_loss: 0.7546 - val_acc: 0.6701\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7597 - acc: 0.6706 - val_loss: 0.7498 - val_acc: 0.6752\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 0s 24us/step - loss: 0.7558 - acc: 0.6724 - val_loss: 0.7432 - val_acc: 0.6785\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7585 - acc: 0.6698 - val_loss: 0.7477 - val_acc: 0.6736\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7548 - acc: 0.6751 - val_loss: 0.7418 - val_acc: 0.6803\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7486 - acc: 0.6801 - val_loss: 0.7340 - val_acc: 0.6874\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7452 - acc: 0.6771 - val_loss: 0.7323 - val_acc: 0.6849\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7462 - acc: 0.6785 - val_loss: 0.7328 - val_acc: 0.6862\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7427 - acc: 0.6808 - val_loss: 0.7280 - val_acc: 0.6885\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7341 - acc: 0.6835 - val_loss: 0.7290 - val_acc: 0.6864\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7366 - acc: 0.6852 - val_loss: 0.7288 - val_acc: 0.6943\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7355 - acc: 0.6835 - val_loss: 0.7214 - val_acc: 0.6923\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7414 - acc: 0.6798 - val_loss: 0.7219 - val_acc: 0.6948\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7360 - acc: 0.6824 - val_loss: 0.7178 - val_acc: 0.6931\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7354 - acc: 0.6810 - val_loss: 0.7257 - val_acc: 0.6834\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7318 - acc: 0.6854 - val_loss: 0.7157 - val_acc: 0.6987\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7248 - acc: 0.6854 - val_loss: 0.7145 - val_acc: 0.6928\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7283 - acc: 0.6884 - val_loss: 0.7158 - val_acc: 0.6997\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7308 - acc: 0.6893 - val_loss: 0.7107 - val_acc: 0.6974\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7233 - acc: 0.6870 - val_loss: 0.7087 - val_acc: 0.6994\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7228 - acc: 0.6918 - val_loss: 0.7078 - val_acc: 0.7017\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7182 - acc: 0.6928 - val_loss: 0.7061 - val_acc: 0.7012\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7258 - acc: 0.6892 - val_loss: 0.7041 - val_acc: 0.7028\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7220 - acc: 0.6919 - val_loss: 0.7050 - val_acc: 0.6992\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7219 - acc: 0.6930 - val_loss: 0.7017 - val_acc: 0.7025\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7170 - acc: 0.6927 - val_loss: 0.7056 - val_acc: 0.6987\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7149 - acc: 0.6911 - val_loss: 0.7029 - val_acc: 0.7015\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7082 - acc: 0.6963 - val_loss: 0.6983 - val_acc: 0.7081\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7125 - acc: 0.6922 - val_loss: 0.6982 - val_acc: 0.6999\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7167 - acc: 0.6953 - val_loss: 0.7016 - val_acc: 0.6989\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7102 - acc: 0.6955 - val_loss: 0.6958 - val_acc: 0.7030\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7035 - acc: 0.7004 - val_loss: 0.7012 - val_acc: 0.6997\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7077 - acc: 0.6971 - val_loss: 0.6963 - val_acc: 0.7005\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7055 - acc: 0.6936 - val_loss: 0.6937 - val_acc: 0.7058\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6967 - acc: 0.7041 - val_loss: 0.6930 - val_acc: 0.7109\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6987 - acc: 0.7051 - val_loss: 0.6906 - val_acc: 0.7017\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7018 - acc: 0.6986 - val_loss: 0.6900 - val_acc: 0.7071\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6988 - acc: 0.7008 - val_loss: 0.6896 - val_acc: 0.7061\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6991 - acc: 0.7053 - val_loss: 0.6936 - val_acc: 0.7015\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6996 - acc: 0.7049 - val_loss: 0.6873 - val_acc: 0.7045\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6972 - acc: 0.7009 - val_loss: 0.6860 - val_acc: 0.7061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6985 - acc: 0.7053 - val_loss: 0.6841 - val_acc: 0.7132\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6976 - acc: 0.7032 - val_loss: 0.6907 - val_acc: 0.6994\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6942 - acc: 0.7076 - val_loss: 0.6825 - val_acc: 0.7068\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6932 - acc: 0.7055 - val_loss: 0.6836 - val_acc: 0.7099\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6968 - acc: 0.6968 - val_loss: 0.6820 - val_acc: 0.7071\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6971 - acc: 0.7031 - val_loss: 0.6808 - val_acc: 0.7063\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6911 - acc: 0.7050 - val_loss: 0.6805 - val_acc: 0.7091\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6939 - acc: 0.7020 - val_loss: 0.6781 - val_acc: 0.7104\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6842 - acc: 0.7089 - val_loss: 0.6801 - val_acc: 0.7071\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6859 - acc: 0.7053 - val_loss: 0.6795 - val_acc: 0.7153\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6897 - acc: 0.7087 - val_loss: 0.6782 - val_acc: 0.7127\n",
      "15663/15663 [==============================] - 0s 28us/step\n",
      "====== Training loss, score are: 0.6160765802396757 0.7513247781547864 =======\n",
      "3916/3916 [==============================] - 0s 28us/step\n",
      "===== CV loss, score are: 0.6781785657992767 0.7127170581922346 =======\n",
      "Running fold 4\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_32 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 158, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_32  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 24,747,531\n",
      "Trainable params: 12,931\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 62us/step\n",
      "Before training loss, score are: 1.1296464027759026 0.28934431462968163\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 63us/step - loss: 1.0543 - acc: 0.4539 - val_loss: 0.9894 - val_acc: 0.5735\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.9602 - acc: 0.5593 - val_loss: 0.9125 - val_acc: 0.6065\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.9113 - acc: 0.5860 - val_loss: 0.8711 - val_acc: 0.6256\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8774 - acc: 0.6049 - val_loss: 0.8465 - val_acc: 0.6336\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8580 - acc: 0.6167 - val_loss: 0.8175 - val_acc: 0.6563\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8480 - acc: 0.6245 - val_loss: 0.8053 - val_acc: 0.6673\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8320 - acc: 0.6358 - val_loss: 0.7944 - val_acc: 0.6637\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8241 - acc: 0.6352 - val_loss: 0.7907 - val_acc: 0.6652\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8132 - acc: 0.6414 - val_loss: 0.7760 - val_acc: 0.6701\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8044 - acc: 0.6464 - val_loss: 0.7739 - val_acc: 0.6706\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.8010 - acc: 0.6521 - val_loss: 0.7673 - val_acc: 0.6772\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7980 - acc: 0.6474 - val_loss: 0.7566 - val_acc: 0.6818\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7927 - acc: 0.6512 - val_loss: 0.7504 - val_acc: 0.6859\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7863 - acc: 0.6577 - val_loss: 0.7446 - val_acc: 0.6859\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7818 - acc: 0.6618 - val_loss: 0.7461 - val_acc: 0.6892\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7765 - acc: 0.6636 - val_loss: 0.7361 - val_acc: 0.6941\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7763 - acc: 0.6628 - val_loss: 0.7415 - val_acc: 0.6879\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7719 - acc: 0.6630 - val_loss: 0.7316 - val_acc: 0.6961\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7658 - acc: 0.6692 - val_loss: 0.7276 - val_acc: 0.6951\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7695 - acc: 0.6636 - val_loss: 0.7224 - val_acc: 0.6961\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7622 - acc: 0.6706 - val_loss: 0.7223 - val_acc: 0.7015\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7658 - acc: 0.6689 - val_loss: 0.7188 - val_acc: 0.7025\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7545 - acc: 0.6739 - val_loss: 0.7149 - val_acc: 0.6992\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7536 - acc: 0.6740 - val_loss: 0.7132 - val_acc: 0.6992\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7577 - acc: 0.6734 - val_loss: 0.7112 - val_acc: 0.7040\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7500 - acc: 0.6718 - val_loss: 0.7048 - val_acc: 0.7056\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7496 - acc: 0.6746 - val_loss: 0.7028 - val_acc: 0.7086\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7455 - acc: 0.6772 - val_loss: 0.7031 - val_acc: 0.7109\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7407 - acc: 0.6825 - val_loss: 0.6988 - val_acc: 0.7094\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7445 - acc: 0.6763 - val_loss: 0.6974 - val_acc: 0.7097\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7432 - acc: 0.6769 - val_loss: 0.7053 - val_acc: 0.7071\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7393 - acc: 0.6822 - val_loss: 0.6991 - val_acc: 0.7089\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7370 - acc: 0.6805 - val_loss: 0.6919 - val_acc: 0.7135\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7343 - acc: 0.6834 - val_loss: 0.6924 - val_acc: 0.7122\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7343 - acc: 0.6819 - val_loss: 0.6905 - val_acc: 0.7160\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7307 - acc: 0.6862 - val_loss: 0.6973 - val_acc: 0.7104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7271 - acc: 0.6882 - val_loss: 0.6869 - val_acc: 0.7153\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7234 - acc: 0.6895 - val_loss: 0.6836 - val_acc: 0.7186\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7300 - acc: 0.6895 - val_loss: 0.6896 - val_acc: 0.7158\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7258 - acc: 0.6900 - val_loss: 0.6831 - val_acc: 0.7183\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7238 - acc: 0.6900 - val_loss: 0.6819 - val_acc: 0.7176\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7181 - acc: 0.6907 - val_loss: 0.6758 - val_acc: 0.7188\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7196 - acc: 0.6911 - val_loss: 0.6770 - val_acc: 0.7201\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7191 - acc: 0.6893 - val_loss: 0.6767 - val_acc: 0.7229\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7205 - acc: 0.6921 - val_loss: 0.6754 - val_acc: 0.7245\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7167 - acc: 0.6888 - val_loss: 0.6772 - val_acc: 0.7181\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7122 - acc: 0.6958 - val_loss: 0.6735 - val_acc: 0.7273\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7138 - acc: 0.6928 - val_loss: 0.6715 - val_acc: 0.7224\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7125 - acc: 0.6928 - val_loss: 0.6687 - val_acc: 0.7237\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7123 - acc: 0.6949 - val_loss: 0.6665 - val_acc: 0.7252\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7119 - acc: 0.6919 - val_loss: 0.6686 - val_acc: 0.7285\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.7130 - acc: 0.6955 - val_loss: 0.6665 - val_acc: 0.7319\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7078 - acc: 0.6992 - val_loss: 0.6643 - val_acc: 0.7296\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7115 - acc: 0.6935 - val_loss: 0.6634 - val_acc: 0.7270\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7071 - acc: 0.6995 - val_loss: 0.6608 - val_acc: 0.7308\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7034 - acc: 0.6950 - val_loss: 0.6619 - val_acc: 0.7314\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7055 - acc: 0.6983 - val_loss: 0.6579 - val_acc: 0.7314\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 0s 26us/step - loss: 0.6996 - acc: 0.7015 - val_loss: 0.6583 - val_acc: 0.7326\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7040 - acc: 0.6965 - val_loss: 0.6564 - val_acc: 0.7303\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7036 - acc: 0.6980 - val_loss: 0.6574 - val_acc: 0.7357\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.6998 - acc: 0.7012 - val_loss: 0.6573 - val_acc: 0.7296\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 0s 25us/step - loss: 0.7021 - acc: 0.6967 - val_loss: 0.6613 - val_acc: 0.7303\n",
      "15663/15663 [==============================] - 0s 28us/step\n",
      "====== Training loss, score are: 0.6350062289495528 0.7438549447933361 =======\n",
      "3916/3916 [==============================] - 0s 29us/step\n",
      "===== CV loss, score are: 0.6612772687706445 0.7303370785908022 =======\n",
      "Running fold 5\n",
      "Creating CNN model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_33 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 158, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_33  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 24,747,531\n",
      "Trainable params: 12,931\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15664/15664 [==============================] - 1s 68us/step\n",
      "Before training loss, score are: 1.1106298339987921 0.31115934627170583\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/150\n",
      "15664/15664 [==============================] - 1s 83us/step - loss: 1.0462 - acc: 0.4755 - val_loss: 0.9780 - val_acc: 0.5709\n",
      "Epoch 2/150\n",
      "15664/15664 [==============================] - 0s 27us/step - loss: 0.9554 - acc: 0.5568 - val_loss: 0.9026 - val_acc: 0.6146\n",
      "Epoch 3/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.9077 - acc: 0.5877 - val_loss: 0.8619 - val_acc: 0.6276\n",
      "Epoch 4/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.8772 - acc: 0.6068 - val_loss: 0.8354 - val_acc: 0.6524\n",
      "Epoch 5/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.8568 - acc: 0.6212 - val_loss: 0.8180 - val_acc: 0.6572\n",
      "Epoch 6/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.8420 - acc: 0.6256 - val_loss: 0.8022 - val_acc: 0.6582\n",
      "Epoch 7/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.8311 - acc: 0.6328 - val_loss: 0.7981 - val_acc: 0.6720\n",
      "Epoch 8/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.8195 - acc: 0.6360 - val_loss: 0.7817 - val_acc: 0.6705\n",
      "Epoch 9/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.8130 - acc: 0.6401 - val_loss: 0.7781 - val_acc: 0.6802\n",
      "Epoch 10/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.8045 - acc: 0.6503 - val_loss: 0.7865 - val_acc: 0.6779\n",
      "Epoch 11/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.8011 - acc: 0.6502 - val_loss: 0.7761 - val_acc: 0.6799\n",
      "Epoch 12/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7931 - acc: 0.6500 - val_loss: 0.7676 - val_acc: 0.6861\n",
      "Epoch 13/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7860 - acc: 0.6563 - val_loss: 0.7566 - val_acc: 0.6899\n",
      "Epoch 14/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7869 - acc: 0.6526 - val_loss: 0.7548 - val_acc: 0.6932\n",
      "Epoch 15/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7827 - acc: 0.6559 - val_loss: 0.7483 - val_acc: 0.6927\n",
      "Epoch 16/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7762 - acc: 0.6604 - val_loss: 0.7538 - val_acc: 0.6940\n",
      "Epoch 17/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7710 - acc: 0.6626 - val_loss: 0.7381 - val_acc: 0.6948\n",
      "Epoch 18/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7715 - acc: 0.6604 - val_loss: 0.7372 - val_acc: 0.6966\n",
      "Epoch 19/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7668 - acc: 0.6652 - val_loss: 0.7332 - val_acc: 0.6991\n",
      "Epoch 20/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7609 - acc: 0.6662 - val_loss: 0.7324 - val_acc: 0.7006\n",
      "Epoch 21/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7592 - acc: 0.6689 - val_loss: 0.7319 - val_acc: 0.7017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7619 - acc: 0.6672 - val_loss: 0.7289 - val_acc: 0.7047\n",
      "Epoch 23/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7521 - acc: 0.6734 - val_loss: 0.7257 - val_acc: 0.7029\n",
      "Epoch 24/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7511 - acc: 0.6729 - val_loss: 0.7210 - val_acc: 0.7042\n",
      "Epoch 25/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7455 - acc: 0.6755 - val_loss: 0.7193 - val_acc: 0.7060\n",
      "Epoch 26/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7471 - acc: 0.6766 - val_loss: 0.7170 - val_acc: 0.7022\n",
      "Epoch 27/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7469 - acc: 0.6769 - val_loss: 0.7175 - val_acc: 0.7088\n",
      "Epoch 28/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7377 - acc: 0.6786 - val_loss: 0.7142 - val_acc: 0.7068\n",
      "Epoch 29/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7420 - acc: 0.6811 - val_loss: 0.7114 - val_acc: 0.7022\n",
      "Epoch 30/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7363 - acc: 0.6828 - val_loss: 0.7099 - val_acc: 0.7093\n",
      "Epoch 31/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7335 - acc: 0.6788 - val_loss: 0.7165 - val_acc: 0.7065\n",
      "Epoch 32/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7350 - acc: 0.6842 - val_loss: 0.7105 - val_acc: 0.7114\n",
      "Epoch 33/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7313 - acc: 0.6814 - val_loss: 0.7040 - val_acc: 0.7093\n",
      "Epoch 34/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7263 - acc: 0.6853 - val_loss: 0.7086 - val_acc: 0.7040\n",
      "Epoch 35/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7268 - acc: 0.6881 - val_loss: 0.6978 - val_acc: 0.7119\n",
      "Epoch 36/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7282 - acc: 0.6876 - val_loss: 0.7009 - val_acc: 0.7093\n",
      "Epoch 37/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7259 - acc: 0.6916 - val_loss: 0.6982 - val_acc: 0.7114\n",
      "Epoch 38/150\n",
      "15664/15664 [==============================] - 0s 25us/step - loss: 0.7204 - acc: 0.6915 - val_loss: 0.7007 - val_acc: 0.7109\n",
      "15664/15664 [==============================] - 0s 27us/step\n",
      "====== Training loss, score are: 0.6713878553174246 0.7183988764044944 =======\n",
      "3915/3915 [==============================] - 0s 46us/step\n",
      "===== CV loss, score are: 0.7007125627339876 0.7108556832542516 =======\n",
      "\n",
      "\n",
      "===== Model: cnn_glove  ========:\n",
      " Cross-val log losses are: [0.70234836755586638, 0.72288030508889789, 0.67817856567603885, 0.66127726744962534, 0.70071256569482632]\n",
      "====== Mean cross-val log loss is: 0.6930794142930509 =========\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CNN and add predictions as features:\n",
    "\n",
    "# No pre-trained vectors:\n",
    "print('Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "model_name = \"cnn_none\"\n",
    "model_preprocess_function = run_NN_preprocess\n",
    "model_function = run_CNN\n",
    "model_params = {'temp': 0 }\n",
    "model_preprocess_params = {'char_level': 0, 'word_vector_type': None }\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"NN\", \n",
    "                                           train_raw, y_train_one_hot, test_raw, 1) \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n",
    "# Pre-trained word2vec using vectors:\n",
    "print('Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "wordvecsize = 20\n",
    "create_gensim_wordvectors = 0\n",
    "if create_gensim_wordvectors:\n",
    "    create_gensim_wordvec(train_raw['text'], wordvecsize)\n",
    "model_name = \"cnn_gensim\"\n",
    "model_preprocess_function = run_NN_preprocess\n",
    "model_function = run_CNN\n",
    "model_params = {'temp': 0 }\n",
    "model_preprocess_params = {'char_level': 0, 'word_vector_type': \"gensim\" }\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"NN\", \n",
    "                                           train_raw, y_train_one_hot, test_raw, 1) \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n",
    "    \n",
    "# Glove vectors:\n",
    "print('Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "model_name = \"cnn_glove\"\n",
    "model_preprocess_function = run_NN_preprocess\n",
    "model_function = run_CNN\n",
    "model_params = {'temp': 0 }\n",
    "model_preprocess_params = {'char_level': 0, 'word_vector_type': \"glove\" }\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"NN\", \n",
    "                                           train_raw, y_train_one_hot, test_raw, 1) \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run FastText at char level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 2018-Jan-14 03:09:47\n",
      "Running kfold training with model fast_text_char_none\n",
      "Shapes: x_train_raw.shape (19579, 36), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 35)\n",
      "Running preprocess: <function run_NN_preprocess at 0x7f620ee8eea0>\n",
      "No of unique words found is 247346\n",
      "No of words occuring more than 2 times is 75104\n",
      "No of unique words found is after capping is 247346\n",
      "max sentence length is 1439\n",
      "mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000\n",
      "After padding: max sentence size is 161\n",
      "After padding: mean, median length of sentence is 161.000000 161.000000\n",
      "Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)\n",
      "Running fold 1\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_34 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_34  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 60us/step\n",
      "Before training loss, score are: 1.0967000038818802 0.40158334932115386\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 86us/step - loss: 1.0839 - acc: 0.4067 - val_loss: 1.0800 - val_acc: 0.3910\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 1.0612 - acc: 0.4115 - val_loss: 1.0540 - val_acc: 0.4047\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 1.0187 - acc: 0.4510 - val_loss: 1.0073 - val_acc: 0.4676\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.9545 - acc: 0.5750 - val_loss: 0.9459 - val_acc: 0.5664\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.8780 - acc: 0.6929 - val_loss: 0.8801 - val_acc: 0.6519\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7994 - acc: 0.7654 - val_loss: 0.8168 - val_acc: 0.7109\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7260 - acc: 0.8102 - val_loss: 0.7614 - val_acc: 0.7365\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6604 - acc: 0.8322 - val_loss: 0.7118 - val_acc: 0.7689\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6025 - acc: 0.8532 - val_loss: 0.6704 - val_acc: 0.7812\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5520 - acc: 0.8678 - val_loss: 0.6346 - val_acc: 0.7921\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5075 - acc: 0.8821 - val_loss: 0.6052 - val_acc: 0.7949\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4682 - acc: 0.8940 - val_loss: 0.5763 - val_acc: 0.8105\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4330 - acc: 0.9053 - val_loss: 0.5557 - val_acc: 0.8054\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4012 - acc: 0.9129 - val_loss: 0.5337 - val_acc: 0.8161\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3727 - acc: 0.9228 - val_loss: 0.5132 - val_acc: 0.8279\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3467 - acc: 0.9303 - val_loss: 0.4976 - val_acc: 0.8289\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3232 - acc: 0.9364 - val_loss: 0.4826 - val_acc: 0.8350\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3016 - acc: 0.9420 - val_loss: 0.4746 - val_acc: 0.8279\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2818 - acc: 0.9474 - val_loss: 0.4577 - val_acc: 0.8378\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2632 - acc: 0.9525 - val_loss: 0.4457 - val_acc: 0.8432\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2465 - acc: 0.9579 - val_loss: 0.4378 - val_acc: 0.8422\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2309 - acc: 0.9600 - val_loss: 0.4271 - val_acc: 0.8463\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2163 - acc: 0.9639 - val_loss: 0.4213 - val_acc: 0.8455\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.2030 - acc: 0.9667 - val_loss: 0.4134 - val_acc: 0.8437\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1905 - acc: 0.9701 - val_loss: 0.4064 - val_acc: 0.8498\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1793 - acc: 0.9724 - val_loss: 0.3981 - val_acc: 0.8539\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1684 - acc: 0.9740 - val_loss: 0.3949 - val_acc: 0.8486\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1582 - acc: 0.9764 - val_loss: 0.3879 - val_acc: 0.8547\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1488 - acc: 0.9779 - val_loss: 0.3823 - val_acc: 0.8555\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1400 - acc: 0.9798 - val_loss: 0.3821 - val_acc: 0.8514\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1320 - acc: 0.9817 - val_loss: 0.3748 - val_acc: 0.8552\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1244 - acc: 0.9834 - val_loss: 0.3705 - val_acc: 0.8573\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1175 - acc: 0.9845 - val_loss: 0.3691 - val_acc: 0.8560\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1105 - acc: 0.9861 - val_loss: 0.3656 - val_acc: 0.8583\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1042 - acc: 0.9872 - val_loss: 0.3630 - val_acc: 0.8593\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0982 - acc: 0.9884 - val_loss: 0.3614 - val_acc: 0.8590\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0929 - acc: 0.9894 - val_loss: 0.3571 - val_acc: 0.8608\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0876 - acc: 0.9906 - val_loss: 0.3560 - val_acc: 0.8608\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0831 - acc: 0.9903 - val_loss: 0.3545 - val_acc: 0.8618\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0781 - acc: 0.9911 - val_loss: 0.3530 - val_acc: 0.8621\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0737 - acc: 0.9918 - val_loss: 0.3508 - val_acc: 0.8624\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0699 - acc: 0.9922 - val_loss: 0.3509 - val_acc: 0.8606\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0659 - acc: 0.9930 - val_loss: 0.3508 - val_acc: 0.8603\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0625 - acc: 0.9933 - val_loss: 0.3487 - val_acc: 0.8618\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0590 - acc: 0.9941 - val_loss: 0.3482 - val_acc: 0.8616\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0559 - acc: 0.9944 - val_loss: 0.3483 - val_acc: 0.8608\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0527 - acc: 0.9951 - val_loss: 0.3486 - val_acc: 0.8624\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0500 - acc: 0.9956 - val_loss: 0.3485 - val_acc: 0.8624\n",
      "15663/15663 [==============================] - 0s 23us/step\n",
      "==== Training loss, score are: 0.04770270062365888 0.9959139373044755 =======\n",
      "3916/3916 [==============================] - 0s 24us/step\n",
      "==== CV loss, score are: 0.34853063423312114 0.8623595505617978 =======\n",
      "Running fold 2\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_35  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 60us/step\n",
      "Before training loss, score are: 1.097707118140489 0.3122645725623893\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 87us/step - loss: 1.0826 - acc: 0.4056 - val_loss: 1.0777 - val_acc: 0.3943\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 1.0583 - acc: 0.4122 - val_loss: 1.0503 - val_acc: 0.4145\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 1.0152 - acc: 0.4627 - val_loss: 1.0056 - val_acc: 0.4609\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.9505 - acc: 0.5664 - val_loss: 0.9449 - val_acc: 0.5996\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.8739 - acc: 0.7055 - val_loss: 0.8814 - val_acc: 0.6535\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.7958 - acc: 0.7663 - val_loss: 0.8198 - val_acc: 0.7204\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7220 - acc: 0.8082 - val_loss: 0.7642 - val_acc: 0.7510\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6563 - acc: 0.8394 - val_loss: 0.7169 - val_acc: 0.7589\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5981 - acc: 0.8555 - val_loss: 0.6752 - val_acc: 0.7806\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5473 - acc: 0.8731 - val_loss: 0.6400 - val_acc: 0.7932\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5028 - acc: 0.8840 - val_loss: 0.6077 - val_acc: 0.8077\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4630 - acc: 0.8975 - val_loss: 0.5820 - val_acc: 0.8062\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4277 - acc: 0.9070 - val_loss: 0.5573 - val_acc: 0.8246\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3961 - acc: 0.9176 - val_loss: 0.5367 - val_acc: 0.8200\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3676 - acc: 0.9263 - val_loss: 0.5194 - val_acc: 0.8202\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3421 - acc: 0.9318 - val_loss: 0.5005 - val_acc: 0.8371\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3184 - acc: 0.9380 - val_loss: 0.4873 - val_acc: 0.8304\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2970 - acc: 0.9448 - val_loss: 0.4715 - val_acc: 0.8435\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2773 - acc: 0.9501 - val_loss: 0.4599 - val_acc: 0.8427\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2590 - acc: 0.9547 - val_loss: 0.4495 - val_acc: 0.8458\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2424 - acc: 0.9589 - val_loss: 0.4387 - val_acc: 0.8465\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2271 - acc: 0.9620 - val_loss: 0.4291 - val_acc: 0.8504\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2128 - acc: 0.9660 - val_loss: 0.4205 - val_acc: 0.8519\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1993 - acc: 0.9690 - val_loss: 0.4130 - val_acc: 0.8524\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1871 - acc: 0.9712 - val_loss: 0.4055 - val_acc: 0.8537\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1756 - acc: 0.9739 - val_loss: 0.4003 - val_acc: 0.8565\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1650 - acc: 0.9760 - val_loss: 0.3943 - val_acc: 0.8550\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1550 - acc: 0.9781 - val_loss: 0.3898 - val_acc: 0.8557\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1458 - acc: 0.9791 - val_loss: 0.3862 - val_acc: 0.8565\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1372 - acc: 0.9805 - val_loss: 0.3803 - val_acc: 0.8590\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1291 - acc: 0.9820 - val_loss: 0.3745 - val_acc: 0.8613\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1217 - acc: 0.9837 - val_loss: 0.3724 - val_acc: 0.8603\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1144 - acc: 0.9854 - val_loss: 0.3711 - val_acc: 0.8585\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1079 - acc: 0.9864 - val_loss: 0.3657 - val_acc: 0.8626\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1016 - acc: 0.9873 - val_loss: 0.3627 - val_acc: 0.8618\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0958 - acc: 0.9887 - val_loss: 0.3588 - val_acc: 0.8616\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0904 - acc: 0.9896 - val_loss: 0.3577 - val_acc: 0.8636\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0852 - acc: 0.9900 - val_loss: 0.3549 - val_acc: 0.8629\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0803 - acc: 0.9914 - val_loss: 0.3537 - val_acc: 0.8621\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0758 - acc: 0.9918 - val_loss: 0.3519 - val_acc: 0.8624\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0715 - acc: 0.9923 - val_loss: 0.3524 - val_acc: 0.8616\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0677 - acc: 0.9927 - val_loss: 0.3490 - val_acc: 0.8616\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0639 - acc: 0.9934 - val_loss: 0.3500 - val_acc: 0.8621\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0602 - acc: 0.9939 - val_loss: 0.3472 - val_acc: 0.8608\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0570 - acc: 0.9943 - val_loss: 0.3479 - val_acc: 0.8606\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0539 - acc: 0.9947 - val_loss: 0.3482 - val_acc: 0.8626\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0509 - acc: 0.9951 - val_loss: 0.3454 - val_acc: 0.8631\n",
      "Epoch 48/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0482 - acc: 0.9955 - val_loss: 0.3458 - val_acc: 0.8629\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0454 - acc: 0.9957 - val_loss: 0.3471 - val_acc: 0.8616\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0430 - acc: 0.9959 - val_loss: 0.3467 - val_acc: 0.8631\n",
      "15663/15663 [==============================] - 0s 23us/step\n",
      "==== Training loss, score are: 0.04114073684328111 0.9962331609525633 =======\n",
      "3916/3916 [==============================] - 0s 25us/step\n",
      "==== CV loss, score are: 0.3467473699135727 0.8631256384065373 =======\n",
      "Running fold 3\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_36  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 61us/step\n",
      "Before training loss, score are: 1.0956880871320922 0.39539041054444407\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 86us/step - loss: 1.0827 - acc: 0.4046 - val_loss: 1.0754 - val_acc: 0.4002\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 1.0587 - acc: 0.4167 - val_loss: 1.0486 - val_acc: 0.4367\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 1.0149 - acc: 0.4722 - val_loss: 1.0026 - val_acc: 0.4875\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.9501 - acc: 0.5916 - val_loss: 0.9436 - val_acc: 0.6083\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.8741 - acc: 0.6942 - val_loss: 0.8812 - val_acc: 0.7043\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.7971 - acc: 0.7681 - val_loss: 0.8212 - val_acc: 0.7268\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.7250 - acc: 0.8076 - val_loss: 0.7677 - val_acc: 0.7354\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.6602 - acc: 0.8327 - val_loss: 0.7200 - val_acc: 0.7674\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.6028 - acc: 0.8561 - val_loss: 0.6785 - val_acc: 0.7850\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5520 - acc: 0.8697 - val_loss: 0.6438 - val_acc: 0.7745\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5072 - acc: 0.8854 - val_loss: 0.6115 - val_acc: 0.8003\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4672 - acc: 0.8980 - val_loss: 0.5840 - val_acc: 0.8062\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4317 - acc: 0.9066 - val_loss: 0.5597 - val_acc: 0.8151\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3993 - acc: 0.9173 - val_loss: 0.5383 - val_acc: 0.8238\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3704 - acc: 0.9249 - val_loss: 0.5200 - val_acc: 0.8228\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3447 - acc: 0.9325 - val_loss: 0.5028 - val_acc: 0.8299\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3204 - acc: 0.9396 - val_loss: 0.4877 - val_acc: 0.8302\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2987 - acc: 0.9444 - val_loss: 0.4729 - val_acc: 0.8384\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2785 - acc: 0.9505 - val_loss: 0.4610 - val_acc: 0.8473\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2605 - acc: 0.9558 - val_loss: 0.4495 - val_acc: 0.8460\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2434 - acc: 0.9595 - val_loss: 0.4396 - val_acc: 0.8468\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2277 - acc: 0.9634 - val_loss: 0.4289 - val_acc: 0.8509\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2132 - acc: 0.9662 - val_loss: 0.4204 - val_acc: 0.8521\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1999 - acc: 0.9694 - val_loss: 0.4123 - val_acc: 0.8539\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1872 - acc: 0.9712 - val_loss: 0.4056 - val_acc: 0.8529\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1759 - acc: 0.9731 - val_loss: 0.3986 - val_acc: 0.8588\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1652 - acc: 0.9757 - val_loss: 0.3920 - val_acc: 0.8580\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1551 - acc: 0.9777 - val_loss: 0.3879 - val_acc: 0.8550\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1459 - acc: 0.9801 - val_loss: 0.3822 - val_acc: 0.8613\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1370 - acc: 0.9819 - val_loss: 0.3765 - val_acc: 0.8641\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1289 - acc: 0.9835 - val_loss: 0.3725 - val_acc: 0.8629\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1215 - acc: 0.9844 - val_loss: 0.3683 - val_acc: 0.8659\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1143 - acc: 0.9860 - val_loss: 0.3645 - val_acc: 0.8662\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1074 - acc: 0.9872 - val_loss: 0.3613 - val_acc: 0.8680\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1013 - acc: 0.9882 - val_loss: 0.3590 - val_acc: 0.8664\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0955 - acc: 0.9895 - val_loss: 0.3575 - val_acc: 0.8652\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0900 - acc: 0.9903 - val_loss: 0.3537 - val_acc: 0.8687\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0848 - acc: 0.9911 - val_loss: 0.3512 - val_acc: 0.8713\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0799 - acc: 0.9918 - val_loss: 0.3484 - val_acc: 0.8710\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0755 - acc: 0.9923 - val_loss: 0.3484 - val_acc: 0.8680\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0714 - acc: 0.9930 - val_loss: 0.3470 - val_acc: 0.8677\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0672 - acc: 0.9933 - val_loss: 0.3448 - val_acc: 0.8690\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0635 - acc: 0.9937 - val_loss: 0.3425 - val_acc: 0.8718\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0601 - acc: 0.9941 - val_loss: 0.3414 - val_acc: 0.8708\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0566 - acc: 0.9946 - val_loss: 0.3414 - val_acc: 0.8703\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0534 - acc: 0.9951 - val_loss: 0.3423 - val_acc: 0.8667\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0505 - acc: 0.9951 - val_loss: 0.3394 - val_acc: 0.8723\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0478 - acc: 0.9957 - val_loss: 0.3395 - val_acc: 0.8698\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0451 - acc: 0.9957 - val_loss: 0.3387 - val_acc: 0.8705\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0427 - acc: 0.9962 - val_loss: 0.3382 - val_acc: 0.8728\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0403 - acc: 0.9966 - val_loss: 0.3391 - val_acc: 0.8700\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0381 - acc: 0.9969 - val_loss: 0.3384 - val_acc: 0.8723\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0361 - acc: 0.9969 - val_loss: 0.3405 - val_acc: 0.8687\n",
      "15663/15663 [==============================] - 0s 23us/step\n",
      "==== Training loss, score are: 0.03473636081730856 0.9964885398710337 =======\n",
      "3916/3916 [==============================] - 0s 24us/step\n",
      "==== CV loss, score are: 0.3405472373816283 0.8687436159955103 =======\n",
      "Running fold 4\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_37 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_37  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 63us/step\n",
      "Before training loss, score are: 1.1030528882465729 0.30747621784107154\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 90us/step - loss: 1.0864 - acc: 0.3888 - val_loss: 1.0711 - val_acc: 0.4127\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 1.0574 - acc: 0.4165 - val_loss: 1.0411 - val_acc: 0.4402\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 1.0111 - acc: 0.4767 - val_loss: 0.9942 - val_acc: 0.5375\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.9452 - acc: 0.5839 - val_loss: 0.9341 - val_acc: 0.6287\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.8686 - acc: 0.6981 - val_loss: 0.8707 - val_acc: 0.6696\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7916 - acc: 0.7659 - val_loss: 0.8091 - val_acc: 0.7199\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.7189 - acc: 0.8099 - val_loss: 0.7548 - val_acc: 0.7740\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.6532 - acc: 0.8433 - val_loss: 0.7066 - val_acc: 0.7806\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5955 - acc: 0.8622 - val_loss: 0.6647 - val_acc: 0.7817\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5447 - acc: 0.8770 - val_loss: 0.6299 - val_acc: 0.7911\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5002 - acc: 0.8887 - val_loss: 0.5991 - val_acc: 0.8057\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4604 - acc: 0.9032 - val_loss: 0.5732 - val_acc: 0.8159\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4250 - acc: 0.9121 - val_loss: 0.5492 - val_acc: 0.8159\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3935 - acc: 0.9194 - val_loss: 0.5284 - val_acc: 0.8233\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3653 - acc: 0.9262 - val_loss: 0.5090 - val_acc: 0.8233\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3394 - acc: 0.9330 - val_loss: 0.4925 - val_acc: 0.8297\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3159 - acc: 0.9408 - val_loss: 0.4779 - val_acc: 0.8343\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2946 - acc: 0.9454 - val_loss: 0.4662 - val_acc: 0.8355\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2751 - acc: 0.9503 - val_loss: 0.4525 - val_acc: 0.8404\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2570 - acc: 0.9549 - val_loss: 0.4418 - val_acc: 0.8450\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2404 - acc: 0.9589 - val_loss: 0.4318 - val_acc: 0.8447\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2249 - acc: 0.9632 - val_loss: 0.4224 - val_acc: 0.8486\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2109 - acc: 0.9658 - val_loss: 0.4145 - val_acc: 0.8516\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1976 - acc: 0.9692 - val_loss: 0.4068 - val_acc: 0.8524\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1857 - acc: 0.9713 - val_loss: 0.4005 - val_acc: 0.8524\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1741 - acc: 0.9739 - val_loss: 0.3942 - val_acc: 0.8555\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1636 - acc: 0.9759 - val_loss: 0.3900 - val_acc: 0.8567\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1541 - acc: 0.9775 - val_loss: 0.3820 - val_acc: 0.8608\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1446 - acc: 0.9798 - val_loss: 0.3777 - val_acc: 0.8570\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1363 - acc: 0.9812 - val_loss: 0.3754 - val_acc: 0.8570\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1282 - acc: 0.9830 - val_loss: 0.3687 - val_acc: 0.8649\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1206 - acc: 0.9844 - val_loss: 0.3650 - val_acc: 0.8670\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1136 - acc: 0.9853 - val_loss: 0.3617 - val_acc: 0.8659\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1071 - acc: 0.9863 - val_loss: 0.3585 - val_acc: 0.8631\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1010 - acc: 0.9879 - val_loss: 0.3557 - val_acc: 0.8672\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0951 - acc: 0.9883 - val_loss: 0.3529 - val_acc: 0.8647\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0897 - acc: 0.9892 - val_loss: 0.3504 - val_acc: 0.8662\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0849 - acc: 0.9904 - val_loss: 0.3496 - val_acc: 0.8626\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0798 - acc: 0.9911 - val_loss: 0.3474 - val_acc: 0.8667\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0755 - acc: 0.9918 - val_loss: 0.3452 - val_acc: 0.8641\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0712 - acc: 0.9923 - val_loss: 0.3433 - val_acc: 0.8657\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0672 - acc: 0.9932 - val_loss: 0.3417 - val_acc: 0.8677\n",
      "Epoch 43/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0635 - acc: 0.9935 - val_loss: 0.3401 - val_acc: 0.8680\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0600 - acc: 0.9939 - val_loss: 0.3390 - val_acc: 0.8682\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0566 - acc: 0.9950 - val_loss: 0.3379 - val_acc: 0.8687\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 51us/step - loss: 0.0535 - acc: 0.9948 - val_loss: 0.3371 - val_acc: 0.8690\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0506 - acc: 0.9955 - val_loss: 0.3368 - val_acc: 0.8682\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0479 - acc: 0.9955 - val_loss: 0.3363 - val_acc: 0.8685\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0454 - acc: 0.9958 - val_loss: 0.3361 - val_acc: 0.8685\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0427 - acc: 0.9962 - val_loss: 0.3365 - val_acc: 0.8667\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0406 - acc: 0.9960 - val_loss: 0.3353 - val_acc: 0.8700\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0383 - acc: 0.9967 - val_loss: 0.3366 - val_acc: 0.8664\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0363 - acc: 0.9970 - val_loss: 0.3355 - val_acc: 0.8705\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0342 - acc: 0.9972 - val_loss: 0.3373 - val_acc: 0.8670\n",
      "15663/15663 [==============================] - 0s 24us/step\n",
      "==== Training loss, score are: 0.033365172293047404 0.9978931239226202 =======\n",
      "3916/3916 [==============================] - 0s 24us/step\n",
      "==== CV loss, score are: 0.3373016334377587 0.8669560776911182 =======\n",
      "Running fold 5\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type None \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_38 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_38  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15664/15664 [==============================] - 1s 66us/step\n",
      "Before training loss, score are: 1.1011670322535108 0.2913687436159346\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/150\n",
      "15664/15664 [==============================] - 1s 93us/step - loss: 1.0854 - acc: 0.3929 - val_loss: 1.0691 - val_acc: 0.4212\n",
      "Epoch 2/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 1.0595 - acc: 0.4026 - val_loss: 1.0413 - val_acc: 0.4299\n",
      "Epoch 3/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 1.0165 - acc: 0.4510 - val_loss: 0.9977 - val_acc: 0.4991\n",
      "Epoch 4/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.9541 - acc: 0.5873 - val_loss: 0.9406 - val_acc: 0.6051\n",
      "Epoch 5/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.8800 - acc: 0.6984 - val_loss: 0.8788 - val_acc: 0.7055\n",
      "Epoch 6/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.8036 - acc: 0.7842 - val_loss: 0.8172 - val_acc: 0.7246\n",
      "Epoch 7/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.7312 - acc: 0.8151 - val_loss: 0.7636 - val_acc: 0.7630\n",
      "Epoch 8/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.6662 - acc: 0.8371 - val_loss: 0.7144 - val_acc: 0.7811\n",
      "Epoch 9/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.6080 - acc: 0.8579 - val_loss: 0.6728 - val_acc: 0.7908\n",
      "Epoch 10/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.5572 - acc: 0.8703 - val_loss: 0.6385 - val_acc: 0.8148\n",
      "Epoch 11/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.5126 - acc: 0.8857 - val_loss: 0.6057 - val_acc: 0.8143\n",
      "Epoch 12/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.4730 - acc: 0.8976 - val_loss: 0.5787 - val_acc: 0.8169\n",
      "Epoch 13/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.4377 - acc: 0.9060 - val_loss: 0.5561 - val_acc: 0.8289\n",
      "Epoch 14/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.4059 - acc: 0.9146 - val_loss: 0.5363 - val_acc: 0.8360\n",
      "Epoch 15/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.3774 - acc: 0.9227 - val_loss: 0.5169 - val_acc: 0.8363\n",
      "Epoch 16/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.3517 - acc: 0.9318 - val_loss: 0.4993 - val_acc: 0.8375\n",
      "Epoch 17/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.3278 - acc: 0.9399 - val_loss: 0.4840 - val_acc: 0.8406\n",
      "Epoch 18/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.3062 - acc: 0.9441 - val_loss: 0.4713 - val_acc: 0.8429\n",
      "Epoch 19/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.2865 - acc: 0.9489 - val_loss: 0.4594 - val_acc: 0.8483\n",
      "Epoch 20/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.2682 - acc: 0.9527 - val_loss: 0.4474 - val_acc: 0.8490\n",
      "Epoch 21/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.2512 - acc: 0.9564 - val_loss: 0.4373 - val_acc: 0.8526\n",
      "Epoch 22/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.2358 - acc: 0.9607 - val_loss: 0.4282 - val_acc: 0.8544\n",
      "Epoch 23/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.2215 - acc: 0.9628 - val_loss: 0.4200 - val_acc: 0.8513\n",
      "Epoch 24/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.2079 - acc: 0.9665 - val_loss: 0.4119 - val_acc: 0.8570\n",
      "Epoch 25/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1957 - acc: 0.9689 - val_loss: 0.4048 - val_acc: 0.8567\n",
      "Epoch 26/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1837 - acc: 0.9718 - val_loss: 0.3987 - val_acc: 0.8559\n",
      "Epoch 27/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1733 - acc: 0.9736 - val_loss: 0.3930 - val_acc: 0.8603\n",
      "Epoch 28/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.1635 - acc: 0.9767 - val_loss: 0.3889 - val_acc: 0.8605\n",
      "Epoch 29/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1537 - acc: 0.9784 - val_loss: 0.3824 - val_acc: 0.8608\n",
      "Epoch 30/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1447 - acc: 0.9799 - val_loss: 0.3781 - val_acc: 0.8633\n",
      "Epoch 31/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1364 - acc: 0.9817 - val_loss: 0.3736 - val_acc: 0.8595\n",
      "Epoch 32/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1285 - acc: 0.9837 - val_loss: 0.3698 - val_acc: 0.8608\n",
      "Epoch 33/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1215 - acc: 0.9848 - val_loss: 0.3660 - val_acc: 0.8613\n",
      "Epoch 34/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1144 - acc: 0.9853 - val_loss: 0.3633 - val_acc: 0.8603\n",
      "Epoch 35/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.1082 - acc: 0.9867 - val_loss: 0.3599 - val_acc: 0.8639\n",
      "Epoch 36/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.1022 - acc: 0.9881 - val_loss: 0.3570 - val_acc: 0.8651\n",
      "Epoch 37/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0965 - acc: 0.9886 - val_loss: 0.3557 - val_acc: 0.8626\n",
      "Epoch 38/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0912 - acc: 0.9898 - val_loss: 0.3527 - val_acc: 0.8636\n",
      "Epoch 39/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0862 - acc: 0.9901 - val_loss: 0.3504 - val_acc: 0.8662\n",
      "Epoch 40/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.0816 - acc: 0.9909 - val_loss: 0.3487 - val_acc: 0.8659\n",
      "Epoch 41/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0772 - acc: 0.9918 - val_loss: 0.3475 - val_acc: 0.8623\n",
      "Epoch 42/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0730 - acc: 0.9921 - val_loss: 0.3466 - val_acc: 0.8641\n",
      "Epoch 43/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0691 - acc: 0.9931 - val_loss: 0.3453 - val_acc: 0.8644\n",
      "Epoch 44/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0654 - acc: 0.9930 - val_loss: 0.3436 - val_acc: 0.8649\n",
      "Epoch 45/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0618 - acc: 0.9940 - val_loss: 0.3426 - val_acc: 0.8654\n",
      "Epoch 46/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0586 - acc: 0.9941 - val_loss: 0.3419 - val_acc: 0.8651\n",
      "Epoch 47/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0556 - acc: 0.9943 - val_loss: 0.3413 - val_acc: 0.8654\n",
      "Epoch 48/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0527 - acc: 0.9946 - val_loss: 0.3408 - val_acc: 0.8659\n",
      "Epoch 49/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0498 - acc: 0.9952 - val_loss: 0.3416 - val_acc: 0.8656\n",
      "Epoch 50/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0474 - acc: 0.9953 - val_loss: 0.3404 - val_acc: 0.8626\n",
      "Epoch 51/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0446 - acc: 0.9956 - val_loss: 0.3417 - val_acc: 0.8626\n",
      "Epoch 52/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0425 - acc: 0.9959 - val_loss: 0.3417 - val_acc: 0.8621\n",
      "Epoch 53/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0402 - acc: 0.9960 - val_loss: 0.3406 - val_acc: 0.8623\n",
      "15664/15664 [==============================] - 0s 24us/step\n",
      "==== Training loss, score are: 0.038446925123877616 0.9961695607763024 =======\n",
      "3915/3915 [==============================] - 0s 25us/step\n",
      "==== CV loss, score are: 0.3405798800984257 0.8623243934349996 =======\n",
      "\n",
      "\n",
      "===== Model: fast_text_char_none  ========:\n",
      " Cross-val log losses are: [0.34853063227308723, 0.34674736640115189, 0.34054723574078843, 0.33730163001916968, 0.34057987701431947]\n",
      "====== Mean cross-val log loss is: 0.3427413482897033 =========\n",
      "\n",
      "\n",
      "Timestamp: 2018-Jan-14 03:13:22\n",
      "Running kfold training with model fast_text_char_gensim\n",
      "Shapes: x_train_raw.shape (19579, 39), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 38)\n",
      "Running preprocess: <function run_NN_preprocess at 0x7f620ee8eea0>\n",
      "No of unique words found is 247346\n",
      "No of words occuring more than 2 times is 75104\n",
      "No of unique words found is after capping is 247346\n",
      "max sentence length is 1439\n",
      "mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000\n",
      "After padding: max sentence size is 161\n",
      "After padding: mean, median length of sentence is 161.000000 161.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9853it [00:00, 98505.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word embedding matrix for gensim vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76597it [00:00, 98739.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76597 word vectors.\n",
      "of--mirth 247345\n",
      "None\n",
      "(247346, 20)\n",
      "Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)\n",
      "Running fold 1\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_39 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_39  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 67us/step\n",
      "Before training loss, score are: 1.2088994647757438 0.40643554877208915\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 92us/step - loss: 1.1343 - acc: 0.3630 - val_loss: 1.1100 - val_acc: 0.3565\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 1.0794 - acc: 0.4147 - val_loss: 1.0683 - val_acc: 0.4157\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 1.0393 - acc: 0.4599 - val_loss: 1.0310 - val_acc: 0.4597\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 1.0015 - acc: 0.5125 - val_loss: 0.9953 - val_acc: 0.5117\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.9638 - acc: 0.5675 - val_loss: 0.9605 - val_acc: 0.5534\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.9262 - acc: 0.6137 - val_loss: 0.9262 - val_acc: 0.5975\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.8883 - acc: 0.6473 - val_loss: 0.8925 - val_acc: 0.6364\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.8508 - acc: 0.6783 - val_loss: 0.8606 - val_acc: 0.6537\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.8138 - acc: 0.7027 - val_loss: 0.8301 - val_acc: 0.6739\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.7779 - acc: 0.7241 - val_loss: 0.8022 - val_acc: 0.6834\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7432 - acc: 0.7417 - val_loss: 0.7751 - val_acc: 0.7007\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7097 - acc: 0.7583 - val_loss: 0.7497 - val_acc: 0.7171\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6774 - acc: 0.7732 - val_loss: 0.7268 - val_acc: 0.7219\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6462 - acc: 0.7871 - val_loss: 0.7035 - val_acc: 0.7349\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.6165 - acc: 0.8005 - val_loss: 0.6832 - val_acc: 0.7431\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5881 - acc: 0.8140 - val_loss: 0.6623 - val_acc: 0.7569\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5610 - acc: 0.8259 - val_loss: 0.6442 - val_acc: 0.7633\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5352 - acc: 0.8360 - val_loss: 0.6271 - val_acc: 0.7689\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5102 - acc: 0.8478 - val_loss: 0.6098 - val_acc: 0.7799\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4870 - acc: 0.8567 - val_loss: 0.5942 - val_acc: 0.7865\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4643 - acc: 0.8643 - val_loss: 0.5804 - val_acc: 0.7893\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4429 - acc: 0.8735 - val_loss: 0.5664 - val_acc: 0.7955\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4222 - acc: 0.8815 - val_loss: 0.5532 - val_acc: 0.7998\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4027 - acc: 0.8894 - val_loss: 0.5423 - val_acc: 0.7995\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3844 - acc: 0.8956 - val_loss: 0.5321 - val_acc: 0.8008\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3665 - acc: 0.9015 - val_loss: 0.5198 - val_acc: 0.8031\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3495 - acc: 0.9081 - val_loss: 0.5091 - val_acc: 0.8108\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3336 - acc: 0.9140 - val_loss: 0.5003 - val_acc: 0.8100\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3183 - acc: 0.9202 - val_loss: 0.4907 - val_acc: 0.8110\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3035 - acc: 0.9254 - val_loss: 0.4808 - val_acc: 0.8215\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.2895 - acc: 0.9300 - val_loss: 0.4746 - val_acc: 0.8195\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2762 - acc: 0.9343 - val_loss: 0.4662 - val_acc: 0.8238\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2637 - acc: 0.9383 - val_loss: 0.4583 - val_acc: 0.8241\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2516 - acc: 0.9420 - val_loss: 0.4519 - val_acc: 0.8274\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2400 - acc: 0.9475 - val_loss: 0.4454 - val_acc: 0.8317\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2291 - acc: 0.9510 - val_loss: 0.4392 - val_acc: 0.8320\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2187 - acc: 0.9540 - val_loss: 0.4351 - val_acc: 0.8327\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2087 - acc: 0.9569 - val_loss: 0.4301 - val_acc: 0.8355\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1990 - acc: 0.9582 - val_loss: 0.4237 - val_acc: 0.8371\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1899 - acc: 0.9618 - val_loss: 0.4200 - val_acc: 0.8386\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.1815 - acc: 0.9638 - val_loss: 0.4144 - val_acc: 0.8394\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1732 - acc: 0.9664 - val_loss: 0.4101 - val_acc: 0.8409\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1653 - acc: 0.9687 - val_loss: 0.4068 - val_acc: 0.8422\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1579 - acc: 0.9701 - val_loss: 0.4027 - val_acc: 0.8440\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1507 - acc: 0.9730 - val_loss: 0.3991 - val_acc: 0.8450\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1438 - acc: 0.9745 - val_loss: 0.3969 - val_acc: 0.8473\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1375 - acc: 0.9759 - val_loss: 0.3937 - val_acc: 0.8483\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1314 - acc: 0.9774 - val_loss: 0.3910 - val_acc: 0.8483\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1253 - acc: 0.9790 - val_loss: 0.3891 - val_acc: 0.8493\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1196 - acc: 0.9801 - val_loss: 0.3854 - val_acc: 0.8511\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1143 - acc: 0.9814 - val_loss: 0.3832 - val_acc: 0.8506\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1092 - acc: 0.9829 - val_loss: 0.3833 - val_acc: 0.8514\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1042 - acc: 0.9840 - val_loss: 0.3798 - val_acc: 0.8496\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0996 - acc: 0.9849 - val_loss: 0.3777 - val_acc: 0.8511\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0954 - acc: 0.9855 - val_loss: 0.3775 - val_acc: 0.8521\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0911 - acc: 0.9865 - val_loss: 0.3751 - val_acc: 0.8534\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0870 - acc: 0.9874 - val_loss: 0.3741 - val_acc: 0.8524\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0831 - acc: 0.9882 - val_loss: 0.3746 - val_acc: 0.8534\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0796 - acc: 0.9887 - val_loss: 0.3711 - val_acc: 0.8537\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0760 - acc: 0.9897 - val_loss: 0.3711 - val_acc: 0.8524\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 1s 51us/step - loss: 0.0727 - acc: 0.9902 - val_loss: 0.3709 - val_acc: 0.8539\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0693 - acc: 0.9907 - val_loss: 0.3701 - val_acc: 0.8529\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0663 - acc: 0.9914 - val_loss: 0.3686 - val_acc: 0.8534\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.0634 - acc: 0.9912 - val_loss: 0.3680 - val_acc: 0.8537\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 1s 50us/step - loss: 0.0606 - acc: 0.9920 - val_loss: 0.3683 - val_acc: 0.8542\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0581 - acc: 0.9923 - val_loss: 0.3673 - val_acc: 0.8537\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.0556 - acc: 0.9928 - val_loss: 0.3673 - val_acc: 0.8552\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.0530 - acc: 0.9936 - val_loss: 0.3677 - val_acc: 0.8555\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.0507 - acc: 0.9938 - val_loss: 0.3668 - val_acc: 0.8552\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 1s 50us/step - loss: 0.0486 - acc: 0.9943 - val_loss: 0.3680 - val_acc: 0.8565\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 1s 50us/step - loss: 0.0464 - acc: 0.9944 - val_loss: 0.3698 - val_acc: 0.8565\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 1s 50us/step - loss: 0.0444 - acc: 0.9948 - val_loss: 0.3677 - val_acc: 0.8575\n",
      "15663/15663 [==============================] - 0s 25us/step\n",
      "==== Training loss, score are: 0.04277276798767856 0.9948924216305944 =======\n",
      "3916/3916 [==============================] - 0s 26us/step\n",
      "==== CV loss, score are: 0.3676888651011913 0.8575076608784474 =======\n",
      "Running fold 2\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_40 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_40  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 71us/step\n",
      "Before training loss, score are: 1.2846237993145981 0.30900849135189323\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 2s 96us/step - loss: 1.0967 - acc: 0.4092 - val_loss: 1.0369 - val_acc: 0.4640\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 1.0176 - acc: 0.4918 - val_loss: 1.0116 - val_acc: 0.4949\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.9889 - acc: 0.5323 - val_loss: 0.9863 - val_acc: 0.5174\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 51us/step - loss: 0.9592 - acc: 0.5673 - val_loss: 0.9607 - val_acc: 0.5411\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.9289 - acc: 0.5964 - val_loss: 0.9338 - val_acc: 0.5799\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 50us/step - loss: 0.8972 - acc: 0.6249 - val_loss: 0.9075 - val_acc: 0.6016\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.8648 - acc: 0.6527 - val_loss: 0.8810 - val_acc: 0.6157\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.8318 - acc: 0.6757 - val_loss: 0.8548 - val_acc: 0.6310\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7989 - acc: 0.6972 - val_loss: 0.8285 - val_acc: 0.6499\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7657 - acc: 0.7158 - val_loss: 0.8024 - val_acc: 0.6711\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7329 - acc: 0.7357 - val_loss: 0.7780 - val_acc: 0.6849\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7010 - acc: 0.7530 - val_loss: 0.7547 - val_acc: 0.6954\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6697 - acc: 0.7684 - val_loss: 0.7330 - val_acc: 0.7079\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6399 - acc: 0.7831 - val_loss: 0.7109 - val_acc: 0.7224\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6107 - acc: 0.7972 - val_loss: 0.6909 - val_acc: 0.7314\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5831 - acc: 0.8103 - val_loss: 0.6710 - val_acc: 0.7434\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5563 - acc: 0.8212 - val_loss: 0.6529 - val_acc: 0.7480\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5306 - acc: 0.8313 - val_loss: 0.6362 - val_acc: 0.7600\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5063 - acc: 0.8436 - val_loss: 0.6193 - val_acc: 0.7656\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4832 - acc: 0.8543 - val_loss: 0.6044 - val_acc: 0.7697\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4610 - acc: 0.8622 - val_loss: 0.5914 - val_acc: 0.7720\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4397 - acc: 0.8714 - val_loss: 0.5757 - val_acc: 0.7814\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4196 - acc: 0.8812 - val_loss: 0.5633 - val_acc: 0.7824\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4003 - acc: 0.8885 - val_loss: 0.5516 - val_acc: 0.7901\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3820 - acc: 0.8970 - val_loss: 0.5399 - val_acc: 0.7939\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.3646 - acc: 0.9028 - val_loss: 0.5289 - val_acc: 0.7975\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3479 - acc: 0.9092 - val_loss: 0.5175 - val_acc: 0.8049\n",
      "Epoch 28/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3319 - acc: 0.9148 - val_loss: 0.5082 - val_acc: 0.8080\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3171 - acc: 0.9190 - val_loss: 0.4985 - val_acc: 0.8118\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3027 - acc: 0.9256 - val_loss: 0.4905 - val_acc: 0.8138\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2889 - acc: 0.9291 - val_loss: 0.4812 - val_acc: 0.8200\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2757 - acc: 0.9351 - val_loss: 0.4741 - val_acc: 0.8212\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2631 - acc: 0.9393 - val_loss: 0.4672 - val_acc: 0.8230\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2511 - acc: 0.9430 - val_loss: 0.4595 - val_acc: 0.8307\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2402 - acc: 0.9475 - val_loss: 0.4536 - val_acc: 0.8294\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2290 - acc: 0.9515 - val_loss: 0.4474 - val_acc: 0.8327\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2188 - acc: 0.9544 - val_loss: 0.4405 - val_acc: 0.8378\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2088 - acc: 0.9575 - val_loss: 0.4354 - val_acc: 0.8384\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1998 - acc: 0.9604 - val_loss: 0.4301 - val_acc: 0.8381\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1907 - acc: 0.9631 - val_loss: 0.4253 - val_acc: 0.8401\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1822 - acc: 0.9648 - val_loss: 0.4201 - val_acc: 0.8419\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1740 - acc: 0.9674 - val_loss: 0.4156 - val_acc: 0.8430\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1662 - acc: 0.9687 - val_loss: 0.4112 - val_acc: 0.8445\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1588 - acc: 0.9705 - val_loss: 0.4073 - val_acc: 0.8465\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1517 - acc: 0.9723 - val_loss: 0.4052 - val_acc: 0.8447\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1451 - acc: 0.9740 - val_loss: 0.4001 - val_acc: 0.8481\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1385 - acc: 0.9752 - val_loss: 0.3976 - val_acc: 0.8470\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1325 - acc: 0.9770 - val_loss: 0.3950 - val_acc: 0.8481\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1266 - acc: 0.9778 - val_loss: 0.3919 - val_acc: 0.8491\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1210 - acc: 0.9794 - val_loss: 0.3884 - val_acc: 0.8516\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1156 - acc: 0.9807 - val_loss: 0.3875 - val_acc: 0.8498\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1104 - acc: 0.9810 - val_loss: 0.3845 - val_acc: 0.8511\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1056 - acc: 0.9819 - val_loss: 0.3821 - val_acc: 0.8521\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1008 - acc: 0.9833 - val_loss: 0.3799 - val_acc: 0.8534\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0964 - acc: 0.9842 - val_loss: 0.3778 - val_acc: 0.8534\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0921 - acc: 0.9856 - val_loss: 0.3767 - val_acc: 0.8537\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0880 - acc: 0.9862 - val_loss: 0.3746 - val_acc: 0.8560\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0842 - acc: 0.9874 - val_loss: 0.3741 - val_acc: 0.8521\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0804 - acc: 0.9885 - val_loss: 0.3719 - val_acc: 0.8565\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0770 - acc: 0.9890 - val_loss: 0.3715 - val_acc: 0.8537\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0734 - acc: 0.9897 - val_loss: 0.3699 - val_acc: 0.8560\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0702 - acc: 0.9904 - val_loss: 0.3689 - val_acc: 0.8567\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0670 - acc: 0.9912 - val_loss: 0.3694 - val_acc: 0.8539\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0641 - acc: 0.9911 - val_loss: 0.3680 - val_acc: 0.8542\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0614 - acc: 0.9916 - val_loss: 0.3676 - val_acc: 0.8573\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0587 - acc: 0.9920 - val_loss: 0.3670 - val_acc: 0.8547\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0561 - acc: 0.9927 - val_loss: 0.3664 - val_acc: 0.8555\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0535 - acc: 0.9928 - val_loss: 0.3683 - val_acc: 0.8560\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0512 - acc: 0.9932 - val_loss: 0.3664 - val_acc: 0.8552\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0490 - acc: 0.9936 - val_loss: 0.3660 - val_acc: 0.8562\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0467 - acc: 0.9943 - val_loss: 0.3666 - val_acc: 0.8560\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0447 - acc: 0.9941 - val_loss: 0.3657 - val_acc: 0.8570\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0427 - acc: 0.9947 - val_loss: 0.3670 - val_acc: 0.8560\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0409 - acc: 0.9948 - val_loss: 0.3663 - val_acc: 0.8588\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0391 - acc: 0.9950 - val_loss: 0.3663 - val_acc: 0.8593\n",
      "15663/15663 [==============================] - 0s 25us/step\n",
      "==== Training loss, score are: 0.03774240515516809 0.9960416267637107 =======\n",
      "3916/3916 [==============================] - 0s 25us/step\n",
      "==== CV loss, score are: 0.3663391914290963 0.8592951991828396 =======\n",
      "Running fold 3\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_41 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_41  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 67us/step\n",
      "Before training loss, score are: 1.1209678778754624 0.33486560684986316\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 92us/step - loss: 1.0868 - acc: 0.3923 - val_loss: 1.0690 - val_acc: 0.4104\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 1.0433 - acc: 0.4504 - val_loss: 1.0325 - val_acc: 0.4617\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 1.0055 - acc: 0.5028 - val_loss: 0.9997 - val_acc: 0.5186\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.9693 - acc: 0.5588 - val_loss: 0.9677 - val_acc: 0.5429\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.9332 - acc: 0.5978 - val_loss: 0.9375 - val_acc: 0.5697\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.8973 - acc: 0.6315 - val_loss: 0.9064 - val_acc: 0.6065\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.8610 - acc: 0.6629 - val_loss: 0.8768 - val_acc: 0.6305\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.8254 - acc: 0.6889 - val_loss: 0.8484 - val_acc: 0.6563\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7905 - acc: 0.7132 - val_loss: 0.8213 - val_acc: 0.6662\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7562 - acc: 0.7319 - val_loss: 0.7954 - val_acc: 0.6821\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.7232 - acc: 0.7499 - val_loss: 0.7708 - val_acc: 0.6974\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6913 - acc: 0.7629 - val_loss: 0.7481 - val_acc: 0.7145\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6604 - acc: 0.7801 - val_loss: 0.7255 - val_acc: 0.7188\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6307 - acc: 0.7911 - val_loss: 0.7046 - val_acc: 0.7283\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.6023 - acc: 0.8037 - val_loss: 0.6852 - val_acc: 0.7349\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5751 - acc: 0.8175 - val_loss: 0.6671 - val_acc: 0.7395\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5491 - acc: 0.8284 - val_loss: 0.6494 - val_acc: 0.7485\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.5242 - acc: 0.8365 - val_loss: 0.6330 - val_acc: 0.7597\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5005 - acc: 0.8472 - val_loss: 0.6172 - val_acc: 0.7653\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4778 - acc: 0.8568 - val_loss: 0.6023 - val_acc: 0.7704\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4561 - acc: 0.8639 - val_loss: 0.5883 - val_acc: 0.7776\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4355 - acc: 0.8737 - val_loss: 0.5753 - val_acc: 0.7824\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4157 - acc: 0.8823 - val_loss: 0.5626 - val_acc: 0.7870\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3966 - acc: 0.8888 - val_loss: 0.5514 - val_acc: 0.7901\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3789 - acc: 0.8964 - val_loss: 0.5401 - val_acc: 0.7952\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3614 - acc: 0.9014 - val_loss: 0.5289 - val_acc: 0.7978\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3450 - acc: 0.9086 - val_loss: 0.5191 - val_acc: 0.8018\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3292 - acc: 0.9139 - val_loss: 0.5103 - val_acc: 0.8046\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3143 - acc: 0.9199 - val_loss: 0.5004 - val_acc: 0.8118\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2999 - acc: 0.9252 - val_loss: 0.4920 - val_acc: 0.8113\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2863 - acc: 0.9286 - val_loss: 0.4842 - val_acc: 0.8126\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2734 - acc: 0.9336 - val_loss: 0.4758 - val_acc: 0.8187\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2610 - acc: 0.9375 - val_loss: 0.4687 - val_acc: 0.8220\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2492 - acc: 0.9415 - val_loss: 0.4618 - val_acc: 0.8241\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2380 - acc: 0.9454 - val_loss: 0.4553 - val_acc: 0.8266\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2271 - acc: 0.9487 - val_loss: 0.4493 - val_acc: 0.8266\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2170 - acc: 0.9528 - val_loss: 0.4442 - val_acc: 0.8317\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2072 - acc: 0.9556 - val_loss: 0.4376 - val_acc: 0.8317\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1981 - acc: 0.9579 - val_loss: 0.4325 - val_acc: 0.8361\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1892 - acc: 0.9607 - val_loss: 0.4270 - val_acc: 0.8386\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1805 - acc: 0.9638 - val_loss: 0.4231 - val_acc: 0.8399\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1727 - acc: 0.9650 - val_loss: 0.4179 - val_acc: 0.8407\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1649 - acc: 0.9674 - val_loss: 0.4134 - val_acc: 0.8424\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1574 - acc: 0.9692 - val_loss: 0.4098 - val_acc: 0.8427\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1504 - acc: 0.9714 - val_loss: 0.4068 - val_acc: 0.8450\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1435 - acc: 0.9728 - val_loss: 0.4042 - val_acc: 0.8455\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1372 - acc: 0.9752 - val_loss: 0.3985 - val_acc: 0.8458\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1311 - acc: 0.9762 - val_loss: 0.3957 - val_acc: 0.8455\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1252 - acc: 0.9775 - val_loss: 0.3925 - val_acc: 0.8465\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1198 - acc: 0.9787 - val_loss: 0.3894 - val_acc: 0.8468\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1143 - acc: 0.9804 - val_loss: 0.3868 - val_acc: 0.8481\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.1092 - acc: 0.9814 - val_loss: 0.3846 - val_acc: 0.8483\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1044 - acc: 0.9829 - val_loss: 0.3824 - val_acc: 0.8498\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0997 - acc: 0.9835 - val_loss: 0.3804 - val_acc: 0.8506\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0952 - acc: 0.9854 - val_loss: 0.3784 - val_acc: 0.8504\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0911 - acc: 0.9855 - val_loss: 0.3781 - val_acc: 0.8524\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0869 - acc: 0.9863 - val_loss: 0.3743 - val_acc: 0.8514\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0831 - acc: 0.9876 - val_loss: 0.3729 - val_acc: 0.8516\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0793 - acc: 0.9882 - val_loss: 0.3723 - val_acc: 0.8532\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0760 - acc: 0.9889 - val_loss: 0.3701 - val_acc: 0.8527\n",
      "Epoch 61/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0726 - acc: 0.9893 - val_loss: 0.3684 - val_acc: 0.8542\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0694 - acc: 0.9897 - val_loss: 0.3674 - val_acc: 0.8542\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0663 - acc: 0.9907 - val_loss: 0.3665 - val_acc: 0.8555\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0633 - acc: 0.9913 - val_loss: 0.3652 - val_acc: 0.8560\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0606 - acc: 0.9914 - val_loss: 0.3646 - val_acc: 0.8580\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0579 - acc: 0.9923 - val_loss: 0.3639 - val_acc: 0.8583\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0552 - acc: 0.9927 - val_loss: 0.3642 - val_acc: 0.8583\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0528 - acc: 0.9930 - val_loss: 0.3624 - val_acc: 0.8596\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0504 - acc: 0.9934 - val_loss: 0.3632 - val_acc: 0.8585\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0482 - acc: 0.9938 - val_loss: 0.3617 - val_acc: 0.8598\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0461 - acc: 0.9942 - val_loss: 0.3627 - val_acc: 0.8588\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0442 - acc: 0.9946 - val_loss: 0.3612 - val_acc: 0.8611\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0421 - acc: 0.9951 - val_loss: 0.3619 - val_acc: 0.8590\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0404 - acc: 0.9953 - val_loss: 0.3624 - val_acc: 0.8601\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0384 - acc: 0.9956 - val_loss: 0.3626 - val_acc: 0.8606\n",
      "15663/15663 [==============================] - 0s 25us/step\n",
      "==== Training loss, score are: 0.037526346300010636 0.9963608504117986 =======\n",
      "3916/3916 [==============================] - 0s 26us/step\n",
      "==== CV loss, score are: 0.362552332500637 0.8605720122574055 =======\n",
      "Running fold 4\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_42 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_42  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 69us/step\n",
      "Before training loss, score are: 1.1164819725261659 0.3409947008931499\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 94us/step - loss: 1.0487 - acc: 0.4497 - val_loss: 1.0208 - val_acc: 0.4939\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 1.0059 - acc: 0.5025 - val_loss: 0.9904 - val_acc: 0.5306\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.9729 - acc: 0.5472 - val_loss: 0.9608 - val_acc: 0.5659\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.9398 - acc: 0.5864 - val_loss: 0.9310 - val_acc: 0.5919\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.9061 - acc: 0.6199 - val_loss: 0.9023 - val_acc: 0.6213\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.8720 - acc: 0.6420 - val_loss: 0.8739 - val_acc: 0.6412\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.8377 - acc: 0.6715 - val_loss: 0.8469 - val_acc: 0.6632\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.8039 - acc: 0.6937 - val_loss: 0.8206 - val_acc: 0.6818\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.7702 - acc: 0.7138 - val_loss: 0.7950 - val_acc: 0.6925\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.7375 - acc: 0.7313 - val_loss: 0.7706 - val_acc: 0.7061\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.7055 - acc: 0.7494 - val_loss: 0.7472 - val_acc: 0.7140\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.6745 - acc: 0.7624 - val_loss: 0.7259 - val_acc: 0.7298\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.6445 - acc: 0.7765 - val_loss: 0.7053 - val_acc: 0.7403\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.6155 - acc: 0.7939 - val_loss: 0.6852 - val_acc: 0.7441\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5876 - acc: 0.8058 - val_loss: 0.6671 - val_acc: 0.7510\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5610 - acc: 0.8174 - val_loss: 0.6484 - val_acc: 0.7605\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5357 - acc: 0.8282 - val_loss: 0.6311 - val_acc: 0.7615\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.5110 - acc: 0.8390 - val_loss: 0.6150 - val_acc: 0.7692\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4878 - acc: 0.8485 - val_loss: 0.6003 - val_acc: 0.7783\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4654 - acc: 0.8587 - val_loss: 0.5858 - val_acc: 0.7812\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4441 - acc: 0.8676 - val_loss: 0.5728 - val_acc: 0.7855\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.4237 - acc: 0.8756 - val_loss: 0.5601 - val_acc: 0.7891\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.4041 - acc: 0.8832 - val_loss: 0.5481 - val_acc: 0.7932\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3856 - acc: 0.8915 - val_loss: 0.5367 - val_acc: 0.7955\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3681 - acc: 0.8971 - val_loss: 0.5255 - val_acc: 0.8034\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3509 - acc: 0.9044 - val_loss: 0.5155 - val_acc: 0.8062\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3349 - acc: 0.9112 - val_loss: 0.5054 - val_acc: 0.8108\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.3198 - acc: 0.9167 - val_loss: 0.4963 - val_acc: 0.8136\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.3048 - acc: 0.9221 - val_loss: 0.4881 - val_acc: 0.8151\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2909 - acc: 0.9284 - val_loss: 0.4793 - val_acc: 0.8192\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2771 - acc: 0.9320 - val_loss: 0.4751 - val_acc: 0.8210\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2652 - acc: 0.9366 - val_loss: 0.4659 - val_acc: 0.8230\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2531 - acc: 0.9414 - val_loss: 0.4574 - val_acc: 0.8281\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2413 - acc: 0.9457 - val_loss: 0.4519 - val_acc: 0.8292\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.2305 - acc: 0.9487 - val_loss: 0.4452 - val_acc: 0.8348\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2200 - acc: 0.9520 - val_loss: 0.4388 - val_acc: 0.8363\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.2103 - acc: 0.9554 - val_loss: 0.4346 - val_acc: 0.8376\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.2007 - acc: 0.9575 - val_loss: 0.4281 - val_acc: 0.8394\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1917 - acc: 0.9589 - val_loss: 0.4238 - val_acc: 0.8412\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1830 - acc: 0.9623 - val_loss: 0.4195 - val_acc: 0.8417\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1748 - acc: 0.9651 - val_loss: 0.4143 - val_acc: 0.8450\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1669 - acc: 0.9670 - val_loss: 0.4109 - val_acc: 0.8460\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.1595 - acc: 0.9695 - val_loss: 0.4068 - val_acc: 0.8432\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1523 - acc: 0.9718 - val_loss: 0.4025 - val_acc: 0.8468\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1455 - acc: 0.9740 - val_loss: 0.3998 - val_acc: 0.8501\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1391 - acc: 0.9762 - val_loss: 0.3964 - val_acc: 0.8491\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1330 - acc: 0.9767 - val_loss: 0.3928 - val_acc: 0.8496\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1270 - acc: 0.9784 - val_loss: 0.3901 - val_acc: 0.8529\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.1212 - acc: 0.9799 - val_loss: 0.3893 - val_acc: 0.8529\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1160 - acc: 0.9810 - val_loss: 0.3847 - val_acc: 0.8537\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1109 - acc: 0.9819 - val_loss: 0.3828 - val_acc: 0.8544\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.1058 - acc: 0.9829 - val_loss: 0.3799 - val_acc: 0.8537\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.1011 - acc: 0.9835 - val_loss: 0.3790 - val_acc: 0.8501\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.0967 - acc: 0.9842 - val_loss: 0.3769 - val_acc: 0.8547\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0924 - acc: 0.9856 - val_loss: 0.3746 - val_acc: 0.8560\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0885 - acc: 0.9865 - val_loss: 0.3729 - val_acc: 0.8565\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0844 - acc: 0.9874 - val_loss: 0.3715 - val_acc: 0.8588\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.0807 - acc: 0.9880 - val_loss: 0.3710 - val_acc: 0.8560\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 1s 49us/step - loss: 0.0771 - acc: 0.9885 - val_loss: 0.3696 - val_acc: 0.8578\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0737 - acc: 0.9894 - val_loss: 0.3680 - val_acc: 0.8575\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0705 - acc: 0.9897 - val_loss: 0.3676 - val_acc: 0.8573\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0673 - acc: 0.9899 - val_loss: 0.3660 - val_acc: 0.8593\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0645 - acc: 0.9906 - val_loss: 0.3654 - val_acc: 0.8606\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0617 - acc: 0.9908 - val_loss: 0.3650 - val_acc: 0.8593\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0590 - acc: 0.9912 - val_loss: 0.3638 - val_acc: 0.8585\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0564 - acc: 0.9918 - val_loss: 0.3632 - val_acc: 0.8590\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0538 - acc: 0.9922 - val_loss: 0.3628 - val_acc: 0.8585\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0515 - acc: 0.9928 - val_loss: 0.3625 - val_acc: 0.8588\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0492 - acc: 0.9930 - val_loss: 0.3620 - val_acc: 0.8593\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0470 - acc: 0.9934 - val_loss: 0.3621 - val_acc: 0.8588\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0450 - acc: 0.9936 - val_loss: 0.3618 - val_acc: 0.8601\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0430 - acc: 0.9944 - val_loss: 0.3622 - val_acc: 0.8598\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0412 - acc: 0.9945 - val_loss: 0.3617 - val_acc: 0.8593\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0394 - acc: 0.9952 - val_loss: 0.3625 - val_acc: 0.8598\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 1s 48us/step - loss: 0.0376 - acc: 0.9955 - val_loss: 0.3631 - val_acc: 0.8596\n",
      "Epoch 76/150\n",
      "15663/15663 [==============================] - 1s 47us/step - loss: 0.0362 - acc: 0.9959 - val_loss: 0.3626 - val_acc: 0.8598\n",
      "15663/15663 [==============================] - 0s 29us/step\n",
      "==== Training loss, score are: 0.03467361978794812 0.9958500925748579 =======\n",
      "3916/3916 [==============================] - 0s 25us/step\n",
      "==== CV loss, score are: 0.3626462497707529 0.8598059244735492 =======\n",
      "Running fold 5\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 20, sentence_maxlength_cap 161, word_vector_type gensim \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_43 (Embedding)     (None, 161, 20)           4946920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_43  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 4,946,983\n",
      "Trainable params: 4,946,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15664/15664 [==============================] - 1s 68us/step\n",
      "Before training loss, score are: 1.1358444641997791 0.3150536261491318\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/150\n",
      "15664/15664 [==============================] - 1s 94us/step - loss: 1.0948 - acc: 0.3810 - val_loss: 1.0665 - val_acc: 0.4345\n",
      "Epoch 2/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 1.0491 - acc: 0.4484 - val_loss: 1.0269 - val_acc: 0.4713\n",
      "Epoch 3/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 1.0107 - acc: 0.5015 - val_loss: 0.9923 - val_acc: 0.5333\n",
      "Epoch 4/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.9737 - acc: 0.5550 - val_loss: 0.9587 - val_acc: 0.5839\n",
      "Epoch 5/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.9371 - acc: 0.5984 - val_loss: 0.9262 - val_acc: 0.6179\n",
      "Epoch 6/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.9002 - acc: 0.6385 - val_loss: 0.8930 - val_acc: 0.6388\n",
      "Epoch 7/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.8630 - acc: 0.6643 - val_loss: 0.8624 - val_acc: 0.6669\n",
      "Epoch 8/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.8257 - acc: 0.6909 - val_loss: 0.8305 - val_acc: 0.6764\n",
      "Epoch 9/150\n",
      "15664/15664 [==============================] - 1s 51us/step - loss: 0.7891 - acc: 0.7141 - val_loss: 0.8011 - val_acc: 0.6996\n",
      "Epoch 10/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.7533 - acc: 0.7365 - val_loss: 0.7733 - val_acc: 0.7114\n",
      "Epoch 11/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.7183 - acc: 0.7527 - val_loss: 0.7472 - val_acc: 0.7282\n",
      "Epoch 12/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.6853 - acc: 0.7684 - val_loss: 0.7221 - val_acc: 0.7382\n",
      "Epoch 13/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.6528 - acc: 0.7845 - val_loss: 0.6981 - val_acc: 0.7443\n",
      "Epoch 14/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.6223 - acc: 0.7980 - val_loss: 0.6769 - val_acc: 0.7545\n",
      "Epoch 15/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.5928 - acc: 0.8114 - val_loss: 0.6561 - val_acc: 0.7612\n",
      "Epoch 16/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.5651 - acc: 0.8249 - val_loss: 0.6368 - val_acc: 0.7640\n",
      "Epoch 17/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.5385 - acc: 0.8333 - val_loss: 0.6205 - val_acc: 0.7745\n",
      "Epoch 18/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.5132 - acc: 0.8447 - val_loss: 0.6029 - val_acc: 0.7783\n",
      "Epoch 19/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.4891 - acc: 0.8551 - val_loss: 0.5869 - val_acc: 0.7842\n",
      "Epoch 20/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.4665 - acc: 0.8663 - val_loss: 0.5727 - val_acc: 0.7921\n",
      "Epoch 21/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.4447 - acc: 0.8749 - val_loss: 0.5590 - val_acc: 0.7941\n",
      "Epoch 22/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.4245 - acc: 0.8825 - val_loss: 0.5452 - val_acc: 0.7990\n",
      "Epoch 23/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.4047 - acc: 0.8885 - val_loss: 0.5343 - val_acc: 0.8033\n",
      "Epoch 24/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.3860 - acc: 0.8954 - val_loss: 0.5224 - val_acc: 0.8066\n",
      "Epoch 25/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.3682 - acc: 0.9042 - val_loss: 0.5114 - val_acc: 0.8110\n",
      "Epoch 26/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.3516 - acc: 0.9081 - val_loss: 0.5015 - val_acc: 0.8133\n",
      "Epoch 27/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.3354 - acc: 0.9147 - val_loss: 0.4917 - val_acc: 0.8176\n",
      "Epoch 28/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.3203 - acc: 0.9197 - val_loss: 0.4826 - val_acc: 0.8199\n",
      "Epoch 29/150\n",
      "15664/15664 [==============================] - 1s 51us/step - loss: 0.3057 - acc: 0.9245 - val_loss: 0.4741 - val_acc: 0.8238\n",
      "Epoch 30/150\n",
      "15664/15664 [==============================] - 1s 51us/step - loss: 0.2918 - acc: 0.9302 - val_loss: 0.4669 - val_acc: 0.8253\n",
      "Epoch 31/150\n",
      "15664/15664 [==============================] - 1s 52us/step - loss: 0.2788 - acc: 0.9340 - val_loss: 0.4593 - val_acc: 0.8301\n",
      "Epoch 32/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.2662 - acc: 0.9390 - val_loss: 0.4518 - val_acc: 0.8314\n",
      "Epoch 33/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.2542 - acc: 0.9438 - val_loss: 0.4446 - val_acc: 0.8312\n",
      "Epoch 34/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.2430 - acc: 0.9462 - val_loss: 0.4384 - val_acc: 0.8330\n",
      "Epoch 35/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.2320 - acc: 0.9498 - val_loss: 0.4325 - val_acc: 0.8352\n",
      "Epoch 36/150\n",
      "15664/15664 [==============================] - 1s 51us/step - loss: 0.2216 - acc: 0.9537 - val_loss: 0.4271 - val_acc: 0.8388\n",
      "Epoch 37/150\n",
      "15664/15664 [==============================] - 1s 51us/step - loss: 0.2118 - acc: 0.9563 - val_loss: 0.4214 - val_acc: 0.8406\n",
      "Epoch 38/150\n",
      "15664/15664 [==============================] - 1s 51us/step - loss: 0.2024 - acc: 0.9590 - val_loss: 0.4163 - val_acc: 0.8421\n",
      "Epoch 39/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.1934 - acc: 0.9603 - val_loss: 0.4117 - val_acc: 0.8457\n",
      "Epoch 40/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.1847 - acc: 0.9630 - val_loss: 0.4071 - val_acc: 0.8442\n",
      "Epoch 41/150\n",
      "15664/15664 [==============================] - 1s 51us/step - loss: 0.1765 - acc: 0.9646 - val_loss: 0.4033 - val_acc: 0.8480\n",
      "Epoch 42/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.1688 - acc: 0.9675 - val_loss: 0.3988 - val_acc: 0.8493\n",
      "Epoch 43/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1612 - acc: 0.9695 - val_loss: 0.3951 - val_acc: 0.8501\n",
      "Epoch 44/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1541 - acc: 0.9712 - val_loss: 0.3930 - val_acc: 0.8511\n",
      "Epoch 45/150\n",
      "15664/15664 [==============================] - 1s 51us/step - loss: 0.1474 - acc: 0.9734 - val_loss: 0.3881 - val_acc: 0.8521\n",
      "Epoch 46/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.1410 - acc: 0.9761 - val_loss: 0.3852 - val_acc: 0.8521\n",
      "Epoch 47/150\n",
      "15664/15664 [==============================] - 1s 51us/step - loss: 0.1346 - acc: 0.9767 - val_loss: 0.3823 - val_acc: 0.8524\n",
      "Epoch 48/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.1286 - acc: 0.9779 - val_loss: 0.3801 - val_acc: 0.8547\n",
      "Epoch 49/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.1229 - acc: 0.9789 - val_loss: 0.3776 - val_acc: 0.8529\n",
      "Epoch 50/150\n",
      "15664/15664 [==============================] - 1s 50us/step - loss: 0.1176 - acc: 0.9805 - val_loss: 0.3753 - val_acc: 0.8547\n",
      "Epoch 51/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.1123 - acc: 0.9817 - val_loss: 0.3733 - val_acc: 0.8547\n",
      "Epoch 52/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.1073 - acc: 0.9831 - val_loss: 0.3717 - val_acc: 0.8559\n",
      "Epoch 53/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.1027 - acc: 0.9838 - val_loss: 0.3692 - val_acc: 0.8534\n",
      "Epoch 54/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0982 - acc: 0.9849 - val_loss: 0.3669 - val_acc: 0.8552\n",
      "Epoch 55/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0940 - acc: 0.9859 - val_loss: 0.3654 - val_acc: 0.8554\n",
      "Epoch 56/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0897 - acc: 0.9866 - val_loss: 0.3638 - val_acc: 0.8552\n",
      "Epoch 57/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0858 - acc: 0.9870 - val_loss: 0.3625 - val_acc: 0.8562\n",
      "Epoch 58/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0821 - acc: 0.9881 - val_loss: 0.3613 - val_acc: 0.8557\n",
      "Epoch 59/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.0784 - acc: 0.9886 - val_loss: 0.3602 - val_acc: 0.8559\n",
      "Epoch 60/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0749 - acc: 0.9887 - val_loss: 0.3592 - val_acc: 0.8552\n",
      "Epoch 61/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0717 - acc: 0.9897 - val_loss: 0.3585 - val_acc: 0.8547\n",
      "Epoch 62/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0686 - acc: 0.9900 - val_loss: 0.3582 - val_acc: 0.8567\n",
      "Epoch 63/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0656 - acc: 0.9906 - val_loss: 0.3572 - val_acc: 0.8559\n",
      "Epoch 64/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0627 - acc: 0.9908 - val_loss: 0.3575 - val_acc: 0.8562\n",
      "Epoch 65/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0600 - acc: 0.9921 - val_loss: 0.3573 - val_acc: 0.8582\n",
      "Epoch 66/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0575 - acc: 0.9921 - val_loss: 0.3557 - val_acc: 0.8570\n",
      "Epoch 67/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0548 - acc: 0.9929 - val_loss: 0.3570 - val_acc: 0.8582\n",
      "Epoch 68/150\n",
      "15664/15664 [==============================] - 1s 49us/step - loss: 0.0526 - acc: 0.9928 - val_loss: 0.3554 - val_acc: 0.8567\n",
      "Epoch 69/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0502 - acc: 0.9937 - val_loss: 0.3550 - val_acc: 0.8582\n",
      "Epoch 70/150\n",
      "15664/15664 [==============================] - 1s 47us/step - loss: 0.0480 - acc: 0.9936 - val_loss: 0.3555 - val_acc: 0.8595\n",
      "Epoch 71/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0460 - acc: 0.9941 - val_loss: 0.3551 - val_acc: 0.8593\n",
      "Epoch 72/150\n",
      "15664/15664 [==============================] - 1s 48us/step - loss: 0.0440 - acc: 0.9944 - val_loss: 0.3561 - val_acc: 0.8590\n",
      "15664/15664 [==============================] - 0s 25us/step\n",
      "==== Training loss, score are: 0.04242006542714764 0.9948289070480082 =======\n",
      "3915/3915 [==============================] - 0s 26us/step\n",
      "==== CV loss, score are: 0.356064371366915 0.8590038314937479 =======\n",
      "\n",
      "\n",
      "===== Model: fast_text_char_gensim  ========:\n",
      " Cross-val log losses are: [0.36850243459488807, 0.36633918897628798, 0.36255233081937394, 0.36264624448035826, 0.35606436668685643]\n",
      "====== Mean cross-val log loss is: 0.3632209131115529 =========\n",
      "\n",
      "\n",
      "Timestamp: 2018-Jan-14 03:18:30\n",
      "Running kfold training with model fast_text_char_glove\n",
      "Shapes: x_train_raw.shape (19579, 42), y_train_raw.shape (19579, 3) , x_test_raw.shape (8392, 41)\n",
      "Running preprocess: <function run_NN_preprocess at 0x7f620ee8eea0>\n",
      "No of unique words found is 247346\n",
      "No of words occuring more than 2 times is 75104\n",
      "No of unique words found is after capping is 247346\n",
      "max sentence length is 1439\n",
      "mean, median, 99th perc length of sentence is 50.867358 44.000000 161.000000\n",
      "After padding: max sentence size is 161\n",
      "After padding: mean, median length of sentence is 161.000000 161.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4954it [00:00, 49533.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word embedding matrix for glove vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:08, 49388.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "of--mirth 247345\n",
      "None\n",
      "(247346, 100)\n",
      "Shapes: x_train.shape (19579, 161), y_train.shape (19579, 3) , x_test.shape (8392, 161)\n",
      "Running fold 1\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_44 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_44  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 24,734,903\n",
      "Trainable params: 303\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 73us/step\n",
      "Before training loss, score are: 1.1214955906023782 0.28640745707107884\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 63us/step - loss: 1.0913 - acc: 0.3850 - val_loss: 1.0819 - val_acc: 0.3910\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0740 - acc: 0.4084 - val_loss: 1.0707 - val_acc: 0.3956\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0631 - acc: 0.4181 - val_loss: 1.0604 - val_acc: 0.4104\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0530 - acc: 0.4373 - val_loss: 1.0505 - val_acc: 0.4367\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0436 - acc: 0.4530 - val_loss: 1.0414 - val_acc: 0.4660\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0350 - acc: 0.4691 - val_loss: 1.0330 - val_acc: 0.4888\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0268 - acc: 0.4844 - val_loss: 1.0255 - val_acc: 0.4857\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0193 - acc: 0.4950 - val_loss: 1.0184 - val_acc: 0.4918\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0123 - acc: 0.5000 - val_loss: 1.0112 - val_acc: 0.5082\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0057 - acc: 0.5106 - val_loss: 1.0049 - val_acc: 0.5151\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9994 - acc: 0.5171 - val_loss: 0.9992 - val_acc: 0.5273\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9934 - acc: 0.5264 - val_loss: 0.9931 - val_acc: 0.5324\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9879 - acc: 0.5326 - val_loss: 0.9878 - val_acc: 0.5337\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9825 - acc: 0.5368 - val_loss: 0.9823 - val_acc: 0.5383\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9776 - acc: 0.5438 - val_loss: 0.9778 - val_acc: 0.5449\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9727 - acc: 0.5473 - val_loss: 0.9727 - val_acc: 0.5529\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9680 - acc: 0.5541 - val_loss: 0.9684 - val_acc: 0.5531\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9636 - acc: 0.5585 - val_loss: 0.9642 - val_acc: 0.5554\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9595 - acc: 0.5593 - val_loss: 0.9599 - val_acc: 0.5603\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9554 - acc: 0.5662 - val_loss: 0.9560 - val_acc: 0.5644\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9516 - acc: 0.5668 - val_loss: 0.9521 - val_acc: 0.5674\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9478 - acc: 0.5731 - val_loss: 0.9485 - val_acc: 0.5692\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9443 - acc: 0.5748 - val_loss: 0.9449 - val_acc: 0.5741\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9409 - acc: 0.5778 - val_loss: 0.9417 - val_acc: 0.5753\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9373 - acc: 0.5817 - val_loss: 0.9398 - val_acc: 0.5695\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9344 - acc: 0.5811 - val_loss: 0.9352 - val_acc: 0.5774\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9312 - acc: 0.5867 - val_loss: 0.9330 - val_acc: 0.5728\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9282 - acc: 0.5847 - val_loss: 0.9290 - val_acc: 0.5850\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9254 - acc: 0.5908 - val_loss: 0.9262 - val_acc: 0.5866\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9225 - acc: 0.5899 - val_loss: 0.9237 - val_acc: 0.5909\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9199 - acc: 0.5941 - val_loss: 0.9211 - val_acc: 0.5889\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9173 - acc: 0.5943 - val_loss: 0.9184 - val_acc: 0.5912\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9147 - acc: 0.5941 - val_loss: 0.9159 - val_acc: 0.5953\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9122 - acc: 0.5980 - val_loss: 0.9139 - val_acc: 0.5922\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9099 - acc: 0.5977 - val_loss: 0.9112 - val_acc: 0.5955\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9075 - acc: 0.6003 - val_loss: 0.9093 - val_acc: 0.5955\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9054 - acc: 0.6008 - val_loss: 0.9066 - val_acc: 0.5986\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9033 - acc: 0.6028 - val_loss: 0.9043 - val_acc: 0.5981\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9009 - acc: 0.6027 - val_loss: 0.9024 - val_acc: 0.5986\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8991 - acc: 0.6057 - val_loss: 0.9005 - val_acc: 0.6006\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8970 - acc: 0.6051 - val_loss: 0.8985 - val_acc: 0.5998\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8949 - acc: 0.6068 - val_loss: 0.8964 - val_acc: 0.6062\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8931 - acc: 0.6088 - val_loss: 0.8952 - val_acc: 0.6004\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8914 - acc: 0.6075 - val_loss: 0.8928 - val_acc: 0.6044\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8895 - acc: 0.6117 - val_loss: 0.8914 - val_acc: 0.6029\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8879 - acc: 0.6108 - val_loss: 0.8892 - val_acc: 0.6090\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8860 - acc: 0.6137 - val_loss: 0.8880 - val_acc: 0.6055\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8846 - acc: 0.6122 - val_loss: 0.8860 - val_acc: 0.6103\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8828 - acc: 0.6140 - val_loss: 0.8843 - val_acc: 0.6126\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8813 - acc: 0.6156 - val_loss: 0.8827 - val_acc: 0.6144\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8797 - acc: 0.6153 - val_loss: 0.8812 - val_acc: 0.6152\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8781 - acc: 0.6181 - val_loss: 0.8799 - val_acc: 0.6157\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8766 - acc: 0.6178 - val_loss: 0.8786 - val_acc: 0.6147\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8753 - acc: 0.6192 - val_loss: 0.8769 - val_acc: 0.6159\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8739 - acc: 0.6203 - val_loss: 0.8756 - val_acc: 0.6170\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8725 - acc: 0.6211 - val_loss: 0.8747 - val_acc: 0.6159\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8711 - acc: 0.6212 - val_loss: 0.8732 - val_acc: 0.6185\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8700 - acc: 0.6217 - val_loss: 0.8719 - val_acc: 0.6221\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8686 - acc: 0.6228 - val_loss: 0.8704 - val_acc: 0.6221\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8674 - acc: 0.6234 - val_loss: 0.8692 - val_acc: 0.6216\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8661 - acc: 0.6234 - val_loss: 0.8681 - val_acc: 0.6190\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8648 - acc: 0.6244 - val_loss: 0.8671 - val_acc: 0.6193\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8638 - acc: 0.6240 - val_loss: 0.8656 - val_acc: 0.6190\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8625 - acc: 0.6249 - val_loss: 0.8646 - val_acc: 0.6236\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 0s 19us/step - loss: 0.8615 - acc: 0.6250 - val_loss: 0.8638 - val_acc: 0.6198\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8603 - acc: 0.6262 - val_loss: 0.8625 - val_acc: 0.6251\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8593 - acc: 0.6257 - val_loss: 0.8614 - val_acc: 0.6221\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8583 - acc: 0.6283 - val_loss: 0.8608 - val_acc: 0.6261\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8572 - acc: 0.6264 - val_loss: 0.8592 - val_acc: 0.6244\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8562 - acc: 0.6291 - val_loss: 0.8584 - val_acc: 0.6269\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8550 - acc: 0.6275 - val_loss: 0.8573 - val_acc: 0.6274\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8542 - acc: 0.6288 - val_loss: 0.8569 - val_acc: 0.6239\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8533 - acc: 0.6293 - val_loss: 0.8556 - val_acc: 0.6287\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8522 - acc: 0.6287 - val_loss: 0.8550 - val_acc: 0.6264\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8515 - acc: 0.6305 - val_loss: 0.8540 - val_acc: 0.6290\n",
      "Epoch 76/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8505 - acc: 0.6295 - val_loss: 0.8529 - val_acc: 0.6295\n",
      "Epoch 77/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8496 - acc: 0.6317 - val_loss: 0.8522 - val_acc: 0.6305\n",
      "Epoch 78/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8487 - acc: 0.6304 - val_loss: 0.8515 - val_acc: 0.6305\n",
      "Epoch 79/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8478 - acc: 0.6310 - val_loss: 0.8504 - val_acc: 0.6328\n",
      "Epoch 80/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8469 - acc: 0.6316 - val_loss: 0.8506 - val_acc: 0.6315\n",
      "Epoch 81/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8463 - acc: 0.6320 - val_loss: 0.8487 - val_acc: 0.6320\n",
      "Epoch 82/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8454 - acc: 0.6314 - val_loss: 0.8483 - val_acc: 0.6313\n",
      "Epoch 83/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8445 - acc: 0.6328 - val_loss: 0.8470 - val_acc: 0.6353\n",
      "Epoch 84/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8437 - acc: 0.6324 - val_loss: 0.8461 - val_acc: 0.6364\n",
      "Epoch 85/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8431 - acc: 0.6340 - val_loss: 0.8461 - val_acc: 0.6353\n",
      "Epoch 86/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8423 - acc: 0.6337 - val_loss: 0.8450 - val_acc: 0.6361\n",
      "Epoch 87/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8414 - acc: 0.6346 - val_loss: 0.8453 - val_acc: 0.6361\n",
      "Epoch 88/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8410 - acc: 0.6329 - val_loss: 0.8435 - val_acc: 0.6356\n",
      "Epoch 89/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8401 - acc: 0.6335 - val_loss: 0.8426 - val_acc: 0.6387\n",
      "Epoch 90/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8393 - acc: 0.6346 - val_loss: 0.8420 - val_acc: 0.6379\n",
      "Epoch 91/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8387 - acc: 0.6339 - val_loss: 0.8412 - val_acc: 0.6389\n",
      "Epoch 92/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8380 - acc: 0.6351 - val_loss: 0.8407 - val_acc: 0.6382\n",
      "Epoch 93/150\n",
      "15663/15663 [==============================] - 0s 18us/step - loss: 0.8374 - acc: 0.6364 - val_loss: 0.8400 - val_acc: 0.6394\n",
      "Epoch 94/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8368 - acc: 0.6344 - val_loss: 0.8393 - val_acc: 0.6382\n",
      "Epoch 95/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8359 - acc: 0.6350 - val_loss: 0.8386 - val_acc: 0.6392\n",
      "Epoch 96/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8353 - acc: 0.6371 - val_loss: 0.8383 - val_acc: 0.6389\n",
      "Epoch 97/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8348 - acc: 0.6376 - val_loss: 0.8377 - val_acc: 0.6404\n",
      "Epoch 98/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8341 - acc: 0.6368 - val_loss: 0.8371 - val_acc: 0.6394\n",
      "Epoch 99/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8335 - acc: 0.6367 - val_loss: 0.8366 - val_acc: 0.6412\n",
      "Epoch 100/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8328 - acc: 0.6383 - val_loss: 0.8360 - val_acc: 0.6430\n",
      "Epoch 101/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8323 - acc: 0.6369 - val_loss: 0.8352 - val_acc: 0.6420\n",
      "Epoch 102/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8316 - acc: 0.6386 - val_loss: 0.8350 - val_acc: 0.6443\n",
      "Epoch 103/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8312 - acc: 0.6379 - val_loss: 0.8341 - val_acc: 0.6430\n",
      "Epoch 104/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8305 - acc: 0.6391 - val_loss: 0.8335 - val_acc: 0.6427\n",
      "Epoch 105/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8300 - acc: 0.6391 - val_loss: 0.8332 - val_acc: 0.6435\n",
      "Epoch 106/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8294 - acc: 0.6387 - val_loss: 0.8326 - val_acc: 0.6440\n",
      "Epoch 107/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8288 - acc: 0.6378 - val_loss: 0.8319 - val_acc: 0.6438\n",
      "Epoch 108/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8284 - acc: 0.6395 - val_loss: 0.8315 - val_acc: 0.6448\n",
      "Epoch 109/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8279 - acc: 0.6398 - val_loss: 0.8311 - val_acc: 0.6458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8274 - acc: 0.6403 - val_loss: 0.8306 - val_acc: 0.6450\n",
      "Epoch 111/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8268 - acc: 0.6400 - val_loss: 0.8301 - val_acc: 0.6456\n",
      "Epoch 112/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8263 - acc: 0.6411 - val_loss: 0.8297 - val_acc: 0.6463\n",
      "Epoch 113/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8258 - acc: 0.6406 - val_loss: 0.8290 - val_acc: 0.6473\n",
      "Epoch 114/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8253 - acc: 0.6425 - val_loss: 0.8287 - val_acc: 0.6481\n",
      "Epoch 115/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8249 - acc: 0.6418 - val_loss: 0.8284 - val_acc: 0.6491\n",
      "Epoch 116/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8244 - acc: 0.6413 - val_loss: 0.8277 - val_acc: 0.6461\n",
      "Epoch 117/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8239 - acc: 0.6427 - val_loss: 0.8273 - val_acc: 0.6484\n",
      "Epoch 118/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8235 - acc: 0.6422 - val_loss: 0.8269 - val_acc: 0.6499\n",
      "Epoch 119/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8231 - acc: 0.6423 - val_loss: 0.8267 - val_acc: 0.6489\n",
      "Epoch 120/150\n",
      "15663/15663 [==============================] - 0s 20us/step - loss: 0.8225 - acc: 0.6420 - val_loss: 0.8264 - val_acc: 0.6486\n",
      "Epoch 121/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8220 - acc: 0.6427 - val_loss: 0.8262 - val_acc: 0.6496\n",
      "Epoch 122/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8216 - acc: 0.6430 - val_loss: 0.8255 - val_acc: 0.6504\n",
      "Epoch 123/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8211 - acc: 0.6416 - val_loss: 0.8252 - val_acc: 0.6509\n",
      "Epoch 124/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8207 - acc: 0.6423 - val_loss: 0.8243 - val_acc: 0.6499\n",
      "Epoch 125/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8203 - acc: 0.6427 - val_loss: 0.8239 - val_acc: 0.6496\n",
      "Epoch 126/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8199 - acc: 0.6435 - val_loss: 0.8238 - val_acc: 0.6519\n",
      "Epoch 127/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8196 - acc: 0.6429 - val_loss: 0.8232 - val_acc: 0.6504\n",
      "Epoch 128/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8190 - acc: 0.6439 - val_loss: 0.8233 - val_acc: 0.6514\n",
      "Epoch 129/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8187 - acc: 0.6431 - val_loss: 0.8223 - val_acc: 0.6509\n",
      "Epoch 130/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8183 - acc: 0.6439 - val_loss: 0.8220 - val_acc: 0.6522\n",
      "Epoch 131/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8179 - acc: 0.6446 - val_loss: 0.8217 - val_acc: 0.6527\n",
      "Epoch 132/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8175 - acc: 0.6438 - val_loss: 0.8213 - val_acc: 0.6512\n",
      "Epoch 133/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8171 - acc: 0.6445 - val_loss: 0.8210 - val_acc: 0.6522\n",
      "Epoch 134/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8168 - acc: 0.6447 - val_loss: 0.8207 - val_acc: 0.6527\n",
      "Epoch 135/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8164 - acc: 0.6444 - val_loss: 0.8208 - val_acc: 0.6525\n",
      "Epoch 136/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8159 - acc: 0.6458 - val_loss: 0.8203 - val_acc: 0.6519\n",
      "Epoch 137/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8156 - acc: 0.6445 - val_loss: 0.8198 - val_acc: 0.6530\n",
      "Epoch 138/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8153 - acc: 0.6452 - val_loss: 0.8195 - val_acc: 0.6527\n",
      "Epoch 139/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8150 - acc: 0.6452 - val_loss: 0.8190 - val_acc: 0.6532\n",
      "Epoch 140/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8145 - acc: 0.6457 - val_loss: 0.8192 - val_acc: 0.6522\n",
      "Epoch 141/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8143 - acc: 0.6452 - val_loss: 0.8185 - val_acc: 0.6519\n",
      "Epoch 142/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8139 - acc: 0.6457 - val_loss: 0.8181 - val_acc: 0.6530\n",
      "Epoch 143/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8136 - acc: 0.6457 - val_loss: 0.8177 - val_acc: 0.6542\n",
      "Epoch 144/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8133 - acc: 0.6462 - val_loss: 0.8175 - val_acc: 0.6532\n",
      "Epoch 145/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8128 - acc: 0.6469 - val_loss: 0.8172 - val_acc: 0.6535\n",
      "Epoch 146/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8125 - acc: 0.6471 - val_loss: 0.8173 - val_acc: 0.6525\n",
      "Epoch 147/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8123 - acc: 0.6454 - val_loss: 0.8165 - val_acc: 0.6542\n",
      "Epoch 148/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8119 - acc: 0.6466 - val_loss: 0.8162 - val_acc: 0.6540\n",
      "Epoch 149/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8117 - acc: 0.6471 - val_loss: 0.8161 - val_acc: 0.6540\n",
      "Epoch 150/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8113 - acc: 0.6462 - val_loss: 0.8160 - val_acc: 0.6537\n",
      "15663/15663 [==============================] - 0s 26us/step\n",
      "==== Training loss, score are: 0.8109515775119954 0.6466194215933878 =======\n",
      "3916/3916 [==============================] - 0s 25us/step\n",
      "==== CV loss, score are: 0.8159530744245761 0.6537282942386156 =======\n",
      "Running fold 2\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_45 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_45  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 24,734,903\n",
      "Trainable params: 303\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 72us/step\n",
      "Before training loss, score are: 1.1027436079912272 0.2971972163831077\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 62us/step - loss: 1.0891 - acc: 0.3957 - val_loss: 1.0830 - val_acc: 0.3943\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0751 - acc: 0.4061 - val_loss: 1.0711 - val_acc: 0.3950\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0633 - acc: 0.4152 - val_loss: 1.0612 - val_acc: 0.3981\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0526 - acc: 0.4289 - val_loss: 1.0511 - val_acc: 0.4377\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 1.0430 - acc: 0.4550 - val_loss: 1.0425 - val_acc: 0.4517\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0340 - acc: 0.4695 - val_loss: 1.0342 - val_acc: 0.4696\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0257 - acc: 0.4878 - val_loss: 1.0268 - val_acc: 0.4768\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0181 - acc: 0.4992 - val_loss: 1.0195 - val_acc: 0.4903\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0108 - acc: 0.5090 - val_loss: 1.0130 - val_acc: 0.4923\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0041 - acc: 0.5175 - val_loss: 1.0074 - val_acc: 0.4923\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9979 - acc: 0.5217 - val_loss: 1.0010 - val_acc: 0.5033\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9918 - acc: 0.5314 - val_loss: 0.9949 - val_acc: 0.5217\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9861 - acc: 0.5379 - val_loss: 0.9894 - val_acc: 0.5386\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9809 - acc: 0.5427 - val_loss: 0.9845 - val_acc: 0.5360\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9757 - acc: 0.5477 - val_loss: 0.9796 - val_acc: 0.5485\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9708 - acc: 0.5547 - val_loss: 0.9751 - val_acc: 0.5513\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9662 - acc: 0.5614 - val_loss: 0.9712 - val_acc: 0.5539\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.9618 - acc: 0.5628 - val_loss: 0.9665 - val_acc: 0.5564\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.9576 - acc: 0.5664 - val_loss: 0.9627 - val_acc: 0.5564\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9535 - acc: 0.5705 - val_loss: 0.9588 - val_acc: 0.5600\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9496 - acc: 0.5708 - val_loss: 0.9554 - val_acc: 0.5649\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9460 - acc: 0.5745 - val_loss: 0.9517 - val_acc: 0.5651\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9423 - acc: 0.5765 - val_loss: 0.9481 - val_acc: 0.5705\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9389 - acc: 0.5802 - val_loss: 0.9448 - val_acc: 0.5728\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9354 - acc: 0.5814 - val_loss: 0.9418 - val_acc: 0.5751\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9322 - acc: 0.5841 - val_loss: 0.9387 - val_acc: 0.5784\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9291 - acc: 0.5867 - val_loss: 0.9352 - val_acc: 0.5830\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9260 - acc: 0.5895 - val_loss: 0.9325 - val_acc: 0.5845\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9231 - acc: 0.5948 - val_loss: 0.9300 - val_acc: 0.5840\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9203 - acc: 0.5924 - val_loss: 0.9269 - val_acc: 0.5855\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9176 - acc: 0.5967 - val_loss: 0.9244 - val_acc: 0.5878\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9150 - acc: 0.5980 - val_loss: 0.9219 - val_acc: 0.5889\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9124 - acc: 0.5996 - val_loss: 0.9197 - val_acc: 0.5863\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9099 - acc: 0.6019 - val_loss: 0.9175 - val_acc: 0.5876\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9076 - acc: 0.6042 - val_loss: 0.9149 - val_acc: 0.5919\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.9051 - acc: 0.6048 - val_loss: 0.9125 - val_acc: 0.5978\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9029 - acc: 0.6054 - val_loss: 0.9103 - val_acc: 0.5986\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9007 - acc: 0.6091 - val_loss: 0.9089 - val_acc: 0.5907\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8986 - acc: 0.6076 - val_loss: 0.9063 - val_acc: 0.6034\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8964 - acc: 0.6139 - val_loss: 0.9047 - val_acc: 0.5998\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 0s 18us/step - loss: 0.8944 - acc: 0.6097 - val_loss: 0.9027 - val_acc: 0.6034\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8925 - acc: 0.6125 - val_loss: 0.9005 - val_acc: 0.6034\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8905 - acc: 0.6137 - val_loss: 0.8987 - val_acc: 0.6037\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8887 - acc: 0.6144 - val_loss: 0.8968 - val_acc: 0.6065\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8869 - acc: 0.6164 - val_loss: 0.8954 - val_acc: 0.6060\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8850 - acc: 0.6155 - val_loss: 0.8937 - val_acc: 0.6062\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8833 - acc: 0.6158 - val_loss: 0.8921 - val_acc: 0.6070\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8818 - acc: 0.6173 - val_loss: 0.8907 - val_acc: 0.6108\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8802 - acc: 0.6180 - val_loss: 0.8889 - val_acc: 0.6098\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8784 - acc: 0.6180 - val_loss: 0.8874 - val_acc: 0.6108\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8770 - acc: 0.6190 - val_loss: 0.8858 - val_acc: 0.6113\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8754 - acc: 0.6192 - val_loss: 0.8852 - val_acc: 0.6124\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8740 - acc: 0.6212 - val_loss: 0.8831 - val_acc: 0.6118\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8726 - acc: 0.6205 - val_loss: 0.8818 - val_acc: 0.6131\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8712 - acc: 0.6213 - val_loss: 0.8807 - val_acc: 0.6134\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8698 - acc: 0.6212 - val_loss: 0.8790 - val_acc: 0.6157\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8685 - acc: 0.6231 - val_loss: 0.8779 - val_acc: 0.6162\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8672 - acc: 0.6238 - val_loss: 0.8766 - val_acc: 0.6172\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8658 - acc: 0.6238 - val_loss: 0.8753 - val_acc: 0.6172\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8646 - acc: 0.6246 - val_loss: 0.8744 - val_acc: 0.6167\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8634 - acc: 0.6265 - val_loss: 0.8736 - val_acc: 0.6162\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8622 - acc: 0.6272 - val_loss: 0.8723 - val_acc: 0.6177\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8611 - acc: 0.6259 - val_loss: 0.8709 - val_acc: 0.6200\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8599 - acc: 0.6261 - val_loss: 0.8699 - val_acc: 0.6175\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8588 - acc: 0.6267 - val_loss: 0.8689 - val_acc: 0.6203\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8577 - acc: 0.6287 - val_loss: 0.8680 - val_acc: 0.6187\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8566 - acc: 0.6284 - val_loss: 0.8668 - val_acc: 0.6190\n",
      "Epoch 68/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8556 - acc: 0.6284 - val_loss: 0.8659 - val_acc: 0.6177\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8545 - acc: 0.6297 - val_loss: 0.8651 - val_acc: 0.6193\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8536 - acc: 0.6292 - val_loss: 0.8639 - val_acc: 0.6170\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8526 - acc: 0.6303 - val_loss: 0.8630 - val_acc: 0.6218\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8515 - acc: 0.6303 - val_loss: 0.8620 - val_acc: 0.6210\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8505 - acc: 0.6313 - val_loss: 0.8615 - val_acc: 0.6198\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8496 - acc: 0.6319 - val_loss: 0.8607 - val_acc: 0.6223\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8487 - acc: 0.6316 - val_loss: 0.8593 - val_acc: 0.6236\n",
      "Epoch 76/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8478 - acc: 0.6323 - val_loss: 0.8587 - val_acc: 0.6218\n",
      "Epoch 77/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8469 - acc: 0.6324 - val_loss: 0.8584 - val_acc: 0.6216\n",
      "Epoch 78/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8460 - acc: 0.6328 - val_loss: 0.8568 - val_acc: 0.6239\n",
      "Epoch 79/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8452 - acc: 0.6344 - val_loss: 0.8564 - val_acc: 0.6239\n",
      "Epoch 80/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8444 - acc: 0.6342 - val_loss: 0.8554 - val_acc: 0.6223\n",
      "Epoch 81/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8435 - acc: 0.6365 - val_loss: 0.8547 - val_acc: 0.6261\n",
      "Epoch 82/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8428 - acc: 0.6341 - val_loss: 0.8536 - val_acc: 0.6246\n",
      "Epoch 83/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8419 - acc: 0.6358 - val_loss: 0.8528 - val_acc: 0.6244\n",
      "Epoch 84/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8412 - acc: 0.6374 - val_loss: 0.8528 - val_acc: 0.6236\n",
      "Epoch 85/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8404 - acc: 0.6362 - val_loss: 0.8518 - val_acc: 0.6246\n",
      "Epoch 86/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8396 - acc: 0.6355 - val_loss: 0.8510 - val_acc: 0.6261\n",
      "Epoch 87/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8388 - acc: 0.6388 - val_loss: 0.8503 - val_acc: 0.6256\n",
      "Epoch 88/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8382 - acc: 0.6383 - val_loss: 0.8494 - val_acc: 0.6251\n",
      "Epoch 89/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8375 - acc: 0.6388 - val_loss: 0.8489 - val_acc: 0.6236\n",
      "Epoch 90/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8367 - acc: 0.6371 - val_loss: 0.8482 - val_acc: 0.6236\n",
      "Epoch 91/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8362 - acc: 0.6392 - val_loss: 0.8474 - val_acc: 0.6279\n",
      "Epoch 92/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8355 - acc: 0.6399 - val_loss: 0.8473 - val_acc: 0.6269\n",
      "Epoch 93/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8347 - acc: 0.6384 - val_loss: 0.8461 - val_acc: 0.6256\n",
      "Epoch 94/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8341 - acc: 0.6405 - val_loss: 0.8462 - val_acc: 0.6264\n",
      "Epoch 95/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8334 - acc: 0.6398 - val_loss: 0.8449 - val_acc: 0.6261\n",
      "Epoch 96/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8328 - acc: 0.6416 - val_loss: 0.8451 - val_acc: 0.6254\n",
      "Epoch 97/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8322 - acc: 0.6403 - val_loss: 0.8438 - val_acc: 0.6267\n",
      "Epoch 98/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8316 - acc: 0.6412 - val_loss: 0.8435 - val_acc: 0.6261\n",
      "Epoch 99/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8308 - acc: 0.6431 - val_loss: 0.8431 - val_acc: 0.6241\n",
      "Epoch 100/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8303 - acc: 0.6414 - val_loss: 0.8422 - val_acc: 0.6272\n",
      "Epoch 101/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8298 - acc: 0.6414 - val_loss: 0.8415 - val_acc: 0.6269\n",
      "Epoch 102/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8292 - acc: 0.6422 - val_loss: 0.8417 - val_acc: 0.6259\n",
      "Epoch 103/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8287 - acc: 0.6423 - val_loss: 0.8408 - val_acc: 0.6261\n",
      "Epoch 104/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8281 - acc: 0.6428 - val_loss: 0.8400 - val_acc: 0.6272\n",
      "Epoch 105/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8275 - acc: 0.6434 - val_loss: 0.8399 - val_acc: 0.6267\n",
      "Epoch 106/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8269 - acc: 0.6429 - val_loss: 0.8392 - val_acc: 0.6287\n",
      "Epoch 107/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8266 - acc: 0.6439 - val_loss: 0.8386 - val_acc: 0.6269\n",
      "Epoch 108/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8259 - acc: 0.6446 - val_loss: 0.8383 - val_acc: 0.6241\n",
      "Epoch 109/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8253 - acc: 0.6439 - val_loss: 0.8375 - val_acc: 0.6287\n",
      "Epoch 110/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8250 - acc: 0.6452 - val_loss: 0.8373 - val_acc: 0.6267\n",
      "Epoch 111/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8244 - acc: 0.6445 - val_loss: 0.8371 - val_acc: 0.6256\n",
      "Epoch 112/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8239 - acc: 0.6436 - val_loss: 0.8361 - val_acc: 0.6287\n",
      "Epoch 113/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8235 - acc: 0.6450 - val_loss: 0.8360 - val_acc: 0.6277\n",
      "Epoch 114/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8229 - acc: 0.6462 - val_loss: 0.8357 - val_acc: 0.6264\n",
      "Epoch 115/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8225 - acc: 0.6451 - val_loss: 0.8350 - val_acc: 0.6272\n",
      "Epoch 116/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8220 - acc: 0.6468 - val_loss: 0.8346 - val_acc: 0.6274\n",
      "Epoch 117/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8215 - acc: 0.6464 - val_loss: 0.8341 - val_acc: 0.6284\n",
      "Epoch 118/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8211 - acc: 0.6459 - val_loss: 0.8335 - val_acc: 0.6290\n",
      "Epoch 119/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8206 - acc: 0.6464 - val_loss: 0.8335 - val_acc: 0.6279\n",
      "Epoch 120/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8201 - acc: 0.6472 - val_loss: 0.8330 - val_acc: 0.6287\n",
      "Epoch 121/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8198 - acc: 0.6475 - val_loss: 0.8325 - val_acc: 0.6284\n",
      "Epoch 122/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8195 - acc: 0.6462 - val_loss: 0.8320 - val_acc: 0.6292\n",
      "Epoch 123/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8189 - acc: 0.6475 - val_loss: 0.8316 - val_acc: 0.6292\n",
      "Epoch 124/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8185 - acc: 0.6480 - val_loss: 0.8312 - val_acc: 0.6292\n",
      "Epoch 125/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8181 - acc: 0.6473 - val_loss: 0.8311 - val_acc: 0.6300\n",
      "Epoch 126/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8176 - acc: 0.6486 - val_loss: 0.8307 - val_acc: 0.6292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8171 - acc: 0.6481 - val_loss: 0.8303 - val_acc: 0.6320\n",
      "Epoch 128/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8170 - acc: 0.6482 - val_loss: 0.8299 - val_acc: 0.6302\n",
      "Epoch 129/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8166 - acc: 0.6476 - val_loss: 0.8295 - val_acc: 0.6302\n",
      "Epoch 130/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8162 - acc: 0.6489 - val_loss: 0.8293 - val_acc: 0.6315\n",
      "Epoch 131/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8157 - acc: 0.6483 - val_loss: 0.8289 - val_acc: 0.6302\n",
      "Epoch 132/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8153 - acc: 0.6488 - val_loss: 0.8285 - val_acc: 0.6310\n",
      "Epoch 133/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8149 - acc: 0.6485 - val_loss: 0.8282 - val_acc: 0.6315\n",
      "Epoch 134/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8146 - acc: 0.6484 - val_loss: 0.8277 - val_acc: 0.6302\n",
      "Epoch 135/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8142 - acc: 0.6496 - val_loss: 0.8276 - val_acc: 0.6313\n",
      "Epoch 136/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8138 - acc: 0.6489 - val_loss: 0.8279 - val_acc: 0.6292\n",
      "Epoch 137/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8134 - acc: 0.6488 - val_loss: 0.8267 - val_acc: 0.6320\n",
      "Epoch 138/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8132 - acc: 0.6503 - val_loss: 0.8264 - val_acc: 0.6318\n",
      "Epoch 139/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8127 - acc: 0.6494 - val_loss: 0.8260 - val_acc: 0.6336\n",
      "Epoch 140/150\n",
      "15663/15663 [==============================] - 0s 19us/step - loss: 0.8124 - acc: 0.6500 - val_loss: 0.8259 - val_acc: 0.6328\n",
      "Epoch 141/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8122 - acc: 0.6494 - val_loss: 0.8256 - val_acc: 0.6328\n",
      "Epoch 142/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8118 - acc: 0.6498 - val_loss: 0.8254 - val_acc: 0.6320\n",
      "Epoch 143/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8114 - acc: 0.6506 - val_loss: 0.8251 - val_acc: 0.6333\n",
      "Epoch 144/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8111 - acc: 0.6505 - val_loss: 0.8247 - val_acc: 0.6323\n",
      "Epoch 145/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8108 - acc: 0.6510 - val_loss: 0.8243 - val_acc: 0.6325\n",
      "Epoch 146/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8105 - acc: 0.6505 - val_loss: 0.8244 - val_acc: 0.6333\n",
      "Epoch 147/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8102 - acc: 0.6506 - val_loss: 0.8239 - val_acc: 0.6325\n",
      "Epoch 148/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8098 - acc: 0.6517 - val_loss: 0.8236 - val_acc: 0.6348\n",
      "Epoch 149/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8095 - acc: 0.6517 - val_loss: 0.8230 - val_acc: 0.6343\n",
      "Epoch 150/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8093 - acc: 0.6498 - val_loss: 0.8231 - val_acc: 0.6320\n",
      "15663/15663 [==============================] - 0s 28us/step\n",
      "==== Training loss, score are: 0.808918109964093 0.6514716210443231 =======\n",
      "3916/3916 [==============================] - 0s 27us/step\n",
      "==== CV loss, score are: 0.8230905103610412 0.6320224719101124 =======\n",
      "Running fold 3\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_46 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_46  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 24,734,903\n",
      "Trainable params: 303\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 76us/step\n",
      "Before training loss, score are: 1.110431992002262 0.28742897274496\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 67us/step - loss: 1.0920 - acc: 0.3882 - val_loss: 1.0842 - val_acc: 0.3996\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0770 - acc: 0.4052 - val_loss: 1.0723 - val_acc: 0.4045\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0654 - acc: 0.4194 - val_loss: 1.0615 - val_acc: 0.4323\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0548 - acc: 0.4371 - val_loss: 1.0516 - val_acc: 0.4433\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0452 - acc: 0.4547 - val_loss: 1.0425 - val_acc: 0.4563\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0362 - acc: 0.4656 - val_loss: 1.0340 - val_acc: 0.4742\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0279 - acc: 0.4832 - val_loss: 1.0264 - val_acc: 0.4734\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0201 - acc: 0.4949 - val_loss: 1.0190 - val_acc: 0.4895\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0131 - acc: 0.5012 - val_loss: 1.0121 - val_acc: 0.4959\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0063 - acc: 0.5125 - val_loss: 1.0056 - val_acc: 0.5046\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0001 - acc: 0.5213 - val_loss: 0.9995 - val_acc: 0.5163\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9940 - acc: 0.5317 - val_loss: 0.9938 - val_acc: 0.5161\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9882 - acc: 0.5353 - val_loss: 0.9883 - val_acc: 0.5235\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9829 - acc: 0.5390 - val_loss: 0.9830 - val_acc: 0.5444\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9777 - acc: 0.5507 - val_loss: 0.9780 - val_acc: 0.5455\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9728 - acc: 0.5539 - val_loss: 0.9732 - val_acc: 0.5521\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9681 - acc: 0.5600 - val_loss: 0.9687 - val_acc: 0.5523\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9636 - acc: 0.5613 - val_loss: 0.9643 - val_acc: 0.5544\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9592 - acc: 0.5682 - val_loss: 0.9604 - val_acc: 0.5536\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9551 - acc: 0.5697 - val_loss: 0.9563 - val_acc: 0.5587\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9512 - acc: 0.5740 - val_loss: 0.9526 - val_acc: 0.5649\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9473 - acc: 0.5773 - val_loss: 0.9488 - val_acc: 0.5669\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9438 - acc: 0.5779 - val_loss: 0.9452 - val_acc: 0.5695\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9402 - acc: 0.5808 - val_loss: 0.9418 - val_acc: 0.5720\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9368 - acc: 0.5835 - val_loss: 0.9386 - val_acc: 0.5771\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9336 - acc: 0.5879 - val_loss: 0.9356 - val_acc: 0.5725\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9304 - acc: 0.5877 - val_loss: 0.9324 - val_acc: 0.5820\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9275 - acc: 0.5931 - val_loss: 0.9297 - val_acc: 0.5794\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9245 - acc: 0.5931 - val_loss: 0.9267 - val_acc: 0.5792\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9216 - acc: 0.5958 - val_loss: 0.9240 - val_acc: 0.5809\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9190 - acc: 0.5978 - val_loss: 0.9214 - val_acc: 0.5843\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9164 - acc: 0.5992 - val_loss: 0.9189 - val_acc: 0.5850\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9138 - acc: 0.6017 - val_loss: 0.9164 - val_acc: 0.5873\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9113 - acc: 0.6022 - val_loss: 0.9142 - val_acc: 0.5884\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9090 - acc: 0.6043 - val_loss: 0.9119 - val_acc: 0.5901\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9066 - acc: 0.6045 - val_loss: 0.9097 - val_acc: 0.5894\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9044 - acc: 0.6052 - val_loss: 0.9074 - val_acc: 0.5912\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9021 - acc: 0.6085 - val_loss: 0.9059 - val_acc: 0.5917\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9001 - acc: 0.6083 - val_loss: 0.9033 - val_acc: 0.5935\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8980 - acc: 0.6101 - val_loss: 0.9014 - val_acc: 0.5937\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8961 - acc: 0.6112 - val_loss: 0.8994 - val_acc: 0.5953\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8940 - acc: 0.6111 - val_loss: 0.8976 - val_acc: 0.5965\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8921 - acc: 0.6125 - val_loss: 0.8959 - val_acc: 0.5958\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8903 - acc: 0.6141 - val_loss: 0.8940 - val_acc: 0.6037\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8885 - acc: 0.6161 - val_loss: 0.8923 - val_acc: 0.6009\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8868 - acc: 0.6155 - val_loss: 0.8907 - val_acc: 0.6050\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8850 - acc: 0.6173 - val_loss: 0.8893 - val_acc: 0.6004\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8833 - acc: 0.6181 - val_loss: 0.8874 - val_acc: 0.6034\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8818 - acc: 0.6190 - val_loss: 0.8860 - val_acc: 0.6062\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8800 - acc: 0.6204 - val_loss: 0.8847 - val_acc: 0.6055\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8785 - acc: 0.6202 - val_loss: 0.8830 - val_acc: 0.6083\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8769 - acc: 0.6218 - val_loss: 0.8815 - val_acc: 0.6073\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8754 - acc: 0.6221 - val_loss: 0.8802 - val_acc: 0.6062\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8740 - acc: 0.6235 - val_loss: 0.8789 - val_acc: 0.6070\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8726 - acc: 0.6230 - val_loss: 0.8776 - val_acc: 0.6067\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8713 - acc: 0.6233 - val_loss: 0.8762 - val_acc: 0.6075\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8698 - acc: 0.6247 - val_loss: 0.8752 - val_acc: 0.6080\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8686 - acc: 0.6245 - val_loss: 0.8737 - val_acc: 0.6090\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8673 - acc: 0.6248 - val_loss: 0.8730 - val_acc: 0.6075\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8661 - acc: 0.6259 - val_loss: 0.8713 - val_acc: 0.6090\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8648 - acc: 0.6246 - val_loss: 0.8703 - val_acc: 0.6098\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8636 - acc: 0.6258 - val_loss: 0.8691 - val_acc: 0.6111\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8624 - acc: 0.6273 - val_loss: 0.8680 - val_acc: 0.6118\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8612 - acc: 0.6296 - val_loss: 0.8670 - val_acc: 0.6134\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8602 - acc: 0.6282 - val_loss: 0.8660 - val_acc: 0.6124\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8590 - acc: 0.6291 - val_loss: 0.8652 - val_acc: 0.6093\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8579 - acc: 0.6291 - val_loss: 0.8638 - val_acc: 0.6157\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8568 - acc: 0.6301 - val_loss: 0.8629 - val_acc: 0.6141\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8558 - acc: 0.6296 - val_loss: 0.8619 - val_acc: 0.6162\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8548 - acc: 0.6305 - val_loss: 0.8610 - val_acc: 0.6162\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8537 - acc: 0.6308 - val_loss: 0.8601 - val_acc: 0.6167\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8528 - acc: 0.6320 - val_loss: 0.8591 - val_acc: 0.6180\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8519 - acc: 0.6312 - val_loss: 0.8584 - val_acc: 0.6154\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8508 - acc: 0.6333 - val_loss: 0.8574 - val_acc: 0.6193\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8500 - acc: 0.6333 - val_loss: 0.8567 - val_acc: 0.6170\n",
      "Epoch 76/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8490 - acc: 0.6341 - val_loss: 0.8560 - val_acc: 0.6167\n",
      "Epoch 77/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8481 - acc: 0.6337 - val_loss: 0.8550 - val_acc: 0.6187\n",
      "Epoch 78/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8473 - acc: 0.6334 - val_loss: 0.8542 - val_acc: 0.6185\n",
      "Epoch 79/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8465 - acc: 0.6346 - val_loss: 0.8533 - val_acc: 0.6210\n",
      "Epoch 80/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8455 - acc: 0.6355 - val_loss: 0.8526 - val_acc: 0.6213\n",
      "Epoch 81/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8447 - acc: 0.6365 - val_loss: 0.8521 - val_acc: 0.6182\n",
      "Epoch 82/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8439 - acc: 0.6357 - val_loss: 0.8512 - val_acc: 0.6195\n",
      "Epoch 83/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8431 - acc: 0.6358 - val_loss: 0.8503 - val_acc: 0.6216\n",
      "Epoch 84/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8423 - acc: 0.6369 - val_loss: 0.8497 - val_acc: 0.6213\n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8416 - acc: 0.6371 - val_loss: 0.8489 - val_acc: 0.6221\n",
      "Epoch 86/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8408 - acc: 0.6376 - val_loss: 0.8483 - val_acc: 0.6241\n",
      "Epoch 87/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8400 - acc: 0.6374 - val_loss: 0.8475 - val_acc: 0.6246\n",
      "Epoch 88/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8394 - acc: 0.6386 - val_loss: 0.8469 - val_acc: 0.6223\n",
      "Epoch 89/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8387 - acc: 0.6383 - val_loss: 0.8462 - val_acc: 0.6241\n",
      "Epoch 90/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8378 - acc: 0.6385 - val_loss: 0.8456 - val_acc: 0.6244\n",
      "Epoch 91/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8369 - acc: 0.6384 - val_loss: 0.8453 - val_acc: 0.6190\n",
      "Epoch 92/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8366 - acc: 0.6402 - val_loss: 0.8444 - val_acc: 0.6218\n",
      "Epoch 93/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8357 - acc: 0.6407 - val_loss: 0.8442 - val_acc: 0.6213\n",
      "Epoch 94/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8351 - acc: 0.6400 - val_loss: 0.8433 - val_acc: 0.6236\n",
      "Epoch 95/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8344 - acc: 0.6394 - val_loss: 0.8426 - val_acc: 0.6231\n",
      "Epoch 96/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8339 - acc: 0.6394 - val_loss: 0.8420 - val_acc: 0.6236\n",
      "Epoch 97/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8332 - acc: 0.6410 - val_loss: 0.8414 - val_acc: 0.6259\n",
      "Epoch 98/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8327 - acc: 0.6413 - val_loss: 0.8411 - val_acc: 0.6246\n",
      "Epoch 99/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8320 - acc: 0.6409 - val_loss: 0.8403 - val_acc: 0.6249\n",
      "Epoch 100/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8313 - acc: 0.6421 - val_loss: 0.8399 - val_acc: 0.6239\n",
      "Epoch 101/150\n",
      "15663/15663 [==============================] - 0s 21us/step - loss: 0.8307 - acc: 0.6422 - val_loss: 0.8392 - val_acc: 0.6251\n",
      "Epoch 102/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8301 - acc: 0.6429 - val_loss: 0.8391 - val_acc: 0.6239\n",
      "Epoch 103/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8296 - acc: 0.6429 - val_loss: 0.8383 - val_acc: 0.6249\n",
      "Epoch 104/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8290 - acc: 0.6426 - val_loss: 0.8377 - val_acc: 0.6236\n",
      "Epoch 105/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8284 - acc: 0.6430 - val_loss: 0.8373 - val_acc: 0.6254\n",
      "Epoch 106/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8278 - acc: 0.6443 - val_loss: 0.8367 - val_acc: 0.6249\n",
      "Epoch 107/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8272 - acc: 0.6429 - val_loss: 0.8366 - val_acc: 0.6267\n",
      "Epoch 108/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8266 - acc: 0.6423 - val_loss: 0.8362 - val_acc: 0.6261\n",
      "Epoch 109/150\n",
      "15663/15663 [==============================] - 0s 18us/step - loss: 0.8262 - acc: 0.6423 - val_loss: 0.8354 - val_acc: 0.6267\n",
      "Epoch 110/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8257 - acc: 0.6440 - val_loss: 0.8349 - val_acc: 0.6269\n",
      "Epoch 111/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8252 - acc: 0.6435 - val_loss: 0.8344 - val_acc: 0.6274\n",
      "Epoch 112/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8247 - acc: 0.6449 - val_loss: 0.8341 - val_acc: 0.6277\n",
      "Epoch 113/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8243 - acc: 0.6442 - val_loss: 0.8335 - val_acc: 0.6277\n",
      "Epoch 114/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8237 - acc: 0.6442 - val_loss: 0.8331 - val_acc: 0.6284\n",
      "Epoch 115/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8232 - acc: 0.6443 - val_loss: 0.8326 - val_acc: 0.6292\n",
      "Epoch 116/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8227 - acc: 0.6448 - val_loss: 0.8324 - val_acc: 0.6284\n",
      "Epoch 117/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8223 - acc: 0.6450 - val_loss: 0.8319 - val_acc: 0.6256\n",
      "Epoch 118/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8218 - acc: 0.6459 - val_loss: 0.8314 - val_acc: 0.6272\n",
      "Epoch 119/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8215 - acc: 0.6454 - val_loss: 0.8310 - val_acc: 0.6272\n",
      "Epoch 120/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8209 - acc: 0.6464 - val_loss: 0.8306 - val_acc: 0.6292\n",
      "Epoch 121/150\n",
      "15663/15663 [==============================] - 0s 18us/step - loss: 0.8205 - acc: 0.6457 - val_loss: 0.8301 - val_acc: 0.6305\n",
      "Epoch 122/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8200 - acc: 0.6457 - val_loss: 0.8299 - val_acc: 0.6302\n",
      "Epoch 123/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8195 - acc: 0.6465 - val_loss: 0.8294 - val_acc: 0.6297\n",
      "Epoch 124/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8191 - acc: 0.6467 - val_loss: 0.8291 - val_acc: 0.6307\n",
      "Epoch 125/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8187 - acc: 0.6469 - val_loss: 0.8286 - val_acc: 0.6297\n",
      "Epoch 126/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8183 - acc: 0.6467 - val_loss: 0.8284 - val_acc: 0.6305\n",
      "Epoch 127/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8179 - acc: 0.6474 - val_loss: 0.8279 - val_acc: 0.6315\n",
      "Epoch 128/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8174 - acc: 0.6485 - val_loss: 0.8277 - val_acc: 0.6315\n",
      "Epoch 129/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8171 - acc: 0.6466 - val_loss: 0.8277 - val_acc: 0.6300\n",
      "Epoch 130/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8166 - acc: 0.6480 - val_loss: 0.8270 - val_acc: 0.6305\n",
      "Epoch 131/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8162 - acc: 0.6476 - val_loss: 0.8266 - val_acc: 0.6323\n",
      "Epoch 132/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8159 - acc: 0.6485 - val_loss: 0.8265 - val_acc: 0.6297\n",
      "Epoch 133/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8155 - acc: 0.6473 - val_loss: 0.8259 - val_acc: 0.6313\n",
      "Epoch 134/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8152 - acc: 0.6478 - val_loss: 0.8255 - val_acc: 0.6307\n",
      "Epoch 135/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8147 - acc: 0.6487 - val_loss: 0.8252 - val_acc: 0.6323\n",
      "Epoch 136/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8143 - acc: 0.6488 - val_loss: 0.8249 - val_acc: 0.6313\n",
      "Epoch 137/150\n",
      "15663/15663 [==============================] - 0s 18us/step - loss: 0.8140 - acc: 0.6496 - val_loss: 0.8247 - val_acc: 0.6318\n",
      "Epoch 138/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8136 - acc: 0.6492 - val_loss: 0.8244 - val_acc: 0.6315\n",
      "Epoch 139/150\n",
      "15663/15663 [==============================] - 0s 18us/step - loss: 0.8133 - acc: 0.6490 - val_loss: 0.8241 - val_acc: 0.6320\n",
      "Epoch 140/150\n",
      "15663/15663 [==============================] - 0s 18us/step - loss: 0.8129 - acc: 0.6496 - val_loss: 0.8236 - val_acc: 0.6318\n",
      "Epoch 141/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8125 - acc: 0.6503 - val_loss: 0.8235 - val_acc: 0.6318\n",
      "Epoch 142/150\n",
      "15663/15663 [==============================] - 0s 20us/step - loss: 0.8123 - acc: 0.6499 - val_loss: 0.8232 - val_acc: 0.6320\n",
      "Epoch 143/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8118 - acc: 0.6496 - val_loss: 0.8227 - val_acc: 0.6313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8116 - acc: 0.6507 - val_loss: 0.8225 - val_acc: 0.6310\n",
      "Epoch 145/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8112 - acc: 0.6504 - val_loss: 0.8224 - val_acc: 0.6318\n",
      "Epoch 146/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8108 - acc: 0.6504 - val_loss: 0.8219 - val_acc: 0.6313\n",
      "Epoch 147/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8106 - acc: 0.6500 - val_loss: 0.8216 - val_acc: 0.6320\n",
      "Epoch 148/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8103 - acc: 0.6515 - val_loss: 0.8215 - val_acc: 0.6313\n",
      "Epoch 149/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8099 - acc: 0.6516 - val_loss: 0.8211 - val_acc: 0.6307\n",
      "Epoch 150/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8096 - acc: 0.6514 - val_loss: 0.8208 - val_acc: 0.6328\n",
      "15663/15663 [==============================] - 0s 25us/step\n",
      "==== Training loss, score are: 0.8093209221834485 0.6517269999627934 =======\n",
      "3916/3916 [==============================] - 0s 27us/step\n",
      "==== CV loss, score are: 0.820845923884044 0.6327885597548519 =======\n",
      "Running fold 4\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_47 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_47  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 24,734,903\n",
      "Trainable params: 303\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15663/15663 [==============================] - 1s 78us/step\n",
      "Before training loss, score are: 1.109358027315776 0.29125965651820873\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/150\n",
      "15663/15663 [==============================] - 1s 70us/step - loss: 1.0894 - acc: 0.3925 - val_loss: 1.0789 - val_acc: 0.4147\n",
      "Epoch 2/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0748 - acc: 0.4068 - val_loss: 1.0671 - val_acc: 0.4346\n",
      "Epoch 3/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0636 - acc: 0.4302 - val_loss: 1.0560 - val_acc: 0.4436\n",
      "Epoch 4/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0534 - acc: 0.4424 - val_loss: 1.0466 - val_acc: 0.4686\n",
      "Epoch 5/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0441 - acc: 0.4603 - val_loss: 1.0373 - val_acc: 0.4821\n",
      "Epoch 6/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0356 - acc: 0.4774 - val_loss: 1.0289 - val_acc: 0.4842\n",
      "Epoch 7/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0276 - acc: 0.4891 - val_loss: 1.0212 - val_acc: 0.4987\n",
      "Epoch 8/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0200 - acc: 0.4993 - val_loss: 1.0143 - val_acc: 0.5158\n",
      "Epoch 9/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0131 - acc: 0.5101 - val_loss: 1.0072 - val_acc: 0.5148\n",
      "Epoch 10/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 1.0066 - acc: 0.5126 - val_loss: 1.0009 - val_acc: 0.5294\n",
      "Epoch 11/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 1.0003 - acc: 0.5247 - val_loss: 0.9950 - val_acc: 0.5388\n",
      "Epoch 12/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9945 - acc: 0.5295 - val_loss: 0.9894 - val_acc: 0.5439\n",
      "Epoch 13/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9889 - acc: 0.5372 - val_loss: 0.9838 - val_acc: 0.5470\n",
      "Epoch 14/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9837 - acc: 0.5402 - val_loss: 0.9788 - val_acc: 0.5559\n",
      "Epoch 15/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9787 - acc: 0.5482 - val_loss: 0.9740 - val_acc: 0.5621\n",
      "Epoch 16/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9739 - acc: 0.5540 - val_loss: 0.9695 - val_acc: 0.5595\n",
      "Epoch 17/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9693 - acc: 0.5564 - val_loss: 0.9650 - val_acc: 0.5674\n",
      "Epoch 18/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9649 - acc: 0.5639 - val_loss: 0.9608 - val_acc: 0.5664\n",
      "Epoch 19/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9607 - acc: 0.5632 - val_loss: 0.9569 - val_acc: 0.5764\n",
      "Epoch 20/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9566 - acc: 0.5714 - val_loss: 0.9527 - val_acc: 0.5746\n",
      "Epoch 21/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9528 - acc: 0.5721 - val_loss: 0.9494 - val_acc: 0.5835\n",
      "Epoch 22/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9490 - acc: 0.5772 - val_loss: 0.9455 - val_acc: 0.5781\n",
      "Epoch 23/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9456 - acc: 0.5755 - val_loss: 0.9424 - val_acc: 0.5912\n",
      "Epoch 24/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9421 - acc: 0.5798 - val_loss: 0.9389 - val_acc: 0.5960\n",
      "Epoch 25/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9387 - acc: 0.5833 - val_loss: 0.9353 - val_acc: 0.5919\n",
      "Epoch 26/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9355 - acc: 0.5855 - val_loss: 0.9322 - val_acc: 0.5917\n",
      "Epoch 27/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9325 - acc: 0.5852 - val_loss: 0.9294 - val_acc: 0.5988\n",
      "Epoch 28/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9294 - acc: 0.5887 - val_loss: 0.9266 - val_acc: 0.5991\n",
      "Epoch 29/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9266 - acc: 0.5888 - val_loss: 0.9240 - val_acc: 0.6029\n",
      "Epoch 30/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9237 - acc: 0.5911 - val_loss: 0.9212 - val_acc: 0.6039\n",
      "Epoch 31/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9212 - acc: 0.5941 - val_loss: 0.9185 - val_acc: 0.6044\n",
      "Epoch 32/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9185 - acc: 0.5948 - val_loss: 0.9156 - val_acc: 0.6055\n",
      "Epoch 33/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.9159 - acc: 0.5966 - val_loss: 0.9135 - val_acc: 0.6098\n",
      "Epoch 34/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9133 - acc: 0.5978 - val_loss: 0.9113 - val_acc: 0.6073\n",
      "Epoch 35/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9111 - acc: 0.5985 - val_loss: 0.9085 - val_acc: 0.6101\n",
      "Epoch 36/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9089 - acc: 0.5999 - val_loss: 0.9062 - val_acc: 0.6103\n",
      "Epoch 37/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9065 - acc: 0.6013 - val_loss: 0.9041 - val_acc: 0.6147\n",
      "Epoch 38/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9043 - acc: 0.6049 - val_loss: 0.9018 - val_acc: 0.6106\n",
      "Epoch 39/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9025 - acc: 0.6038 - val_loss: 0.8997 - val_acc: 0.6129\n",
      "Epoch 40/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.9002 - acc: 0.6045 - val_loss: 0.8982 - val_acc: 0.6167\n",
      "Epoch 41/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8983 - acc: 0.6079 - val_loss: 0.8958 - val_acc: 0.6164\n",
      "Epoch 42/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8963 - acc: 0.6074 - val_loss: 0.8940 - val_acc: 0.6193\n",
      "Epoch 43/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8944 - acc: 0.6089 - val_loss: 0.8921 - val_acc: 0.6170\n",
      "Epoch 44/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8926 - acc: 0.6103 - val_loss: 0.8904 - val_acc: 0.6175\n",
      "Epoch 45/150\n",
      "15663/15663 [==============================] - 0s 15us/step - loss: 0.8908 - acc: 0.6124 - val_loss: 0.8889 - val_acc: 0.6180\n",
      "Epoch 46/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8890 - acc: 0.6132 - val_loss: 0.8870 - val_acc: 0.6167\n",
      "Epoch 47/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8875 - acc: 0.6131 - val_loss: 0.8851 - val_acc: 0.6210\n",
      "Epoch 48/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8857 - acc: 0.6138 - val_loss: 0.8838 - val_acc: 0.6213\n",
      "Epoch 49/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8841 - acc: 0.6155 - val_loss: 0.8819 - val_acc: 0.6221\n",
      "Epoch 50/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8826 - acc: 0.6148 - val_loss: 0.8804 - val_acc: 0.6236\n",
      "Epoch 51/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8811 - acc: 0.6169 - val_loss: 0.8790 - val_acc: 0.6254\n",
      "Epoch 52/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8795 - acc: 0.6176 - val_loss: 0.8774 - val_acc: 0.6233\n",
      "Epoch 53/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8780 - acc: 0.6180 - val_loss: 0.8757 - val_acc: 0.6246\n",
      "Epoch 54/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8768 - acc: 0.6187 - val_loss: 0.8746 - val_acc: 0.6256\n",
      "Epoch 55/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8753 - acc: 0.6187 - val_loss: 0.8731 - val_acc: 0.6254\n",
      "Epoch 56/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8739 - acc: 0.6187 - val_loss: 0.8721 - val_acc: 0.6284\n",
      "Epoch 57/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8726 - acc: 0.6197 - val_loss: 0.8703 - val_acc: 0.6256\n",
      "Epoch 58/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8713 - acc: 0.6201 - val_loss: 0.8695 - val_acc: 0.6282\n",
      "Epoch 59/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8701 - acc: 0.6212 - val_loss: 0.8678 - val_acc: 0.6284\n",
      "Epoch 60/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8688 - acc: 0.6227 - val_loss: 0.8665 - val_acc: 0.6277\n",
      "Epoch 61/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8676 - acc: 0.6211 - val_loss: 0.8652 - val_acc: 0.6305\n",
      "Epoch 62/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8665 - acc: 0.6227 - val_loss: 0.8641 - val_acc: 0.6297\n",
      "Epoch 63/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8652 - acc: 0.6225 - val_loss: 0.8629 - val_acc: 0.6305\n",
      "Epoch 64/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8641 - acc: 0.6224 - val_loss: 0.8617 - val_acc: 0.6310\n",
      "Epoch 65/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8631 - acc: 0.6234 - val_loss: 0.8612 - val_acc: 0.6310\n",
      "Epoch 66/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8618 - acc: 0.6234 - val_loss: 0.8599 - val_acc: 0.6330\n",
      "Epoch 67/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8609 - acc: 0.6254 - val_loss: 0.8584 - val_acc: 0.6325\n",
      "Epoch 68/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8597 - acc: 0.6247 - val_loss: 0.8573 - val_acc: 0.6353\n",
      "Epoch 69/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8588 - acc: 0.6257 - val_loss: 0.8565 - val_acc: 0.6336\n",
      "Epoch 70/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8578 - acc: 0.6249 - val_loss: 0.8556 - val_acc: 0.6333\n",
      "Epoch 71/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8569 - acc: 0.6263 - val_loss: 0.8545 - val_acc: 0.6336\n",
      "Epoch 72/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8558 - acc: 0.6271 - val_loss: 0.8537 - val_acc: 0.6364\n",
      "Epoch 73/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8548 - acc: 0.6272 - val_loss: 0.8529 - val_acc: 0.6351\n",
      "Epoch 74/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8539 - acc: 0.6282 - val_loss: 0.8515 - val_acc: 0.6359\n",
      "Epoch 75/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8531 - acc: 0.6275 - val_loss: 0.8505 - val_acc: 0.6359\n",
      "Epoch 76/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8521 - acc: 0.6292 - val_loss: 0.8504 - val_acc: 0.6361\n",
      "Epoch 77/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8514 - acc: 0.6281 - val_loss: 0.8486 - val_acc: 0.6382\n",
      "Epoch 78/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8504 - acc: 0.6294 - val_loss: 0.8481 - val_acc: 0.6371\n",
      "Epoch 79/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8496 - acc: 0.6289 - val_loss: 0.8468 - val_acc: 0.6389\n",
      "Epoch 80/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8487 - acc: 0.6300 - val_loss: 0.8462 - val_acc: 0.6341\n",
      "Epoch 81/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8480 - acc: 0.6298 - val_loss: 0.8452 - val_acc: 0.6394\n",
      "Epoch 82/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8472 - acc: 0.6312 - val_loss: 0.8444 - val_acc: 0.6361\n",
      "Epoch 83/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8466 - acc: 0.6302 - val_loss: 0.8439 - val_acc: 0.6376\n",
      "Epoch 84/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8456 - acc: 0.6305 - val_loss: 0.8428 - val_acc: 0.6384\n",
      "Epoch 85/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8449 - acc: 0.6313 - val_loss: 0.8421 - val_acc: 0.6376\n",
      "Epoch 86/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8441 - acc: 0.6323 - val_loss: 0.8411 - val_acc: 0.6407\n",
      "Epoch 87/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8434 - acc: 0.6314 - val_loss: 0.8403 - val_acc: 0.6394\n",
      "Epoch 88/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8426 - acc: 0.6317 - val_loss: 0.8398 - val_acc: 0.6382\n",
      "Epoch 89/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8419 - acc: 0.6321 - val_loss: 0.8395 - val_acc: 0.6382\n",
      "Epoch 90/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8413 - acc: 0.6340 - val_loss: 0.8380 - val_acc: 0.6420\n",
      "Epoch 91/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8406 - acc: 0.6331 - val_loss: 0.8377 - val_acc: 0.6387\n",
      "Epoch 92/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8399 - acc: 0.6348 - val_loss: 0.8366 - val_acc: 0.6415\n",
      "Epoch 93/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8392 - acc: 0.6334 - val_loss: 0.8363 - val_acc: 0.6410\n",
      "Epoch 94/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8386 - acc: 0.6352 - val_loss: 0.8359 - val_acc: 0.6402\n",
      "Epoch 95/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8379 - acc: 0.6339 - val_loss: 0.8351 - val_acc: 0.6387\n",
      "Epoch 96/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8373 - acc: 0.6342 - val_loss: 0.8342 - val_acc: 0.6422\n",
      "Epoch 97/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8367 - acc: 0.6354 - val_loss: 0.8337 - val_acc: 0.6399\n",
      "Epoch 98/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8361 - acc: 0.6353 - val_loss: 0.8329 - val_acc: 0.6412\n",
      "Epoch 99/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8355 - acc: 0.6362 - val_loss: 0.8324 - val_acc: 0.6412\n",
      "Epoch 100/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8349 - acc: 0.6358 - val_loss: 0.8315 - val_acc: 0.6425\n",
      "Epoch 101/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8344 - acc: 0.6354 - val_loss: 0.8311 - val_acc: 0.6420\n",
      "Epoch 102/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8337 - acc: 0.6367 - val_loss: 0.8308 - val_acc: 0.6433\n",
      "Epoch 103/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8334 - acc: 0.6358 - val_loss: 0.8299 - val_acc: 0.6422\n",
      "Epoch 104/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8327 - acc: 0.6363 - val_loss: 0.8291 - val_acc: 0.6425\n",
      "Epoch 105/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8321 - acc: 0.6374 - val_loss: 0.8285 - val_acc: 0.6425\n",
      "Epoch 106/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8315 - acc: 0.6370 - val_loss: 0.8279 - val_acc: 0.6445\n",
      "Epoch 107/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8311 - acc: 0.6383 - val_loss: 0.8277 - val_acc: 0.6443\n",
      "Epoch 108/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8305 - acc: 0.6366 - val_loss: 0.8274 - val_acc: 0.6461\n",
      "Epoch 109/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8300 - acc: 0.6372 - val_loss: 0.8264 - val_acc: 0.6430\n",
      "Epoch 110/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8296 - acc: 0.6375 - val_loss: 0.8259 - val_acc: 0.6433\n",
      "Epoch 111/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8290 - acc: 0.6390 - val_loss: 0.8257 - val_acc: 0.6448\n",
      "Epoch 112/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8287 - acc: 0.6376 - val_loss: 0.8249 - val_acc: 0.6461\n",
      "Epoch 113/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8281 - acc: 0.6381 - val_loss: 0.8244 - val_acc: 0.6448\n",
      "Epoch 114/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8275 - acc: 0.6381 - val_loss: 0.8240 - val_acc: 0.6468\n",
      "Epoch 115/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8271 - acc: 0.6380 - val_loss: 0.8235 - val_acc: 0.6471\n",
      "Epoch 116/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8266 - acc: 0.6395 - val_loss: 0.8231 - val_acc: 0.6458\n",
      "Epoch 117/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8262 - acc: 0.6393 - val_loss: 0.8222 - val_acc: 0.6471\n",
      "Epoch 118/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8258 - acc: 0.6393 - val_loss: 0.8220 - val_acc: 0.6489\n",
      "Epoch 119/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8252 - acc: 0.6406 - val_loss: 0.8216 - val_acc: 0.6458\n",
      "Epoch 120/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8249 - acc: 0.6400 - val_loss: 0.8208 - val_acc: 0.6481\n",
      "Epoch 121/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8245 - acc: 0.6399 - val_loss: 0.8204 - val_acc: 0.6481\n",
      "Epoch 122/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8240 - acc: 0.6400 - val_loss: 0.8201 - val_acc: 0.6486\n",
      "Epoch 123/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8236 - acc: 0.6399 - val_loss: 0.8196 - val_acc: 0.6486\n",
      "Epoch 124/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8232 - acc: 0.6400 - val_loss: 0.8191 - val_acc: 0.6486\n",
      "Epoch 125/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8226 - acc: 0.6409 - val_loss: 0.8192 - val_acc: 0.6507\n",
      "Epoch 126/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8226 - acc: 0.6413 - val_loss: 0.8184 - val_acc: 0.6504\n",
      "Epoch 127/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8219 - acc: 0.6407 - val_loss: 0.8176 - val_acc: 0.6491\n",
      "Epoch 128/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8215 - acc: 0.6412 - val_loss: 0.8180 - val_acc: 0.6499\n",
      "Epoch 129/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8212 - acc: 0.6412 - val_loss: 0.8170 - val_acc: 0.6458\n",
      "Epoch 130/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8208 - acc: 0.6425 - val_loss: 0.8167 - val_acc: 0.6479\n",
      "Epoch 131/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8204 - acc: 0.6431 - val_loss: 0.8159 - val_acc: 0.6496\n",
      "Epoch 132/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8201 - acc: 0.6427 - val_loss: 0.8158 - val_acc: 0.6479\n",
      "Epoch 133/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8196 - acc: 0.6424 - val_loss: 0.8159 - val_acc: 0.6519\n",
      "Epoch 134/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8193 - acc: 0.6434 - val_loss: 0.8150 - val_acc: 0.6481\n",
      "Epoch 135/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8190 - acc: 0.6425 - val_loss: 0.8148 - val_acc: 0.6514\n",
      "Epoch 136/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8186 - acc: 0.6427 - val_loss: 0.8145 - val_acc: 0.6522\n",
      "Epoch 137/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8182 - acc: 0.6439 - val_loss: 0.8137 - val_acc: 0.6504\n",
      "Epoch 138/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8179 - acc: 0.6437 - val_loss: 0.8134 - val_acc: 0.6486\n",
      "Epoch 139/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8174 - acc: 0.6437 - val_loss: 0.8130 - val_acc: 0.6537\n",
      "Epoch 140/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8172 - acc: 0.6430 - val_loss: 0.8127 - val_acc: 0.6507\n",
      "Epoch 141/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8168 - acc: 0.6448 - val_loss: 0.8124 - val_acc: 0.6486\n",
      "Epoch 142/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8165 - acc: 0.6453 - val_loss: 0.8119 - val_acc: 0.6494\n",
      "Epoch 143/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8162 - acc: 0.6450 - val_loss: 0.8116 - val_acc: 0.6504\n",
      "Epoch 144/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8158 - acc: 0.6447 - val_loss: 0.8115 - val_acc: 0.6519\n",
      "Epoch 145/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8156 - acc: 0.6457 - val_loss: 0.8111 - val_acc: 0.6519\n",
      "Epoch 146/150\n",
      "15663/15663 [==============================] - 0s 17us/step - loss: 0.8152 - acc: 0.6452 - val_loss: 0.8106 - val_acc: 0.6532\n",
      "Epoch 147/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8149 - acc: 0.6442 - val_loss: 0.8105 - val_acc: 0.6527\n",
      "Epoch 148/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8145 - acc: 0.6451 - val_loss: 0.8101 - val_acc: 0.6484\n",
      "Epoch 149/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8143 - acc: 0.6456 - val_loss: 0.8095 - val_acc: 0.6507\n",
      "Epoch 150/150\n",
      "15663/15663 [==============================] - 0s 16us/step - loss: 0.8139 - acc: 0.6452 - val_loss: 0.8091 - val_acc: 0.6489\n",
      "15663/15663 [==============================] - 0s 26us/step\n",
      "==== Training loss, score are: 0.8136896675043862 0.6450233033529486 =======\n",
      "3916/3916 [==============================] - 0s 27us/step\n",
      "==== CV loss, score are: 0.8090695989631169 0.648876404494382 =======\n",
      "Running fold 5\n",
      "Creating Fasttext model vocab_size: 247346, wordvecdim 100, sentence_maxlength_cap 161, word_vector_type glove \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_48 (Embedding)     (None, 161, 100)          24734600  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_48  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 24,734,903\n",
      "Trainable params: 303\n",
      "Non-trainable params: 24,734,600\n",
      "_________________________________________________________________\n",
      "15664/15664 [==============================] - 1s 80us/step\n",
      "Before training loss, score are: 1.107530347173377 0.32073544433094997\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/150\n",
      "15664/15664 [==============================] - 1s 70us/step - loss: 1.0897 - acc: 0.3879 - val_loss: 1.0776 - val_acc: 0.4215\n",
      "Epoch 2/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 1.0756 - acc: 0.4051 - val_loss: 1.0658 - val_acc: 0.4360\n",
      "Epoch 3/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 1.0645 - acc: 0.4273 - val_loss: 1.0548 - val_acc: 0.4524\n",
      "Epoch 4/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 1.0544 - acc: 0.4455 - val_loss: 1.0451 - val_acc: 0.4708\n",
      "Epoch 5/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 1.0449 - acc: 0.4602 - val_loss: 1.0365 - val_acc: 0.4912\n",
      "Epoch 6/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 1.0363 - acc: 0.4750 - val_loss: 1.0282 - val_acc: 0.5032\n",
      "Epoch 7/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 1.0283 - acc: 0.4899 - val_loss: 1.0194 - val_acc: 0.5073\n",
      "Epoch 8/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 1.0209 - acc: 0.4994 - val_loss: 1.0122 - val_acc: 0.5216\n",
      "Epoch 9/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 1.0137 - acc: 0.5109 - val_loss: 1.0047 - val_acc: 0.5098\n",
      "Epoch 10/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 1.0072 - acc: 0.5151 - val_loss: 0.9989 - val_acc: 0.5372\n",
      "Epoch 11/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 1.0010 - acc: 0.5210 - val_loss: 0.9929 - val_acc: 0.5453\n",
      "Epoch 12/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9951 - acc: 0.5317 - val_loss: 0.9867 - val_acc: 0.5458\n",
      "Epoch 13/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9896 - acc: 0.5353 - val_loss: 0.9817 - val_acc: 0.5571\n",
      "Epoch 14/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9842 - acc: 0.5458 - val_loss: 0.9760 - val_acc: 0.5579\n",
      "Epoch 15/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9793 - acc: 0.5474 - val_loss: 0.9712 - val_acc: 0.5642\n",
      "Epoch 16/150\n",
      "15664/15664 [==============================] - 0s 17us/step - loss: 0.9745 - acc: 0.5555 - val_loss: 0.9666 - val_acc: 0.5668\n",
      "Epoch 17/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9699 - acc: 0.5568 - val_loss: 0.9627 - val_acc: 0.5745\n",
      "Epoch 18/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9655 - acc: 0.5605 - val_loss: 0.9587 - val_acc: 0.5849\n",
      "Epoch 19/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9613 - acc: 0.5672 - val_loss: 0.9537 - val_acc: 0.5727\n",
      "Epoch 20/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9574 - acc: 0.5677 - val_loss: 0.9505 - val_acc: 0.5862\n",
      "Epoch 21/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9534 - acc: 0.5718 - val_loss: 0.9458 - val_acc: 0.5821\n",
      "Epoch 22/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9495 - acc: 0.5755 - val_loss: 0.9419 - val_acc: 0.5798\n",
      "Epoch 23/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9462 - acc: 0.5745 - val_loss: 0.9391 - val_acc: 0.5916\n",
      "Epoch 24/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9426 - acc: 0.5792 - val_loss: 0.9363 - val_acc: 0.5992\n",
      "Epoch 25/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9394 - acc: 0.5809 - val_loss: 0.9324 - val_acc: 0.5941\n",
      "Epoch 26/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9361 - acc: 0.5846 - val_loss: 0.9294 - val_acc: 0.6000\n",
      "Epoch 27/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9331 - acc: 0.5861 - val_loss: 0.9266 - val_acc: 0.6038\n",
      "Epoch 28/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9301 - acc: 0.5890 - val_loss: 0.9226 - val_acc: 0.5951\n",
      "Epoch 29/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9272 - acc: 0.5891 - val_loss: 0.9208 - val_acc: 0.6023\n",
      "Epoch 30/150\n",
      "15664/15664 [==============================] - 0s 17us/step - loss: 0.9244 - acc: 0.5897 - val_loss: 0.9178 - val_acc: 0.6059\n",
      "Epoch 31/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9217 - acc: 0.5937 - val_loss: 0.9147 - val_acc: 0.6074\n",
      "Epoch 32/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9191 - acc: 0.5937 - val_loss: 0.9121 - val_acc: 0.6084\n",
      "Epoch 33/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9165 - acc: 0.5957 - val_loss: 0.9099 - val_acc: 0.6123\n",
      "Epoch 34/150\n",
      "15664/15664 [==============================] - 0s 17us/step - loss: 0.9142 - acc: 0.5978 - val_loss: 0.9078 - val_acc: 0.6138\n",
      "Epoch 35/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9116 - acc: 0.5975 - val_loss: 0.9051 - val_acc: 0.6140\n",
      "Epoch 36/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9093 - acc: 0.5998 - val_loss: 0.9032 - val_acc: 0.6169\n",
      "Epoch 37/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9071 - acc: 0.6015 - val_loss: 0.9007 - val_acc: 0.6158\n",
      "Epoch 38/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9050 - acc: 0.6036 - val_loss: 0.8988 - val_acc: 0.6176\n",
      "Epoch 39/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9028 - acc: 0.6050 - val_loss: 0.8962 - val_acc: 0.6156\n",
      "Epoch 40/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.9009 - acc: 0.6044 - val_loss: 0.8949 - val_acc: 0.6199\n",
      "Epoch 41/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8988 - acc: 0.6065 - val_loss: 0.8925 - val_acc: 0.6197\n",
      "Epoch 42/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8969 - acc: 0.6075 - val_loss: 0.8907 - val_acc: 0.6202\n",
      "Epoch 43/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8948 - acc: 0.6083 - val_loss: 0.8881 - val_acc: 0.6171\n",
      "Epoch 44/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8932 - acc: 0.6095 - val_loss: 0.8866 - val_acc: 0.6215\n",
      "Epoch 45/150\n",
      "15664/15664 [==============================] - 0s 17us/step - loss: 0.8914 - acc: 0.6120 - val_loss: 0.8851 - val_acc: 0.6209\n",
      "Epoch 46/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8896 - acc: 0.6108 - val_loss: 0.8837 - val_acc: 0.6220\n",
      "Epoch 47/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8879 - acc: 0.6123 - val_loss: 0.8818 - val_acc: 0.6222\n",
      "Epoch 48/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8861 - acc: 0.6127 - val_loss: 0.8805 - val_acc: 0.6250\n",
      "Epoch 49/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8846 - acc: 0.6131 - val_loss: 0.8793 - val_acc: 0.6278\n",
      "Epoch 50/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8831 - acc: 0.6150 - val_loss: 0.8770 - val_acc: 0.6253\n",
      "Epoch 51/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8814 - acc: 0.6163 - val_loss: 0.8755 - val_acc: 0.6284\n",
      "Epoch 52/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8799 - acc: 0.6159 - val_loss: 0.8751 - val_acc: 0.6296\n",
      "Epoch 53/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8784 - acc: 0.6167 - val_loss: 0.8730 - val_acc: 0.6299\n",
      "Epoch 54/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8770 - acc: 0.6173 - val_loss: 0.8712 - val_acc: 0.6299\n",
      "Epoch 55/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8756 - acc: 0.6174 - val_loss: 0.8706 - val_acc: 0.6301\n",
      "Epoch 56/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8743 - acc: 0.6188 - val_loss: 0.8686 - val_acc: 0.6319\n",
      "Epoch 57/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8729 - acc: 0.6187 - val_loss: 0.8672 - val_acc: 0.6322\n",
      "Epoch 58/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8716 - acc: 0.6194 - val_loss: 0.8658 - val_acc: 0.6319\n",
      "Epoch 59/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8703 - acc: 0.6199 - val_loss: 0.8648 - val_acc: 0.6330\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15664/15664 [==============================] - 0s 17us/step - loss: 0.8690 - acc: 0.6201 - val_loss: 0.8636 - val_acc: 0.6332\n",
      "Epoch 61/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8677 - acc: 0.6214 - val_loss: 0.8620 - val_acc: 0.6314\n",
      "Epoch 62/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8667 - acc: 0.6222 - val_loss: 0.8609 - val_acc: 0.6345\n",
      "Epoch 63/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8654 - acc: 0.6233 - val_loss: 0.8595 - val_acc: 0.6358\n",
      "Epoch 64/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8641 - acc: 0.6224 - val_loss: 0.8583 - val_acc: 0.6360\n",
      "Epoch 65/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8628 - acc: 0.6240 - val_loss: 0.8596 - val_acc: 0.6360\n",
      "Epoch 66/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8620 - acc: 0.6251 - val_loss: 0.8563 - val_acc: 0.6358\n",
      "Epoch 67/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8609 - acc: 0.6254 - val_loss: 0.8551 - val_acc: 0.6368\n",
      "Epoch 68/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8598 - acc: 0.6253 - val_loss: 0.8541 - val_acc: 0.6340\n",
      "Epoch 69/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8588 - acc: 0.6251 - val_loss: 0.8542 - val_acc: 0.6398\n",
      "Epoch 70/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8578 - acc: 0.6260 - val_loss: 0.8526 - val_acc: 0.6391\n",
      "Epoch 71/150\n",
      "15664/15664 [==============================] - 0s 17us/step - loss: 0.8567 - acc: 0.6268 - val_loss: 0.8509 - val_acc: 0.6363\n",
      "Epoch 72/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8559 - acc: 0.6259 - val_loss: 0.8502 - val_acc: 0.6373\n",
      "Epoch 73/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8548 - acc: 0.6276 - val_loss: 0.8495 - val_acc: 0.6381\n",
      "Epoch 74/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8539 - acc: 0.6278 - val_loss: 0.8484 - val_acc: 0.6381\n",
      "Epoch 75/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8529 - acc: 0.6278 - val_loss: 0.8482 - val_acc: 0.6429\n",
      "Epoch 76/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8520 - acc: 0.6299 - val_loss: 0.8473 - val_acc: 0.6416\n",
      "Epoch 77/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8511 - acc: 0.6304 - val_loss: 0.8456 - val_acc: 0.6386\n",
      "Epoch 78/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8502 - acc: 0.6300 - val_loss: 0.8448 - val_acc: 0.6355\n",
      "Epoch 79/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8494 - acc: 0.6300 - val_loss: 0.8444 - val_acc: 0.6429\n",
      "Epoch 80/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8486 - acc: 0.6306 - val_loss: 0.8438 - val_acc: 0.6437\n",
      "Epoch 81/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8478 - acc: 0.6314 - val_loss: 0.8425 - val_acc: 0.6414\n",
      "Epoch 82/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8469 - acc: 0.6310 - val_loss: 0.8423 - val_acc: 0.6439\n",
      "Epoch 83/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8461 - acc: 0.6300 - val_loss: 0.8415 - val_acc: 0.6437\n",
      "Epoch 84/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8454 - acc: 0.6314 - val_loss: 0.8405 - val_acc: 0.6429\n",
      "Epoch 85/150\n",
      "15664/15664 [==============================] - 0s 17us/step - loss: 0.8446 - acc: 0.6318 - val_loss: 0.8395 - val_acc: 0.6432\n",
      "Epoch 86/150\n",
      "15664/15664 [==============================] - 0s 20us/step - loss: 0.8438 - acc: 0.6316 - val_loss: 0.8384 - val_acc: 0.6437\n",
      "Epoch 87/150\n",
      "15664/15664 [==============================] - 0s 17us/step - loss: 0.8431 - acc: 0.6323 - val_loss: 0.8381 - val_acc: 0.6447\n",
      "Epoch 88/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8423 - acc: 0.6326 - val_loss: 0.8384 - val_acc: 0.6465\n",
      "Epoch 89/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8416 - acc: 0.6324 - val_loss: 0.8365 - val_acc: 0.6442\n",
      "Epoch 90/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8409 - acc: 0.6330 - val_loss: 0.8361 - val_acc: 0.6457\n",
      "Epoch 91/150\n",
      "15664/15664 [==============================] - ETA: 0s - loss: 0.8391 - acc: 0.634 - 0s 16us/step - loss: 0.8402 - acc: 0.6336 - val_loss: 0.8356 - val_acc: 0.6460\n",
      "Epoch 92/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8395 - acc: 0.6338 - val_loss: 0.8347 - val_acc: 0.6460\n",
      "Epoch 93/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8388 - acc: 0.6344 - val_loss: 0.8344 - val_acc: 0.6460\n",
      "Epoch 94/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8382 - acc: 0.6345 - val_loss: 0.8333 - val_acc: 0.6457\n",
      "Epoch 95/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8376 - acc: 0.6329 - val_loss: 0.8328 - val_acc: 0.6470\n",
      "Epoch 96/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8369 - acc: 0.6337 - val_loss: 0.8322 - val_acc: 0.6473\n",
      "Epoch 97/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8363 - acc: 0.6356 - val_loss: 0.8321 - val_acc: 0.6467\n",
      "Epoch 98/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8357 - acc: 0.6343 - val_loss: 0.8315 - val_acc: 0.6485\n",
      "Epoch 99/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8350 - acc: 0.6355 - val_loss: 0.8305 - val_acc: 0.6508\n",
      "Epoch 100/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8343 - acc: 0.6348 - val_loss: 0.8296 - val_acc: 0.6485\n",
      "Epoch 101/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8338 - acc: 0.6359 - val_loss: 0.8293 - val_acc: 0.6506\n",
      "Epoch 102/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8332 - acc: 0.6359 - val_loss: 0.8287 - val_acc: 0.6524\n",
      "Epoch 103/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8327 - acc: 0.6362 - val_loss: 0.8282 - val_acc: 0.6488\n",
      "Epoch 104/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8320 - acc: 0.6360 - val_loss: 0.8275 - val_acc: 0.6473\n",
      "Epoch 105/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8315 - acc: 0.6366 - val_loss: 0.8271 - val_acc: 0.6475\n",
      "Epoch 106/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8308 - acc: 0.6369 - val_loss: 0.8257 - val_acc: 0.6470\n",
      "Epoch 107/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8306 - acc: 0.6363 - val_loss: 0.8257 - val_acc: 0.6519\n",
      "Epoch 108/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8299 - acc: 0.6365 - val_loss: 0.8252 - val_acc: 0.6498\n",
      "Epoch 109/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8294 - acc: 0.6370 - val_loss: 0.8248 - val_acc: 0.6501\n",
      "Epoch 110/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8287 - acc: 0.6387 - val_loss: 0.8248 - val_acc: 0.6521\n",
      "Epoch 111/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8285 - acc: 0.6385 - val_loss: 0.8239 - val_acc: 0.6496\n",
      "Epoch 112/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8278 - acc: 0.6387 - val_loss: 0.8235 - val_acc: 0.6496\n",
      "Epoch 113/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8274 - acc: 0.6378 - val_loss: 0.8234 - val_acc: 0.6503\n",
      "Epoch 114/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8269 - acc: 0.6384 - val_loss: 0.8225 - val_acc: 0.6529\n",
      "Epoch 115/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8264 - acc: 0.6383 - val_loss: 0.8218 - val_acc: 0.6531\n",
      "Epoch 116/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8259 - acc: 0.6402 - val_loss: 0.8219 - val_acc: 0.6488\n",
      "Epoch 117/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8255 - acc: 0.6396 - val_loss: 0.8214 - val_acc: 0.6534\n",
      "Epoch 118/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8250 - acc: 0.6399 - val_loss: 0.8210 - val_acc: 0.6542\n",
      "Epoch 119/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8246 - acc: 0.6406 - val_loss: 0.8198 - val_acc: 0.6521\n",
      "Epoch 120/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8241 - acc: 0.6393 - val_loss: 0.8203 - val_acc: 0.6542\n",
      "Epoch 121/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8236 - acc: 0.6409 - val_loss: 0.8196 - val_acc: 0.6552\n",
      "Epoch 122/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8232 - acc: 0.6415 - val_loss: 0.8193 - val_acc: 0.6542\n",
      "Epoch 123/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8227 - acc: 0.6410 - val_loss: 0.8183 - val_acc: 0.6552\n",
      "Epoch 124/150\n",
      "15664/15664 [==============================] - 0s 17us/step - loss: 0.8223 - acc: 0.6404 - val_loss: 0.8187 - val_acc: 0.6547\n",
      "Epoch 125/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8218 - acc: 0.6420 - val_loss: 0.8175 - val_acc: 0.6559\n",
      "Epoch 126/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8214 - acc: 0.6422 - val_loss: 0.8177 - val_acc: 0.6549\n",
      "Epoch 127/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8210 - acc: 0.6432 - val_loss: 0.8166 - val_acc: 0.6552\n",
      "Epoch 128/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8208 - acc: 0.6414 - val_loss: 0.8171 - val_acc: 0.6542\n",
      "Epoch 129/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8203 - acc: 0.6430 - val_loss: 0.8160 - val_acc: 0.6552\n",
      "Epoch 130/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8199 - acc: 0.6431 - val_loss: 0.8157 - val_acc: 0.6549\n",
      "Epoch 131/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8195 - acc: 0.6434 - val_loss: 0.8155 - val_acc: 0.6547\n",
      "Epoch 132/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8192 - acc: 0.6433 - val_loss: 0.8155 - val_acc: 0.6549\n",
      "Epoch 133/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8188 - acc: 0.6438 - val_loss: 0.8149 - val_acc: 0.6562\n",
      "Epoch 134/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8184 - acc: 0.6443 - val_loss: 0.8145 - val_acc: 0.6547\n",
      "Epoch 135/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8181 - acc: 0.6431 - val_loss: 0.8144 - val_acc: 0.6549\n",
      "Epoch 136/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8177 - acc: 0.6434 - val_loss: 0.8137 - val_acc: 0.6554\n",
      "Epoch 137/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8173 - acc: 0.6443 - val_loss: 0.8137 - val_acc: 0.6554\n",
      "Epoch 138/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8168 - acc: 0.6442 - val_loss: 0.8131 - val_acc: 0.6552\n",
      "Epoch 139/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8167 - acc: 0.6454 - val_loss: 0.8125 - val_acc: 0.6564\n",
      "Epoch 140/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8161 - acc: 0.6457 - val_loss: 0.8118 - val_acc: 0.6549\n",
      "Epoch 141/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8159 - acc: 0.6438 - val_loss: 0.8120 - val_acc: 0.6564\n",
      "Epoch 142/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8155 - acc: 0.6446 - val_loss: 0.8121 - val_acc: 0.6580\n",
      "Epoch 143/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8152 - acc: 0.6454 - val_loss: 0.8116 - val_acc: 0.6557\n",
      "Epoch 144/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8149 - acc: 0.6455 - val_loss: 0.8108 - val_acc: 0.6567\n",
      "Epoch 145/150\n",
      "15664/15664 [==============================] - 0s 17us/step - loss: 0.8147 - acc: 0.6445 - val_loss: 0.8107 - val_acc: 0.6564\n",
      "Epoch 146/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8143 - acc: 0.6456 - val_loss: 0.8108 - val_acc: 0.6572\n",
      "Epoch 147/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8139 - acc: 0.6454 - val_loss: 0.8107 - val_acc: 0.6559\n",
      "Epoch 148/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8136 - acc: 0.6456 - val_loss: 0.8106 - val_acc: 0.6567\n",
      "Epoch 149/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8132 - acc: 0.6461 - val_loss: 0.8098 - val_acc: 0.6582\n",
      "Epoch 150/150\n",
      "15664/15664 [==============================] - 0s 16us/step - loss: 0.8130 - acc: 0.6463 - val_loss: 0.8087 - val_acc: 0.6554\n",
      "15664/15664 [==============================] - 1s 32us/step\n",
      "==== Training loss, score are: 0.8128567321551345 0.6447906026557712 =======\n",
      "3915/3915 [==============================] - 0s 27us/step\n",
      "==== CV loss, score are: 0.8087320398096838 0.6554278416195135 =======\n",
      "\n",
      "\n",
      "===== Model: fast_text_char_glove  ========:\n",
      " Cross-val log losses are: [0.81595307091933333, 0.82309050842853493, 0.82084592173363069, 0.80906959211613616, 0.80873203528611248]\n",
      "====== Mean cross-val log loss is: 0.8155382256967496 =========\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Fast text (char level) and add predictions as features:\n",
    "\n",
    "# No pre-trained vectors:\n",
    "print('Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "model_name = \"fast_text_char_none\"\n",
    "model_preprocess_function = run_NN_preprocess\n",
    "model_function = run_Fasttext\n",
    "model_params = {'temp': 0 }\n",
    "model_preprocess_params = {'char_level': 1, 'word_vector_type': None }\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"NN\", \n",
    "                                           train_raw, y_train_one_hot, test_raw, 1) \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n",
    "\n",
    "# Pre-trained word2vec using vectors:\n",
    "print('Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "wordvecsize = 20\n",
    "create_gensim_wordvectors = 0\n",
    "if create_gensim_wordvectors:\n",
    "    create_gensim_wordvec(train_raw['text'], wordvecsize)\n",
    "model_name = \"fast_text_char_gensim\"\n",
    "model_preprocess_function = run_NN_preprocess\n",
    "model_function = run_Fasttext\n",
    "model_params = {'temp': 0 }\n",
    "model_preprocess_params = {'char_level': 1, 'word_vector_type': \"gensim\" }\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"NN\", \n",
    "                                           train_raw, y_train_one_hot, test_raw, 1) \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n",
    "    \n",
    "# Glove vectors:\n",
    "print('Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "model_name = \"fast_text_char_glove\"\n",
    "model_preprocess_function = run_NN_preprocess\n",
    "model_function = run_Fasttext\n",
    "model_params = {'temp': 0 }\n",
    "model_preprocess_params = {'char_level': 1, 'word_vector_type': \"glove\" }\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"NN\", \n",
    "                                           train_raw, y_train_one_hot, test_raw, 1) \n",
    "train_raw, test_raw = add_pred_features(model_name, \n",
    "                                        train_raw, test_raw, \n",
    "                                        pred_train, pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final stacking model - Run XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words</th>\n",
       "      <th>mnb_tfidf_eap</th>\n",
       "      <th>mnb_tfidf_hpl</th>\n",
       "      <th>mnb_tfidf_mws</th>\n",
       "      <th>mnb_count_eap</th>\n",
       "      <th>...</th>\n",
       "      <th>cnn_glove_mws</th>\n",
       "      <th>fast_text_char_none_eap</th>\n",
       "      <th>fast_text_char_none_hpl</th>\n",
       "      <th>fast_text_char_none_mws</th>\n",
       "      <th>fast_text_char_gensim_eap</th>\n",
       "      <th>fast_text_char_gensim_hpl</th>\n",
       "      <th>fast_text_char_gensim_mws</th>\n",
       "      <th>fast_text_char_glove_eap</th>\n",
       "      <th>fast_text_char_glove_hpl</th>\n",
       "      <th>fast_text_char_glove_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>0.942005</td>\n",
       "      <td>0.01129</td>\n",
       "      <td>0.046705</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013262</td>\n",
       "      <td>0.998048</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.999284</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.839104</td>\n",
       "      <td>0.099007</td>\n",
       "      <td>0.061889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "\n",
       "   num_punctuations  num_stopwords  num_words  mnb_tfidf_eap  mnb_tfidf_hpl  \\\n",
       "0                 7             19         41       0.942005        0.01129   \n",
       "\n",
       "   mnb_tfidf_mws  mnb_count_eap            ...             cnn_glove_mws  \\\n",
       "0       0.046705       0.999989            ...                  0.013262   \n",
       "\n",
       "   fast_text_char_none_eap  fast_text_char_none_hpl  fast_text_char_none_mws  \\\n",
       "0                 0.998048                 0.000732                 0.001221   \n",
       "\n",
       "   fast_text_char_gensim_eap  fast_text_char_gensim_hpl  \\\n",
       "0                   0.999284                   0.000205   \n",
       "\n",
       "   fast_text_char_gensim_mws  fast_text_char_glove_eap  \\\n",
       "0                   0.000511                  0.839104   \n",
       "\n",
       "   fast_text_char_glove_hpl  fast_text_char_glove_mws  \n",
       "0                  0.099007                  0.061889  \n",
       "\n",
       "[1 rows x 45 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words</th>\n",
       "      <th>mnb_tfidf_eap</th>\n",
       "      <th>mnb_tfidf_hpl</th>\n",
       "      <th>mnb_tfidf_mws</th>\n",
       "      <th>mnb_count_eap</th>\n",
       "      <th>mnb_count_hpl</th>\n",
       "      <th>...</th>\n",
       "      <th>cnn_glove_mws</th>\n",
       "      <th>fast_text_char_none_eap</th>\n",
       "      <th>fast_text_char_none_hpl</th>\n",
       "      <th>fast_text_char_none_mws</th>\n",
       "      <th>fast_text_char_gensim_eap</th>\n",
       "      <th>fast_text_char_gensim_hpl</th>\n",
       "      <th>fast_text_char_gensim_mws</th>\n",
       "      <th>fast_text_char_glove_eap</th>\n",
       "      <th>fast_text_char_glove_hpl</th>\n",
       "      <th>fast_text_char_glove_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.07408</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.920963</td>\n",
       "      <td>0.038465</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577344</td>\n",
       "      <td>0.029432</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.951296</td>\n",
       "      <td>0.011391</td>\n",
       "      <td>0.011352</td>\n",
       "      <td>0.977257</td>\n",
       "      <td>0.314749</td>\n",
       "      <td>0.133217</td>\n",
       "      <td>0.552034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "\n",
       "   num_punctuations  num_stopwords  num_words  mnb_tfidf_eap  mnb_tfidf_hpl  \\\n",
       "0                 3              9         19        0.07408       0.004957   \n",
       "\n",
       "   mnb_tfidf_mws  mnb_count_eap  mnb_count_hpl            ...             \\\n",
       "0       0.920963       0.038465       0.000992            ...              \n",
       "\n",
       "   cnn_glove_mws  fast_text_char_none_eap  fast_text_char_none_hpl  \\\n",
       "0       0.577344                 0.029432                 0.019272   \n",
       "\n",
       "   fast_text_char_none_mws  fast_text_char_gensim_eap  \\\n",
       "0                 0.951296                   0.011391   \n",
       "\n",
       "   fast_text_char_gensim_hpl  fast_text_char_gensim_mws  \\\n",
       "0                   0.011352                   0.977257   \n",
       "\n",
       "   fast_text_char_glove_eap  fast_text_char_glove_hpl  \\\n",
       "0                  0.314749                  0.133217   \n",
       "\n",
       "   fast_text_char_glove_mws  \n",
       "0                  0.552034  \n",
       "\n",
       "[1 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'text', 'author', 'num_punctuations', 'num_stopwords',\n",
      "       'num_words', 'mnb_tfidf_eap', 'mnb_tfidf_hpl', 'mnb_tfidf_mws',\n",
      "       'mnb_count_eap', 'mnb_count_hpl', 'mnb_count_mws', 'mnb_tfidf_char_eap',\n",
      "       'mnb_tfidf_char_hpl', 'mnb_tfidf_char_mws', 'lr_count_eap',\n",
      "       'lr_count_hpl', 'lr_count_mws', 'fast_text_none_eap',\n",
      "       'fast_text_none_hpl', 'fast_text_none_mws', 'fast_text_gensim_eap',\n",
      "       'fast_text_gensim_hpl', 'fast_text_gensim_mws', 'fast_text_glove_eap',\n",
      "       'fast_text_glove_hpl', 'fast_text_glove_mws', 'cnn_none_eap',\n",
      "       'cnn_none_hpl', 'cnn_none_mws', 'cnn_gensim_eap', 'cnn_gensim_hpl',\n",
      "       'cnn_gensim_mws', 'cnn_glove_eap', 'cnn_glove_hpl', 'cnn_glove_mws',\n",
      "       'fast_text_char_none_eap', 'fast_text_char_none_hpl',\n",
      "       'fast_text_char_none_mws', 'fast_text_char_gensim_eap',\n",
      "       'fast_text_char_gensim_hpl', 'fast_text_char_gensim_mws',\n",
      "       'fast_text_char_glove_eap', 'fast_text_char_glove_hpl',\n",
      "       'fast_text_char_glove_mws'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Display the columns in the training and test data at this point\n",
    "display(train_raw.head(1))\n",
    "display(test_raw.head(1))\n",
    "# Display all the features\n",
    "print(train_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running kfold training with model xgboost_stacking_final\n",
      "Shapes: x_train_raw.shape (19579, 45), y_train_raw.shape (19579,) , x_test_raw.shape (8392, 44)\n",
      "Running preprocess: <function run_xgb_preprocess at 0x7f64a2b12158>\n",
      "Shapes: x_train.shape (19579, 42), y_train.shape (19579,) , x_test.shape (8392, 42)\n",
      "Running fold 1\n",
      "[0]\ttrain-mlogloss:0.99489\tcross-valid-mlogloss:0.996059\n",
      "Multiple eval metrics have been passed: 'cross-valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until cross-valid-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.357634\tcross-valid-mlogloss:0.370399\n",
      "[40]\ttrain-mlogloss:0.285885\tcross-valid-mlogloss:0.305027\n",
      "[60]\ttrain-mlogloss:0.266458\tcross-valid-mlogloss:0.29167\n",
      "[80]\ttrain-mlogloss:0.255798\tcross-valid-mlogloss:0.286635\n",
      "[100]\ttrain-mlogloss:0.247489\tcross-valid-mlogloss:0.284522\n",
      "[120]\ttrain-mlogloss:0.240158\tcross-valid-mlogloss:0.283111\n",
      "[140]\ttrain-mlogloss:0.233112\tcross-valid-mlogloss:0.28258\n",
      "[160]\ttrain-mlogloss:0.22696\tcross-valid-mlogloss:0.282749\n",
      "[180]\ttrain-mlogloss:0.220761\tcross-valid-mlogloss:0.281765\n",
      "[200]\ttrain-mlogloss:0.2151\tcross-valid-mlogloss:0.281388\n",
      "[220]\ttrain-mlogloss:0.209764\tcross-valid-mlogloss:0.28103\n",
      "[240]\ttrain-mlogloss:0.20482\tcross-valid-mlogloss:0.280823\n",
      "[260]\ttrain-mlogloss:0.199924\tcross-valid-mlogloss:0.28144\n",
      "[280]\ttrain-mlogloss:0.195522\tcross-valid-mlogloss:0.282084\n",
      "Stopping. Best iteration:\n",
      "[244]\ttrain-mlogloss:0.203734\tcross-valid-mlogloss:0.280771\n",
      "\n",
      "Running fold 2\n",
      "[0]\ttrain-mlogloss:0.994957\tcross-valid-mlogloss:0.996138\n",
      "Multiple eval metrics have been passed: 'cross-valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until cross-valid-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.357542\tcross-valid-mlogloss:0.371007\n",
      "[40]\ttrain-mlogloss:0.284863\tcross-valid-mlogloss:0.306904\n",
      "[60]\ttrain-mlogloss:0.2658\tcross-valid-mlogloss:0.293866\n",
      "[80]\ttrain-mlogloss:0.25492\tcross-valid-mlogloss:0.289668\n",
      "[100]\ttrain-mlogloss:0.246663\tcross-valid-mlogloss:0.287402\n",
      "[120]\ttrain-mlogloss:0.239143\tcross-valid-mlogloss:0.285772\n",
      "[140]\ttrain-mlogloss:0.232208\tcross-valid-mlogloss:0.284951\n",
      "[160]\ttrain-mlogloss:0.226052\tcross-valid-mlogloss:0.284242\n",
      "[180]\ttrain-mlogloss:0.220049\tcross-valid-mlogloss:0.284223\n",
      "[200]\ttrain-mlogloss:0.214272\tcross-valid-mlogloss:0.284181\n",
      "[220]\ttrain-mlogloss:0.209069\tcross-valid-mlogloss:0.284559\n",
      "[240]\ttrain-mlogloss:0.204124\tcross-valid-mlogloss:0.284661\n",
      "Stopping. Best iteration:\n",
      "[209]\ttrain-mlogloss:0.211855\tcross-valid-mlogloss:0.284009\n",
      "\n",
      "Running fold 3\n",
      "[0]\ttrain-mlogloss:0.995459\tcross-valid-mlogloss:0.995887\n",
      "Multiple eval metrics have been passed: 'cross-valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until cross-valid-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.359933\tcross-valid-mlogloss:0.364394\n",
      "[40]\ttrain-mlogloss:0.28762\tcross-valid-mlogloss:0.29664\n",
      "[60]\ttrain-mlogloss:0.268303\tcross-valid-mlogloss:0.282868\n",
      "[80]\ttrain-mlogloss:0.25711\tcross-valid-mlogloss:0.278698\n",
      "[100]\ttrain-mlogloss:0.248783\tcross-valid-mlogloss:0.276603\n",
      "[120]\ttrain-mlogloss:0.241371\tcross-valid-mlogloss:0.275995\n",
      "[140]\ttrain-mlogloss:0.234912\tcross-valid-mlogloss:0.275541\n",
      "[160]\ttrain-mlogloss:0.228865\tcross-valid-mlogloss:0.275494\n",
      "[180]\ttrain-mlogloss:0.222747\tcross-valid-mlogloss:0.27561\n",
      "[200]\ttrain-mlogloss:0.216999\tcross-valid-mlogloss:0.275862\n",
      "Stopping. Best iteration:\n",
      "[154]\ttrain-mlogloss:0.230487\tcross-valid-mlogloss:0.275379\n",
      "\n",
      "Running fold 4\n",
      "[0]\ttrain-mlogloss:0.99508\tcross-valid-mlogloss:0.995546\n",
      "Multiple eval metrics have been passed: 'cross-valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until cross-valid-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.359066\tcross-valid-mlogloss:0.366009\n",
      "[40]\ttrain-mlogloss:0.286764\tcross-valid-mlogloss:0.299644\n",
      "[60]\ttrain-mlogloss:0.267457\tcross-valid-mlogloss:0.286236\n",
      "[80]\ttrain-mlogloss:0.256943\tcross-valid-mlogloss:0.281646\n",
      "[100]\ttrain-mlogloss:0.248592\tcross-valid-mlogloss:0.278934\n",
      "[120]\ttrain-mlogloss:0.241352\tcross-valid-mlogloss:0.278324\n",
      "[140]\ttrain-mlogloss:0.234646\tcross-valid-mlogloss:0.277808\n",
      "[160]\ttrain-mlogloss:0.228471\tcross-valid-mlogloss:0.277539\n",
      "[180]\ttrain-mlogloss:0.222246\tcross-valid-mlogloss:0.277275\n",
      "[200]\ttrain-mlogloss:0.21659\tcross-valid-mlogloss:0.27683\n",
      "[220]\ttrain-mlogloss:0.211481\tcross-valid-mlogloss:0.27662\n",
      "[240]\ttrain-mlogloss:0.206477\tcross-valid-mlogloss:0.276423\n",
      "[260]\ttrain-mlogloss:0.201601\tcross-valid-mlogloss:0.276185\n",
      "[280]\ttrain-mlogloss:0.196648\tcross-valid-mlogloss:0.276687\n",
      "Stopping. Best iteration:\n",
      "[231]\ttrain-mlogloss:0.208626\tcross-valid-mlogloss:0.276124\n",
      "\n",
      "Running fold 5\n",
      "[0]\ttrain-mlogloss:0.994172\tcross-valid-mlogloss:0.994354\n",
      "Multiple eval metrics have been passed: 'cross-valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until cross-valid-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.362262\tcross-valid-mlogloss:0.370273\n",
      "[40]\ttrain-mlogloss:0.287382\tcross-valid-mlogloss:0.300461\n",
      "[60]\ttrain-mlogloss:0.268155\tcross-valid-mlogloss:0.287233\n",
      "[80]\ttrain-mlogloss:0.256901\tcross-valid-mlogloss:0.281848\n",
      "[100]\ttrain-mlogloss:0.24793\tcross-valid-mlogloss:0.279642\n",
      "[120]\ttrain-mlogloss:0.240459\tcross-valid-mlogloss:0.278149\n",
      "[140]\ttrain-mlogloss:0.23362\tcross-valid-mlogloss:0.277311\n",
      "[160]\ttrain-mlogloss:0.227271\tcross-valid-mlogloss:0.276909\n",
      "[180]\ttrain-mlogloss:0.221323\tcross-valid-mlogloss:0.276729\n",
      "[200]\ttrain-mlogloss:0.215852\tcross-valid-mlogloss:0.276869\n",
      "[220]\ttrain-mlogloss:0.210448\tcross-valid-mlogloss:0.277246\n",
      "[240]\ttrain-mlogloss:0.205172\tcross-valid-mlogloss:0.277749\n",
      "Stopping. Best iteration:\n",
      "[192]\ttrain-mlogloss:0.218064\tcross-valid-mlogloss:0.276426\n",
      "\n",
      "\n",
      "\n",
      "===== Model: xgboost_stacking_final  ========:\n",
      " Cross-val log losses are: [0.28077129560865283, 0.2840085180417305, 0.27537939469448852, 0.27612407379749465, 0.27642561327629245]\n",
      "====== Mean cross-val log loss is: 0.2785417790837318 =========\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1AAAALJCAYAAAC6BcIOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMyslf4/chekhOjYDjkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f649b0e4b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run XGboost:\n",
    "model_name = \"xgboost_stacking_final\"\n",
    "model_preprocess_function = run_xgb_preprocess\n",
    "model_function = run_xgb\n",
    "model_params = {'seed_val': 0, 'child': 1, 'colsample': 0.3 }\n",
    "#features_drop = ['num_punctuations','num_stopwords','num_words']\n",
    "#features_drop = ['svm_tfidf_eap', 'svm_tfidf_hpl','svm_tfidf_mws',\n",
    "#                'rf_tfidf_eap', 'rf_tfidf_hpl', 'rf_tfidf_mws']\n",
    "#features_drop = ['fast_text_char_glove_eap', 'fast_text_char_glove_hpl', 'fast_text_char_glove_mws', \n",
    "#                 'fast_text_glove_eap', 'fast_text_glove_hpl', 'fast_text_glove_mws', \n",
    "#                 'cnn_glove_eap', 'cnn_glove_hpl', 'cnn_glove_mws']\n",
    "features_drop = None\n",
    "model_preprocess_params = {'features_drop': features_drop }\n",
    "pred_train, pred_test, model = run_kfold_training(model_name,\n",
    "                                           model_preprocess_function, \n",
    "                                           model_preprocess_params,\n",
    "                                           model_function,\n",
    "                                           model_params,\n",
    "                                           \"VECT\", \n",
    "                                           train_raw, y_train_raw, test_raw, 0) \n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "xgb.plot_importance(model, height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file: final_submission_submission.csv\n"
     ]
    }
   ],
   "source": [
    "create_submission_final(\"final_submission\", pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find number of misclassifications and plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_misclass 2158, num_correctclass 17421, accuracy 88.97798661831554\n"
     ]
    }
   ],
   "source": [
    "def find_misclassifications(y_true, pred):\n",
    "    idx = 0\n",
    "    num_misclass = 0\n",
    "    num_correctclass = 0\n",
    "    for i in y_true:\n",
    "        true_val = i\n",
    "        pred_val = np.argmax(pred[idx])\n",
    "        if true_val != pred_val:\n",
    "            #print(\"idx {}, true_val {}, pred_val {}\".format(idx, true_val, pred_val))\n",
    "            #display(train_raw.loc[idx,'text'])\n",
    "            #display(train_raw.loc[idx,'author'])\n",
    "            num_misclass += 1\n",
    "            #break\n",
    "        else:\n",
    "            num_correctclass += 1\n",
    "        idx+=1\n",
    "    print(\"num_misclass {}, num_correctclass {}, accuracy {}\".format(\n",
    "                                                            num_misclass, num_correctclass,\n",
    "                                                            (num_correctclass*100./idx)))\n",
    "            \n",
    "find_misclassifications(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix:\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    \n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n",
      "Confusion matrix, without normalization\n",
      "[[7109  315  476]\n",
      " [ 428 5006  201]\n",
      " [ 557  181 5306]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAI4CAYAAACflWgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f649ce64dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmatr = confusion_matrix(y_train, np.argmax(pred_train,axis=1))\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.sum(cmatr))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(cmatr, classes=['EAP', 'HPL', 'MWS'],\n",
    "                      title='Confusion matrix without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mnb_tf</th>\n",
       "      <th>mnb_cnt</th>\n",
       "      <th>mnb_tfc</th>\n",
       "      <th>lr_cnt</th>\n",
       "      <th>ft_n</th>\n",
       "      <th>ft_gs</th>\n",
       "      <th>ft_gl</th>\n",
       "      <th>cnn_n</th>\n",
       "      <th>cnn_gs</th>\n",
       "      <th>cnn_gl</th>\n",
       "      <th>ftc_n</th>\n",
       "      <th>ftc_gs</th>\n",
       "      <th>ftc_gl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_mws</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_mws</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_mws</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_mws</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_mws</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_mws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_mws</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_mws</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_mws</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_mws</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_mws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_mws</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_mws</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_mws</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_mws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_mws</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_mws</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_mws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_mws</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_mws</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_mws</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_mws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_mws</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_mws</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_mws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19549</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_mws</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_mws</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_mws</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_mws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19550</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19551</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19552</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19553</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19554</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19555</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19556</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19557</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19558</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19559</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19560</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19561</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19562</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19563</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19564</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19565</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19566</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_mws</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_mws</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19567</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19568</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19569</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_mws</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_mws</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_mws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19570</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_mws</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_mws</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_mws</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_mws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19571</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_mws</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19572</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_mws</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_mws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19573</th>\n",
       "      <td>mnb_tfidf_mws</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_mws</td>\n",
       "      <td>fast_text_none_mws</td>\n",
       "      <td>fast_text_gensim_mws</td>\n",
       "      <td>fast_text_glove_mws</td>\n",
       "      <td>cnn_none_mws</td>\n",
       "      <td>cnn_gensim_mws</td>\n",
       "      <td>cnn_glove_mws</td>\n",
       "      <td>fast_text_char_none_mws</td>\n",
       "      <td>fast_text_char_gensim_mws</td>\n",
       "      <td>fast_text_char_glove_mws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_hpl</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_hpl</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_hpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19575</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_mws</td>\n",
       "      <td>mnb_tfidf_char_mws</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19576</th>\n",
       "      <td>mnb_tfidf_eap</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_eap</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19577</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_eap</td>\n",
       "      <td>mnb_tfidf_char_eap</td>\n",
       "      <td>lr_count_eap</td>\n",
       "      <td>fast_text_none_eap</td>\n",
       "      <td>fast_text_gensim_eap</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_eap</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_eap</td>\n",
       "      <td>fast_text_char_gensim_eap</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19578</th>\n",
       "      <td>mnb_tfidf_hpl</td>\n",
       "      <td>mnb_count_hpl</td>\n",
       "      <td>mnb_tfidf_char_hpl</td>\n",
       "      <td>lr_count_hpl</td>\n",
       "      <td>fast_text_none_hpl</td>\n",
       "      <td>fast_text_gensim_hpl</td>\n",
       "      <td>fast_text_glove_eap</td>\n",
       "      <td>cnn_none_hpl</td>\n",
       "      <td>cnn_gensim_hpl</td>\n",
       "      <td>cnn_glove_eap</td>\n",
       "      <td>fast_text_char_none_hpl</td>\n",
       "      <td>fast_text_char_gensim_hpl</td>\n",
       "      <td>fast_text_char_glove_eap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19579 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              mnb_tf        mnb_cnt             mnb_tfc        lr_cnt  \\\n",
       "0      mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "1      mnb_tfidf_mws  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "2      mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "3      mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_mws  lr_count_mws   \n",
       "4      mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "5      mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_mws  lr_count_mws   \n",
       "6      mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "7      mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "8      mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "9      mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_mws  lr_count_mws   \n",
       "10     mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_mws  lr_count_mws   \n",
       "11     mnb_tfidf_eap  mnb_count_mws  mnb_tfidf_char_eap  lr_count_eap   \n",
       "12     mnb_tfidf_hpl  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_hpl   \n",
       "13     mnb_tfidf_hpl  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_hpl   \n",
       "14     mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "15     mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_mws  lr_count_mws   \n",
       "16     mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "17     mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_mws  lr_count_mws   \n",
       "18     mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19     mnb_tfidf_hpl  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_hpl   \n",
       "20     mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "21     mnb_tfidf_hpl  mnb_count_eap  mnb_tfidf_char_eap  lr_count_hpl   \n",
       "22     mnb_tfidf_hpl  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_mws   \n",
       "23     mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "24     mnb_tfidf_eap  mnb_count_hpl  mnb_tfidf_char_eap  lr_count_hpl   \n",
       "25     mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_hpl  lr_count_hpl   \n",
       "26     mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "27     mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "28     mnb_tfidf_hpl  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_hpl   \n",
       "29     mnb_tfidf_hpl  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_hpl   \n",
       "...              ...            ...                 ...           ...   \n",
       "19549  mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_mws  lr_count_mws   \n",
       "19550  mnb_tfidf_eap  mnb_count_mws  mnb_tfidf_char_mws  lr_count_eap   \n",
       "19551  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19552  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19553  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19554  mnb_tfidf_hpl  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_hpl   \n",
       "19555  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_hpl  lr_count_eap   \n",
       "19556  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_hpl   \n",
       "19557  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19558  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19559  mnb_tfidf_hpl  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_hpl   \n",
       "19560  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19561  mnb_tfidf_hpl  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_hpl   \n",
       "19562  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19563  mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_mws  lr_count_eap   \n",
       "19564  mnb_tfidf_mws  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_hpl   \n",
       "19565  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19566  mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_hpl  lr_count_mws   \n",
       "19567  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_hpl   \n",
       "19568  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19569  mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_mws  lr_count_hpl   \n",
       "19570  mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_mws  lr_count_mws   \n",
       "19571  mnb_tfidf_hpl  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_hpl   \n",
       "19572  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19573  mnb_tfidf_mws  mnb_count_mws  mnb_tfidf_char_mws  lr_count_mws   \n",
       "19574  mnb_tfidf_hpl  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_eap   \n",
       "19575  mnb_tfidf_eap  mnb_count_mws  mnb_tfidf_char_mws  lr_count_eap   \n",
       "19576  mnb_tfidf_eap  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19577  mnb_tfidf_hpl  mnb_count_eap  mnb_tfidf_char_eap  lr_count_eap   \n",
       "19578  mnb_tfidf_hpl  mnb_count_hpl  mnb_tfidf_char_hpl  lr_count_hpl   \n",
       "\n",
       "                     ft_n                 ft_gs                ft_gl  \\\n",
       "0      fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "1      fast_text_none_hpl  fast_text_gensim_mws  fast_text_glove_eap   \n",
       "2      fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "3      fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_hpl   \n",
       "4      fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_hpl   \n",
       "5      fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_mws   \n",
       "6      fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "7      fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "8      fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "9      fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_eap   \n",
       "10     fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_mws   \n",
       "11     fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "12     fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_hpl   \n",
       "13     fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_hpl   \n",
       "14     fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "15     fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_mws   \n",
       "16     fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_mws   \n",
       "17     fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_mws   \n",
       "18     fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19     fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_eap   \n",
       "20     fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "21     fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_eap   \n",
       "22     fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "23     fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "24     fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_hpl   \n",
       "25     fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_hpl   \n",
       "26     fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "27     fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_mws   \n",
       "28     fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_eap   \n",
       "29     fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_hpl   \n",
       "...                   ...                   ...                  ...   \n",
       "19549  fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_mws   \n",
       "19550  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19551  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19552  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19553  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19554  fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_hpl   \n",
       "19555  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19556  fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_hpl   \n",
       "19557  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19558  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19559  fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_hpl   \n",
       "19560  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19561  fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_eap   \n",
       "19562  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19563  fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_hpl   \n",
       "19564  fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_hpl   \n",
       "19565  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19566  fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_eap   \n",
       "19567  fast_text_none_eap  fast_text_gensim_hpl  fast_text_glove_eap   \n",
       "19568  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_hpl   \n",
       "19569  fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_mws   \n",
       "19570  fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_mws   \n",
       "19571  fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_eap   \n",
       "19572  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_mws   \n",
       "19573  fast_text_none_mws  fast_text_gensim_mws  fast_text_glove_mws   \n",
       "19574  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_hpl   \n",
       "19575  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19576  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19577  fast_text_none_eap  fast_text_gensim_eap  fast_text_glove_eap   \n",
       "19578  fast_text_none_hpl  fast_text_gensim_hpl  fast_text_glove_eap   \n",
       "\n",
       "              cnn_n          cnn_gs         cnn_gl                    ftc_n  \\\n",
       "0      cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "1      cnn_none_eap  cnn_gensim_hpl  cnn_glove_eap  fast_text_char_none_hpl   \n",
       "2      cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "3      cnn_none_mws  cnn_gensim_mws  cnn_glove_mws  fast_text_char_none_mws   \n",
       "4      cnn_none_eap  cnn_gensim_eap  cnn_glove_hpl  fast_text_char_none_eap   \n",
       "5      cnn_none_mws  cnn_gensim_mws  cnn_glove_mws  fast_text_char_none_mws   \n",
       "6      cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "7      cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "8      cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "9      cnn_none_mws  cnn_gensim_mws  cnn_glove_eap  fast_text_char_none_mws   \n",
       "10     cnn_none_mws  cnn_gensim_mws  cnn_glove_mws  fast_text_char_none_mws   \n",
       "11     cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "12     cnn_none_hpl  cnn_gensim_hpl  cnn_glove_hpl  fast_text_char_none_hpl   \n",
       "13     cnn_none_hpl  cnn_gensim_hpl  cnn_glove_hpl  fast_text_char_none_hpl   \n",
       "14     cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "15     cnn_none_mws  cnn_gensim_mws  cnn_glove_mws  fast_text_char_none_mws   \n",
       "16     cnn_none_eap  cnn_gensim_eap  cnn_glove_mws  fast_text_char_none_eap   \n",
       "17     cnn_none_mws  cnn_gensim_mws  cnn_glove_mws  fast_text_char_none_mws   \n",
       "18     cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19     cnn_none_hpl  cnn_gensim_hpl  cnn_glove_eap  fast_text_char_none_hpl   \n",
       "20     cnn_none_eap  cnn_gensim_hpl  cnn_glove_eap  fast_text_char_none_eap   \n",
       "21     cnn_none_hpl  cnn_gensim_hpl  cnn_glove_hpl  fast_text_char_none_hpl   \n",
       "22     cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "23     cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "24     cnn_none_hpl  cnn_gensim_hpl  cnn_glove_hpl  fast_text_char_none_hpl   \n",
       "25     cnn_none_mws  cnn_gensim_mws  cnn_glove_hpl  fast_text_char_none_mws   \n",
       "26     cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "27     cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "28     cnn_none_hpl  cnn_gensim_hpl  cnn_glove_eap  fast_text_char_none_hpl   \n",
       "29     cnn_none_hpl  cnn_gensim_hpl  cnn_glove_hpl  fast_text_char_none_hpl   \n",
       "...             ...             ...            ...                      ...   \n",
       "19549  cnn_none_mws  cnn_gensim_mws  cnn_glove_mws  fast_text_char_none_mws   \n",
       "19550  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19551  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19552  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19553  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19554  cnn_none_hpl  cnn_gensim_hpl  cnn_glove_hpl  fast_text_char_none_hpl   \n",
       "19555  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19556  cnn_none_hpl  cnn_gensim_hpl  cnn_glove_hpl  fast_text_char_none_hpl   \n",
       "19557  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19558  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19559  cnn_none_hpl  cnn_gensim_hpl  cnn_glove_hpl  fast_text_char_none_hpl   \n",
       "19560  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19561  cnn_none_hpl  cnn_gensim_hpl  cnn_glove_eap  fast_text_char_none_hpl   \n",
       "19562  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19563  cnn_none_mws  cnn_gensim_mws  cnn_glove_hpl  fast_text_char_none_mws   \n",
       "19564  cnn_none_hpl  cnn_gensim_hpl  cnn_glove_hpl  fast_text_char_none_hpl   \n",
       "19565  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19566  cnn_none_mws  cnn_gensim_mws  cnn_glove_mws  fast_text_char_none_mws   \n",
       "19567  cnn_none_hpl  cnn_gensim_hpl  cnn_glove_eap  fast_text_char_none_hpl   \n",
       "19568  cnn_none_eap  cnn_gensim_eap  cnn_glove_hpl  fast_text_char_none_eap   \n",
       "19569  cnn_none_mws  cnn_gensim_mws  cnn_glove_mws  fast_text_char_none_mws   \n",
       "19570  cnn_none_mws  cnn_gensim_mws  cnn_glove_mws  fast_text_char_none_mws   \n",
       "19571  cnn_none_hpl  cnn_gensim_hpl  cnn_glove_mws  fast_text_char_none_hpl   \n",
       "19572  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19573  cnn_none_mws  cnn_gensim_mws  cnn_glove_mws  fast_text_char_none_mws   \n",
       "19574  cnn_none_hpl  cnn_gensim_hpl  cnn_glove_hpl  fast_text_char_none_eap   \n",
       "19575  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19576  cnn_none_eap  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19577  cnn_none_hpl  cnn_gensim_eap  cnn_glove_eap  fast_text_char_none_eap   \n",
       "19578  cnn_none_hpl  cnn_gensim_hpl  cnn_glove_eap  fast_text_char_none_hpl   \n",
       "\n",
       "                          ftc_gs                    ftc_gl  \n",
       "0      fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "1      fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "2      fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "3      fast_text_char_gensim_mws  fast_text_char_glove_hpl  \n",
       "4      fast_text_char_gensim_eap  fast_text_char_glove_hpl  \n",
       "5      fast_text_char_gensim_mws  fast_text_char_glove_mws  \n",
       "6      fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "7      fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "8      fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "9      fast_text_char_gensim_mws  fast_text_char_glove_eap  \n",
       "10     fast_text_char_gensim_mws  fast_text_char_glove_mws  \n",
       "11     fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "12     fast_text_char_gensim_hpl  fast_text_char_glove_hpl  \n",
       "13     fast_text_char_gensim_hpl  fast_text_char_glove_hpl  \n",
       "14     fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "15     fast_text_char_gensim_mws  fast_text_char_glove_mws  \n",
       "16     fast_text_char_gensim_eap  fast_text_char_glove_mws  \n",
       "17     fast_text_char_gensim_mws  fast_text_char_glove_mws  \n",
       "18     fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19     fast_text_char_gensim_hpl  fast_text_char_glove_eap  \n",
       "20     fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "21     fast_text_char_gensim_hpl  fast_text_char_glove_eap  \n",
       "22     fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "23     fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "24     fast_text_char_gensim_eap  fast_text_char_glove_hpl  \n",
       "25     fast_text_char_gensim_mws  fast_text_char_glove_hpl  \n",
       "26     fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "27     fast_text_char_gensim_eap  fast_text_char_glove_mws  \n",
       "28     fast_text_char_gensim_hpl  fast_text_char_glove_eap  \n",
       "29     fast_text_char_gensim_hpl  fast_text_char_glove_hpl  \n",
       "...                          ...                       ...  \n",
       "19549  fast_text_char_gensim_mws  fast_text_char_glove_mws  \n",
       "19550  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19551  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19552  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19553  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19554  fast_text_char_gensim_hpl  fast_text_char_glove_hpl  \n",
       "19555  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19556  fast_text_char_gensim_hpl  fast_text_char_glove_hpl  \n",
       "19557  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19558  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19559  fast_text_char_gensim_hpl  fast_text_char_glove_hpl  \n",
       "19560  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19561  fast_text_char_gensim_hpl  fast_text_char_glove_eap  \n",
       "19562  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19563  fast_text_char_gensim_mws  fast_text_char_glove_hpl  \n",
       "19564  fast_text_char_gensim_hpl  fast_text_char_glove_hpl  \n",
       "19565  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19566  fast_text_char_gensim_mws  fast_text_char_glove_eap  \n",
       "19567  fast_text_char_gensim_hpl  fast_text_char_glove_eap  \n",
       "19568  fast_text_char_gensim_eap  fast_text_char_glove_hpl  \n",
       "19569  fast_text_char_gensim_mws  fast_text_char_glove_mws  \n",
       "19570  fast_text_char_gensim_mws  fast_text_char_glove_mws  \n",
       "19571  fast_text_char_gensim_hpl  fast_text_char_glove_eap  \n",
       "19572  fast_text_char_gensim_eap  fast_text_char_glove_mws  \n",
       "19573  fast_text_char_gensim_mws  fast_text_char_glove_mws  \n",
       "19574  fast_text_char_gensim_eap  fast_text_char_glove_hpl  \n",
       "19575  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19576  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19577  fast_text_char_gensim_eap  fast_text_char_glove_eap  \n",
       "19578  fast_text_char_gensim_hpl  fast_text_char_glove_eap  \n",
       "\n",
       "[19579 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mnb_tf</th>\n",
       "      <th>mnb_cnt</th>\n",
       "      <th>mnb_tfc</th>\n",
       "      <th>lr_cnt</th>\n",
       "      <th>ft_n</th>\n",
       "      <th>ft_gs</th>\n",
       "      <th>ft_gl</th>\n",
       "      <th>cnn_n</th>\n",
       "      <th>cnn_gs</th>\n",
       "      <th>cnn_gl</th>\n",
       "      <th>ftc_n</th>\n",
       "      <th>ftc_gs</th>\n",
       "      <th>ftc_gl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19549</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19550</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19551</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19552</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19553</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19554</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19555</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19556</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19557</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19558</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19559</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19560</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19561</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19562</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19563</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19564</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19565</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19566</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19569</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19570</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19571</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19572</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19573</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19575</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19576</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19577</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19578</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19579 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mnb_tf  mnb_cnt  mnb_tfc  lr_cnt  ft_n  ft_gs  ft_gl  cnn_n  cnn_gs  \\\n",
       "0           0        0        0       0     0      0      0      0       0   \n",
       "1           2        0        0       0     1      2      0      0       1   \n",
       "2           0        0        0       0     0      0      0      0       0   \n",
       "3           2        2        2       2     2      2      1      2       2   \n",
       "4           0        0        0       0     0      0      1      0       0   \n",
       "5           2        2        2       2     2      2      2      2       2   \n",
       "6           0        0        0       0     0      0      0      0       0   \n",
       "7           0        0        0       0     0      0      0      0       0   \n",
       "8           0        0        0       0     0      0      0      0       0   \n",
       "9           2        2        2       2     2      2      0      2       2   \n",
       "10          2        2        2       2     2      2      2      2       2   \n",
       "11          0        2        0       0     0      0      0      0       0   \n",
       "12          1        1        1       1     1      1      1      1       1   \n",
       "13          1        1        1       1     1      1      1      1       1   \n",
       "14          0        0        0       0     0      0      0      0       0   \n",
       "15          2        2        2       2     2      2      2      2       2   \n",
       "16          0        0        0       0     0      0      2      0       0   \n",
       "17          2        2        2       2     2      2      2      2       2   \n",
       "18          0        0        0       0     0      0      0      0       0   \n",
       "19          1        1        1       1     1      1      0      1       1   \n",
       "20          0        0        0       0     0      0      0      0       1   \n",
       "21          1        0        0       1     1      1      0      1       1   \n",
       "22          1        1        1       2     0      0      0      0       0   \n",
       "23          0        0        0       0     0      0      0      0       0   \n",
       "24          0        1        0       1     1      1      1      1       1   \n",
       "25          2        2        1       1     2      2      1      2       2   \n",
       "26          0        0        0       0     0      0      0      0       0   \n",
       "27          0        0        0       0     0      0      2      0       0   \n",
       "28          1        1        1       1     1      1      0      1       1   \n",
       "29          1        1        1       1     1      1      1      1       1   \n",
       "...       ...      ...      ...     ...   ...    ...    ...    ...     ...   \n",
       "19549       2        2        2       2     2      2      2      2       2   \n",
       "19550       0        2        2       0     0      0      0      0       0   \n",
       "19551       0        0        0       0     0      0      0      0       0   \n",
       "19552       0        0        0       0     0      0      0      0       0   \n",
       "19553       0        0        0       0     0      0      0      0       0   \n",
       "19554       1        1        1       1     1      1      1      1       1   \n",
       "19555       0        0        1       0     0      0      0      0       0   \n",
       "19556       0        0        0       1     1      1      1      1       1   \n",
       "19557       0        0        0       0     0      0      0      0       0   \n",
       "19558       0        0        0       0     0      0      0      0       0   \n",
       "19559       1        1        1       1     1      1      1      1       1   \n",
       "19560       0        0        0       0     0      0      0      0       0   \n",
       "19561       1        1        1       1     1      1      0      1       1   \n",
       "19562       0        0        0       0     0      0      0      0       0   \n",
       "19563       2        2        2       0     2      2      1      2       2   \n",
       "19564       2        1        1       1     1      1      1      1       1   \n",
       "19565       0        0        0       0     0      0      0      0       0   \n",
       "19566       2        2        1       2     2      2      0      2       2   \n",
       "19567       0        0        0       1     0      1      0      1       1   \n",
       "19568       0        0        0       0     0      0      1      0       0   \n",
       "19569       2        2        2       1     2      2      2      2       2   \n",
       "19570       2        2        2       2     2      2      2      2       2   \n",
       "19571       1        1        1       1     1      1      0      1       1   \n",
       "19572       0        0        0       0     0      0      2      0       0   \n",
       "19573       2        2        2       2     2      2      2      2       2   \n",
       "19574       1        1        1       0     0      0      1      1       1   \n",
       "19575       0        2        2       0     0      0      0      0       0   \n",
       "19576       0        0        0       0     0      0      0      0       0   \n",
       "19577       1        0        0       0     0      0      0      1       0   \n",
       "19578       1        1        1       1     1      1      0      1       1   \n",
       "\n",
       "       cnn_gl  ftc_n  ftc_gs  ftc_gl  \n",
       "0           0      0       0       0  \n",
       "1           0      1       0       0  \n",
       "2           0      0       0       0  \n",
       "3           2      2       2       1  \n",
       "4           1      0       0       1  \n",
       "5           2      2       2       2  \n",
       "6           0      0       0       0  \n",
       "7           0      0       0       0  \n",
       "8           0      0       0       0  \n",
       "9           0      2       2       0  \n",
       "10          2      2       2       2  \n",
       "11          0      0       0       0  \n",
       "12          1      1       1       1  \n",
       "13          1      1       1       1  \n",
       "14          0      0       0       0  \n",
       "15          2      2       2       2  \n",
       "16          2      0       0       2  \n",
       "17          2      2       2       2  \n",
       "18          0      0       0       0  \n",
       "19          0      1       1       0  \n",
       "20          0      0       0       0  \n",
       "21          1      1       1       0  \n",
       "22          0      0       0       0  \n",
       "23          0      0       0       0  \n",
       "24          1      1       0       1  \n",
       "25          1      2       2       1  \n",
       "26          0      0       0       0  \n",
       "27          0      0       0       2  \n",
       "28          0      1       1       0  \n",
       "29          1      1       1       1  \n",
       "...       ...    ...     ...     ...  \n",
       "19549       2      2       2       2  \n",
       "19550       0      0       0       0  \n",
       "19551       0      0       0       0  \n",
       "19552       0      0       0       0  \n",
       "19553       0      0       0       0  \n",
       "19554       1      1       1       1  \n",
       "19555       0      0       0       0  \n",
       "19556       1      1       1       1  \n",
       "19557       0      0       0       0  \n",
       "19558       0      0       0       0  \n",
       "19559       1      1       1       1  \n",
       "19560       0      0       0       0  \n",
       "19561       0      1       1       0  \n",
       "19562       0      0       0       0  \n",
       "19563       1      2       2       1  \n",
       "19564       1      1       1       1  \n",
       "19565       0      0       0       0  \n",
       "19566       2      2       2       0  \n",
       "19567       0      1       1       0  \n",
       "19568       1      0       0       1  \n",
       "19569       2      2       2       2  \n",
       "19570       2      2       2       2  \n",
       "19571       2      1       1       0  \n",
       "19572       0      0       0       2  \n",
       "19573       2      2       2       2  \n",
       "19574       1      0       0       1  \n",
       "19575       0      0       0       0  \n",
       "19576       0      0       0       0  \n",
       "19577       0      0       0       0  \n",
       "19578       0      1       1       0  \n",
       "\n",
       "[19579 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check correlation between predictions from different models:\n",
    "\n",
    "# Get predictions with max probability\n",
    "new_df = pd.DataFrame()\n",
    "new_df['mnb_tf'] = train_raw[['mnb_tfidf_eap', 'mnb_tfidf_hpl', 'mnb_tfidf_mws']].idxmax(axis = 1)\n",
    "new_df['mnb_cnt'] = train_raw[['mnb_count_eap', 'mnb_count_hpl', 'mnb_count_mws'] ].idxmax(axis = 1)\n",
    "new_df['mnb_tfc'] = train_raw[['mnb_tfidf_char_eap','mnb_tfidf_char_hpl', 'mnb_tfidf_char_mws']].idxmax(axis = 1) \n",
    "new_df['lr_cnt'] = train_raw[['lr_count_eap', 'lr_count_hpl', 'lr_count_mws']].idxmax(axis = 1) \n",
    "new_df['ft_n'] = train_raw[['fast_text_none_eap', 'fast_text_none_hpl', 'fast_text_none_mws'] ].idxmax(axis = 1)\n",
    "new_df['ft_gs'] = train_raw[['fast_text_gensim_eap', 'fast_text_gensim_hpl', 'fast_text_gensim_mws']].idxmax(axis = 1) \n",
    "new_df['ft_gl'] = train_raw[['fast_text_glove_eap', 'fast_text_glove_hpl', 'fast_text_glove_mws']].idxmax(axis = 1)\n",
    "new_df['cnn_n'] = train_raw[['cnn_none_eap', 'cnn_none_hpl', 'cnn_none_mws']].idxmax(axis = 1)\n",
    "new_df['cnn_gs'] = train_raw[['cnn_gensim_eap', 'cnn_gensim_hpl', 'cnn_gensim_mws'] ].idxmax(axis = 1)\n",
    "new_df['cnn_gl'] = train_raw[['cnn_glove_eap', 'cnn_glove_hpl', 'cnn_glove_mws']].idxmax(axis = 1)\n",
    "new_df['ftc_n'] = train_raw[['fast_text_char_none_eap','fast_text_char_none_hpl','fast_text_char_none_mws'] ].idxmax(axis = 1)\n",
    "new_df['ftc_gs'] = train_raw[['fast_text_char_gensim_eap','fast_text_char_gensim_hpl','fast_text_char_gensim_mws']].idxmax(axis = 1)\n",
    "new_df['ftc_gl'] = train_raw[['fast_text_char_glove_eap', 'fast_text_char_glove_hpl','fast_text_char_glove_mws']].idxmax(axis = 1)\n",
    "\n",
    "# Convert it to numbers 0,1,2 for authors\n",
    "display(new_df)\n",
    "new_df = pd.DataFrame(new_df.applymap(lambda x: re.sub(r'.*_eap$','0',x)))\n",
    "new_df = pd.DataFrame(new_df.applymap(lambda x: re.sub(r'.*_hpl$','1',x)))\n",
    "new_df = pd.DataFrame(new_df.applymap(lambda x: re.sub(r'.*_mws$','2',x)))\n",
    "new_df = new_df.apply(pd.to_numeric)\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAIDCAYAAADPO43QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f620fc6f320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot heatmap of different predictions:\n",
    "corr = new_df.corr()\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.title(\"Heatmap of correlation between different predictions\")\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values);\n",
    "#pd.scatter_matrix(new_df, alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
